import{s as wt,a as Gn,u as qn,g as Cn,b as Hn,n as B,x as et,o as Pi}from"../chunks/scheduler.BeaK0CkN.js";import{S as vt,i as yt,e as x,c as j,h as N,f as r,b as y,d as m,m as w,n as v,r as Li,p as Si,z as zi,t as c,v as _,j as h,w as b,k as $,x as k,y as I,l as ya,s as E,a as T,g as tt}from"../chunks/index.D9G72pLO.js";import{b as Z}from"../chunks/paths.-PoDGFUK.js";function Vn(f){return(f==null?void 0:f.length)!==void 0?f:Array.from(f)}function Ri(f){let t,e,s;const a=f[4].default,n=Gn(a,f,f[3],null);return{c(){t=x("a"),n&&n.c(),this.h()},l(o){t=j(o,"A",{href:!0,class:!0,target:!0,rel:!0});var l=N(t);n&&n.l(l),l.forEach(r),this.h()},h(){y(t,"href",f[0]),y(t,"class",e="font-medium text-blue-600 dark:text-blue-500 hover:underline "+f[1]),y(t,"target",f[2]),y(t,"rel","noopener noreferrer")},m(o,l){m(o,t,l),n&&n.m(t,null),s=!0},p(o,[l]){n&&n.p&&(!s||l&8)&&qn(n,a,o,o[3],s?Hn(a,o[3],l,null):Cn(o[3]),null),(!s||l&1)&&y(t,"href",o[0]),(!s||l&2&&e!==(e="font-medium text-blue-600 dark:text-blue-500 hover:underline "+o[1]))&&y(t,"class",e),(!s||l&4)&&y(t,"target",o[2])},i(o){s||(w(n,o),s=!0)},o(o){v(n,o),s=!1},d(o){o&&r(t),n&&n.d(o)}}}function Ni(f,t,e){let{$$slots:s={},$$scope:a}=t,{href:n}=t,{styling:o=""}=t,{target:l="_blank"}=t;return f.$$set=p=>{"href"in p&&e(0,n=p.href),"styling"in p&&e(1,o=p.styling),"target"in p&&e(2,l=p.target),"$$scope"in p&&e(3,a=p.$$scope)},[n,o,l,a,s]}class U extends vt{constructor(t){super(),yt(this,t,Ni,Ri,wt,{href:0,styling:1,target:2})}}function Ei(f,t,e){const s=f.slice();return s[0]=t[e],s}function Di(f){let t;return{c(){t=c("link")},l(e){t=h(e,"link")},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function Ti(f){let t,e,s=f[0].author+"",a,n,o,l=f[0].title+"",p,d,M,P=f[0].publisher+"",L,z,A,g=f[0].year+"",R,G,C,O,V,J,W,nt;return V=new U({props:{href:f[0].link,$$slots:{default:[Di]},$$scope:{ctx:f}}}),{c(){t=x("li"),e=x("span"),a=c(s),n=c(", "),o=x("span"),p=c(l),d=c(`,
			`),M=x("span"),L=c(P),z=c(`,
			`),A=x("span"),R=c(g),G=c(", "),C=x("span"),O=c("["),_(V.$$.fragment),J=c("]"),W=c(`.
		`),this.h()},l(F){t=j(F,"LI",{});var H=N(t);e=j(H,"SPAN",{class:!0});var X=N(e);a=h(X,s),X.forEach(r),n=h(H,", "),o=j(H,"SPAN",{});var S=N(o);p=h(S,l),S.forEach(r),d=h(H,`,
			`),M=j(H,"SPAN",{});var q=N(M);L=h(q,P),q.forEach(r),z=h(H,`,
			`),A=j(H,"SPAN",{});var at=N(A);R=h(at,g),at.forEach(r),G=h(H,", "),C=j(H,"SPAN",{});var Y=N(C);O=h(Y,"["),b(V.$$.fragment,Y),J=h(Y,"]"),Y.forEach(r),W=h(H,`.
		`),H.forEach(r),this.h()},h(){y(e,"class","ms-4")},m(F,H){m(F,t,H),$(t,e),$(e,a),$(t,n),$(t,o),$(o,p),$(t,d),$(t,M),$(M,L),$(t,z),$(t,A),$(A,R),$(t,G),$(t,C),$(C,O),k(V,C,null),$(C,J),$(t,W),nt=!0},p(F,H){const X={};H&8&&(X.$$scope={dirty:H,ctx:F}),V.$set(X)},i(F){nt||(w(V.$$.fragment,F),nt=!0)},o(F){v(V.$$.fragment,F),nt=!1},d(F){F&&r(t),I(V)}}}function Gi(f){let t,e,s=Vn(Dn),a=[];for(let o=0;o<s.length;o+=1)a[o]=Ti(Ei(f,s,o));const n=o=>v(a[o],1,1,()=>{a[o]=null});return{c(){t=x("ol");for(let o=0;o<a.length;o+=1)a[o].c();this.h()},l(o){t=j(o,"OL",{class:!0});var l=N(t);for(let p=0;p<a.length;p+=1)a[p].l(l);l.forEach(r),this.h()},h(){y(t,"class","pl-5 my-2 text-xs list-decimal")},m(o,l){m(o,t,l);for(let p=0;p<a.length;p+=1)a[p]&&a[p].m(t,null);e=!0},p(o,[l]){if(l&0){s=Vn(Dn);let p;for(p=0;p<s.length;p+=1){const d=Ei(o,s,p);a[p]?(a[p].p(d,l),w(a[p],1)):(a[p]=Ti(d),a[p].c(),w(a[p],1),a[p].m(t,null))}for(Li(),p=s.length;p<a.length;p+=1)n(p);Si()}},i(o){if(!e){for(let l=0;l<s.length;l+=1)w(a[l]);e=!0}},o(o){a=a.filter(Boolean);for(let l=0;l<a.length;l+=1)v(a[l]);e=!1},d(o){o&&r(t),zi(a,o)}}}const Un=f=>{for(let t=0;t<Dn.length;t++)if(Dn[t].id==f)return{index:t+1,link:Dn[t].link};throw new Error},Dn=[{id:"gilpin",author:"William Gilpin",title:"Model scale versus domain knoweldge in statistical forecasting of chaotic systems",publisher:"Phys. Rev. Res., vol. 5, pp. 043252, Dec",year:2023,link:"https://link.aps.org/doi/10.1103/PhysRevResearch.5.043252"},{id:"seo",author:"Seo, J., Kim, S., Jalalvand, A. et al.",title:"Avoiding fusion plasma tearing instability with deep reinforcement learning",publisher:"Nature",year:"2024",link:"https://doi.org/10.1038/s41586-024-07024-9"},{id:"degrave",author:"Jonas Degrave, Federico Felici, Jonas Buchli, Michael Neunert, Brendan Tracey, Francesco Carpanese, Timo Ewalds, Roland Hafner, et. al.",title:"Magnetic control of tokamak plasmas through deep reinforcement learning",publisher:"Nature",year:"2021",link:"https://doi.org/10.1038/s41586-021-04301-9"},{id:"challu",author:"Cristian Challu, Kin G. Olivares, Boris N. Oreshkin, Federico Garza, Max Mergenthaler-Canseco, Artur Dubrawski",title:"N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting",publisher:"arXiv:2201.12886",year:"2022",link:"https://arxiv.org/abs/2201.12886"},{id:"oreshkin",author:"Boris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, Yoshua Bengio",title:"N-BEATS: Neural Basis Expansion Analaysis for Interpretable Time Series Forecasting",publisher:"arXiv:1905.10437",year:"2019",link:"https://arxiv.org/abs/1905.10437"},{id:"osinga",author:"Hinke M. Osinga",title:"Understanding the geometry of dynamics: the stable manifold of the Lorenz system",publisher:"Journal of the Royal Society of New Zealand",year:"2018",link:"https://doi.org/10.1080/03036758.2018.1434802"}];class qi extends vt{constructor(t){super(),yt(this,t,null,Gi,wt,{})}}function Ci(f){let t=Un(f[0]).index+"",e;return{c(){e=c(t)},l(s){e=h(s,t)},m(s,a){m(s,e,a)},p(s,a){a&1&&t!==(t=Un(s[0]).index+"")&&ya(e,t)},d(s){s&&r(e)}}}function Hi(f){let t,e,s,a;return e=new U({props:{href:Un(f[0]).link,$$slots:{default:[Ci]},$$scope:{ctx:f}}}),{c(){t=c("["),_(e.$$.fragment),s=c("]")},l(n){t=h(n,"["),b(e.$$.fragment,n),s=h(n,"]")},m(n,o){m(n,t,o),k(e,n,o),m(n,s,o),a=!0},p(n,[o]){const l={};o&1&&(l.href=Un(n[0]).link),o&3&&(l.$$scope={dirty:o,ctx:n}),e.$set(l)},i(n){a||(w(e.$$.fragment,n),a=!0)},o(n){v(e.$$.fragment,n),a=!1},d(n){n&&(r(t),r(s)),I(e,n)}}}function Fi(f,t,e){let{id:s}=t;return f.$$set=a=>{"id"in a&&e(0,s=a.id)},[s]}class K extends vt{constructor(t){super(),yt(this,t,Fi,Hi,wt,{id:0})}}function Vi(f){let t,e;const s=f[1].default,a=Gn(s,f,f[0],null);return{c(){t=x("div"),a&&a.c(),this.h()},l(n){t=j(n,"DIV",{class:!0});var o=N(t);a&&a.l(o),o.forEach(r),this.h()},h(){y(t,"class","text-2xl font-bold text-center my-8")},m(n,o){m(n,t,o),a&&a.m(t,null),e=!0},p(n,[o]){a&&a.p&&(!e||o&1)&&qn(a,s,n,n[0],e?Hn(s,n[0],o,null):Cn(n[0]),null)},i(n){e||(w(a,n),e=!0)},o(n){v(a,n),e=!1},d(n){n&&r(t),a&&a.d(n)}}}function Ui(f,t,e){let{$$slots:s={},$$scope:a}=t;return f.$$set=n=>{"$$scope"in n&&e(0,a=n.$$scope)},[a,s]}class Rn extends vt{constructor(t){super(),yt(this,t,Ui,Vi,wt,{})}}function Bi(f){let t,e;const s=f[1].default,a=Gn(s,f,f[0],null);return{c(){t=x("div"),a&&a.c(),this.h()},l(n){t=j(n,"DIV",{class:!0});var o=N(t);a&&a.l(o),o.forEach(r),this.h()},h(){y(t,"class","text-xl font-medium text-left mt-8 mb-4")},m(n,o){m(n,t,o),a&&a.m(t,null),e=!0},p(n,[o]){a&&a.p&&(!e||o&1)&&qn(a,s,n,n[0],e?Hn(s,n[0],o,null):Cn(n[0]),null)},i(n){e||(w(a,n),e=!0)},o(n){v(a,n),e=!1},d(n){n&&r(t),a&&a.d(n)}}}function Oi(f,t,e){let{$$slots:s={},$$scope:a}=t;return f.$$set=n=>{"$$scope"in n&&e(0,a=n.$$scope)},[a,s]}class Nn extends vt{constructor(t){super(),yt(this,t,Oi,Bi,wt,{})}}function Wi(f){let t,e,s;const a=f[3].default,n=Gn(a,f,f[2],null);return{c(){t=x("p"),n&&n.c(),this.h()},l(o){t=j(o,"P",{class:!0});var l=N(t);n&&n.l(l),l.forEach(r),this.h()},h(){y(t,"class",e="my-2 "+f[0]+" "+f[1])},m(o,l){m(o,t,l),n&&n.m(t,null),s=!0},p(o,[l]){n&&n.p&&(!s||l&4)&&qn(n,a,o,o[2],s?Hn(a,o[2],l,null):Cn(o[2]),null),(!s||l&3&&e!==(e="my-2 "+o[0]+" "+o[1]))&&y(t,"class",e)},i(o){s||(w(n,o),s=!0)},o(o){v(n,o),s=!1},d(o){o&&r(t),n&&n.d(o)}}}function Ki(f,t,e){let{$$slots:s={},$$scope:a}=t,{indent:n="indent-8"}=t,{style:o=""}=t;return f.$$set=l=>{"indent"in l&&e(0,n=l.indent),"style"in l&&e(1,o=l.style),"$$scope"in l&&e(2,a=l.$$scope)},[n,o,a,s]}class D extends vt{constructor(t){super(),yt(this,t,Ki,Wi,wt,{indent:0,style:1})}}function xi(f,t,e){const s=f.slice();return s[1]=t[e],s}function ji(f){let t,e,s=f[1].desc+"",a,n,o,l,p=f[1].val+"",d,M;return{c(){t=x("li"),e=x("div"),a=c(s),n=E(),o=x("div"),l=x("span"),d=c(p),M=E(),this.h()},l(P){t=j(P,"LI",{class:!0});var L=N(t);e=j(L,"DIV",{class:!0});var z=N(e);a=h(z,s),z.forEach(r),n=T(L),o=j(L,"DIV",{class:!0});var A=N(o);l=j(A,"SPAN",{class:!0});var g=N(l);d=h(g,p),g.forEach(r),A.forEach(r),M=T(L),L.forEach(r),this.h()},h(){y(e,"class","flex-none w-32 sm:w-64"),y(l,"class","rounded-0.5 p-1 font-mono"),y(o,"class","w-fit flex-wrap"),y(t,"class","flex items-center justify-left")},m(P,L){m(P,t,L),$(t,e),$(e,a),$(t,n),$(t,o),$(o,l),$(l,d),$(t,M)},p(P,L){L&1&&s!==(s=P[1].desc+"")&&ya(a,s),L&1&&p!==(p=P[1].val+"")&&ya(d,p)},d(P){P&&r(t)}}}function Ji(f){let t,e,s=Vn(f[0]),a=[];for(let n=0;n<s.length;n+=1)a[n]=ji(xi(f,s,n));return{c(){t=x("section"),e=x("ul");for(let n=0;n<a.length;n+=1)a[n].c();this.h()},l(n){t=j(n,"SECTION",{class:!0});var o=N(t);e=j(o,"UL",{class:!0});var l=N(e);for(let p=0;p<a.length;p+=1)a[p].l(l);l.forEach(r),o.forEach(r),this.h()},h(){y(e,"class","flex flex-col m-auto ps-2 rounded gap-0.5 bg-gray-100 divide-y divide-gray-200 w-fit"),y(t,"class","relative block my-4")},m(n,o){m(n,t,o),$(t,e);for(let l=0;l<a.length;l+=1)a[l]&&a[l].m(e,null)},p(n,[o]){if(o&1){s=Vn(n[0]);let l;for(l=0;l<s.length;l+=1){const p=xi(n,s,l);a[l]?a[l].p(p,o):(a[l]=ji(p),a[l].c(),a[l].m(e,null))}for(;l<a.length;l+=1)a[l].d(1);a.length=s.length}},i:B,o:B,d(n){n&&r(t),zi(a,n)}}}function Yi(f,t,e){let{hps:s=[]}=t;return f.$$set=a=>{"hps"in a&&e(0,s=a.hps)},[s]}class va extends vt{constructor(t){super(),yt(this,t,Yi,Ji,wt,{hps:0})}}function Xi(f){let t,e;const s=f[1].default,a=Gn(s,f,f[0],null);return{c(){t=x("figcaption"),a&&a.c(),this.h()},l(n){t=j(n,"FIGCAPTION",{class:!0});var o=N(t);a&&a.l(o),o.forEach(r),this.h()},h(){y(t,"class","text-center text-xs mt-2 mx-0 sm:mx-36")},m(n,o){m(n,t,o),a&&a.m(t,null),e=!0},p(n,[o]){a&&a.p&&(!e||o&1)&&qn(a,s,n,n[0],e?Hn(s,n[0],o,null):Cn(n[0]),null)},i(n){e||(w(a,n),e=!0)},o(n){v(a,n),e=!1},d(n){n&&r(t),a&&a.d(n)}}}function Zi(f,t,e){let{$$slots:s={},$$scope:a}=t;return f.$$set=n=>{"$$scope"in n&&e(0,a=n.$$scope)},[a,s]}class Q extends vt{constructor(t){super(),yt(this,t,Zi,Xi,wt,{})}}const Qi=`
\\begin{align}
\\dot{x} & = \\sigma(y-x) \\\\
\\dot{y} & = \\rho x - y - xz \\\\
\\dot{z} & = -\\beta z + xy
\\end{align}`,to=`
\\begin{align}
\\sigma & = 10 \\\\
\\beta & = \\frac{8}{3} \\\\
\\rho & = 28 \\\\
\\end{align}`,eo=`
\\begin{align}
dt & \\approx 0.015 \\mathrm{s} \\\\
\\lambda_{max}^{-1} & \\approx 1.121 \\mathrm{s} \\\\
H = 100 \\text{ points} & \\approx 1.34\\lambda_{max}^{-1} \\\\
\\end{align}`,no=`
\\begin{align} 
\\operatorname{\\epsilon}(t) := \\frac{200}{t} \\sum_{t'=1}^t \\frac{|\\operatorname{\\boldsymbol{y}}(t')-\\operatorname{\\boldsymbol{\\hat{y}}}(t')|}{|\\operatorname{\\boldsymbol{y}}(t')| + |\\operatorname{\\boldsymbol{\\hat{y}}}(t')|} \\\\
\\end{align}`,so=[{desc:"horizon length",val:100},{desc:"lookback window length",val:500},{desc:"dt",val:.015008},{desc:"number of stacks",val:3},{desc:"blocks per stack",val:1},{desc:"mlp layers per block",val:4},{desc:"mlp layer size",val:1024},{desc:"activation function",val:"ReLU"},{desc:"max pooling factors",val:"2, 2, 2"},{desc:"frequency downsampling factors",val:"24, 12, 1"},{desc:"batch size",val:512},{desc:"# training steps",val:1e4},{desc:"learning rate",val:"1e-3"},{desc:"learning rate schedule",val:"halve every 3,333 steps"},{desc:"total trainable parameters",val:"~20 million"}],ao=[{desc:"number of stacks",val:4},{desc:"blocks per stack",val:8},{desc:"mlp layer size",val:2048},{desc:"max pooling factors",val:"10, 4, 2, 1"},{desc:"frequency downsampling factors",val:"25, 12, 6, 1"},{desc:"batch size",val:512},{desc:"# training steps",val:15e4},{desc:"run validation every",val:"5000 steps"},{desc:"learning rate",val:"1e-4"},{desc:"learning rate schedule",val:"halve whenever validation loss does not decrease"},{desc:"all other hyperparameters",val:"same as Model 1"},{desc:"total trainable parameters",val:"~645 million"}],io=[{desc:"horizon",val:500},{desc:"lookback",val:2500},{desc:"dt",val:.0030016},{desc:"all other hyperparameters",val:"same as Model 2"}];function oo(f){let t;return{c(){t=c("mochaNN")},l(e){t=h(e,"mochaNN")},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function ro(f){let t,e,s="Model scale versus domain knowledge in statistical forecasting of chaotic systems",a,n,o,l,p,d,M,P,L='<a href="#sabine">1</a>',z,A;return n=new K({props:{id:"gilpin"}}),l=new K({props:{id:"seo"}}),d=new K({props:{id:"degrave"}}),{c(){t=c(`This project is inspired by several recent publications involving the use of deep learning
		to predict or control chaotic dynamical systems, in particular William Gilpin's paper, `),e=x("i"),e.textContent=s,a=E(),_(n.$$.fragment),o=c(`. Gilpin found that, given enough training data, generic neural
		architectures can match or exceed the performance of state-of-the-art domain-specific
		choatic forecasting models such as reservoir computers and neural ODEs. Although I have
		never studied dynamical systems in depth, I have recently become intrigued by the prospect
		of applying deep learning to prediction tasks involving chaotic systems, as I explore ways
		to contribute to the efforts to find technical solutions to climate change. Along with
		Gilpin's paper, there have been several recent publications on the subject that were
		especially exciting to me, particularly the ones applying deep learning to tokamak control
		in nuclear fusion reactors (see e.g. `),_(l.$$.fragment),p=c(", "),_(d.$$.fragment),M=c(")"),P=x("sup"),P.innerHTML=L,z=c(`.
	`)},l(g){t=h(g,`This project is inspired by several recent publications involving the use of deep learning
		to predict or control chaotic dynamical systems, in particular William Gilpin's paper, `),e=j(g,"I",{"data-svelte-h":!0}),tt(e)!=="svelte-1orr88l"&&(e.textContent=s),a=T(g),b(n.$$.fragment,g),o=h(g,`. Gilpin found that, given enough training data, generic neural
		architectures can match or exceed the performance of state-of-the-art domain-specific
		choatic forecasting models such as reservoir computers and neural ODEs. Although I have
		never studied dynamical systems in depth, I have recently become intrigued by the prospect
		of applying deep learning to prediction tasks involving chaotic systems, as I explore ways
		to contribute to the efforts to find technical solutions to climate change. Along with
		Gilpin's paper, there have been several recent publications on the subject that were
		especially exciting to me, particularly the ones applying deep learning to tokamak control
		in nuclear fusion reactors (see e.g. `),b(l.$$.fragment,g),p=h(g,", "),b(d.$$.fragment,g),M=h(g,")"),P=j(g,"SUP",{"data-svelte-h":!0}),tt(P)!=="svelte-dbgmv0"&&(P.innerHTML=L),z=h(g,`.
	`)},m(g,R){m(g,t,R),m(g,e,R),m(g,a,R),k(n,g,R),m(g,o,R),k(l,g,R),m(g,p,R),k(d,g,R),m(g,M,R),m(g,P,R),m(g,z,R),A=!0},p:B,i(g){A||(w(n.$$.fragment,g),w(l.$$.fragment,g),w(d.$$.fragment,g),A=!0)},o(g){v(n.$$.fragment,g),v(l.$$.fragment,g),v(d.$$.fragment,g),A=!1},d(g){g&&(r(t),r(e),r(a),r(o),r(p),r(M),r(P),r(z)),I(n,g),I(l,g),I(d,g)}}}function lo(f){let t,e,s='<a href="#paperspace">2</a>',a,n,o='<a href="#brunton">3</a>',l;return{c(){t=c(`My goal with this project is to get some hands-on experience with a chaotic system and
		probe deeper into Gilpin's findings by testing the limits of a neural network's ability to
		model a single chaotic system (within the computational constraints imposed by my limited
		budget`),e=x("sup"),e.innerHTML=s,a=c(`). I'll start with what is probably the most
		well known chaotic system, the Lorenz Attractor. As my dynamical systems background is a bit
		rusty, I will be (re)discovering many of the properties of the Lorenz System, and dynamical
		systems in general, as I go, often using the results of my experiments to guide my
		investigation`),n=x("sup"),n.innerHTML=o,l=c(`. What exactly makes the Lorenz Attractor
		chaotic? And what constraints will that impose on the ability of a deep neural network to
		model it? Let's find out!`)},l(p){t=h(p,`My goal with this project is to get some hands-on experience with a chaotic system and
		probe deeper into Gilpin's findings by testing the limits of a neural network's ability to
		model a single chaotic system (within the computational constraints imposed by my limited
		budget`),e=j(p,"SUP",{"data-svelte-h":!0}),tt(e)!=="svelte-yuo1gg"&&(e.innerHTML=s),a=h(p,`). I'll start with what is probably the most
		well known chaotic system, the Lorenz Attractor. As my dynamical systems background is a bit
		rusty, I will be (re)discovering many of the properties of the Lorenz System, and dynamical
		systems in general, as I go, often using the results of my experiments to guide my
		investigation`),n=j(p,"SUP",{"data-svelte-h":!0}),tt(n)!=="svelte-1n6e9eh"&&(n.innerHTML=o),l=h(p,`. What exactly makes the Lorenz Attractor
		chaotic? And what constraints will that impose on the ability of a deep neural network to
		model it? Let's find out!`)},m(p,d){m(p,t,d),m(p,e,d),m(p,a,d),m(p,n,d),m(p,l,d)},p:B,d(p){p&&(r(t),r(e),r(a),r(n),r(l))}}}function fo(f){let t;return{c(){t=c("Youtube channel")},l(e){t=h(e,"Youtube channel")},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function co(f){let t;return{c(){t=c("lecture series")},l(e){t=h(e,"lecture series")},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function ho(f){let t;return{c(){t=c("The Lorenz Attractor")},l(e){t=h(e,"The Lorenz Attractor")},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function uo(f){let t;return{c(){t=c("Lorenz Attractor")},l(e){t=h(e,"Lorenz Attractor")},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function mo(f){let t,e,s,a;return e=new U({props:{href:"https://en.wikipedia.org/wiki/Lorenz_system",$$slots:{default:[uo]},$$scope:{ctx:f}}}),{c(){t=c(`The
		`),_(e.$$.fragment),s=c(` was developed
		by Edward Lorenz et. al. in 1963 as a simplified model of atmospheric convection.`)},l(n){t=h(n,`The
		`),b(e.$$.fragment,n),s=h(n,` was developed
		by Edward Lorenz et. al. in 1963 as a simplified model of atmospheric convection.`)},m(n,o){m(n,t,o),k(e,n,o),m(n,s,o),a=!0},p(n,o){const l={};o&1&&(l.$$scope={dirty:o,ctx:n}),e.$set(l)},i(n){a||(w(e.$$.fragment,n),a=!0)},o(n){v(e.$$.fragment,n),a=!1},d(n){n&&(r(t),r(s)),I(e,n)}}}function po(f){let t;return{c(){t=c("The Lorenz Attractor")},l(e){t=h(e,"The Lorenz Attractor")},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function $o(f){let t;return{c(){t=c(`The Lorenz system is comprised of three ordinary differential equations representing the
		properties of convection and horizontal and vertical temperature in a two-dimensional fluid
		layer:`)},l(e){t=h(e,`The Lorenz system is comprised of three ordinary differential equations representing the
		properties of convection and horizontal and vertical temperature in a two-dimensional fluid
		layer:`)},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function go(f){let t,e,s="Attractor",a;return{c(){t=c("The Lorenz "),e=x("i"),e.textContent=s,a=c(" refers to a set of chaotic solutions to the system, most commonly:")},l(n){t=h(n,"The Lorenz "),e=j(n,"I",{"data-svelte-h":!0}),tt(e)!=="svelte-7jrnvq"&&(e.textContent=s),a=h(n," refers to a set of chaotic solutions to the system, most commonly:")},m(n,o){m(n,t,o),m(n,e,o),m(n,a,o)},p:B,d(n){n&&(r(t),r(e),r(a))}}}function wo(f){let t;return{c(){t=c("dysts")},l(e){t=h(e,"dysts")},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function vo(f){let t,e,s,a;return e=new U({props:{href:"https://github.com/williamgilpin/dysts",$$slots:{default:[wo]},$$scope:{ctx:f}}}),{c(){t=c("I used Gilpin's "),_(e.$$.fragment),s=c(` python module
		to generate the training data for this solution.`)},l(n){t=h(n,"I used Gilpin's "),b(e.$$.fragment,n),s=h(n,` python module
		to generate the training data for this solution.`)},m(n,o){m(n,t,o),k(e,n,o),m(n,s,o),a=!0},p(n,o){const l={};o&1&&(l.$$scope={dirty:o,ctx:n}),e.$set(l)},i(n){a||(w(e.$$.fragment,n),a=!0)},o(n){v(e.$$.fragment,n),a=!1},d(n){n&&(r(t),r(s)),I(e,n)}}}function yo(f){let t;return{c(){t=c("Neural Architecture: N-HiTS")},l(e){t=h(e,"Neural Architecture: N-HiTS")},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function _o(f){let t,e,s,a,n,o;return e=new K({props:{id:"challu"}}),a=new K({props:{id:"gilpin"}}),{c(){t=c("The N-HiTS "),_(e.$$.fragment),s=c(` forecasting network is known to produce state-of-the-art results,
		at the time of writing, for univariate time series prediction, with up to an order of magnitude
		lower computational requirement than some competitors. Given my limited budget and its strong
		performance reported in `),_(a.$$.fragment),n=c(`, it seemed like the natural starting point for a
		network architecture.`)},l(l){t=h(l,"The N-HiTS "),b(e.$$.fragment,l),s=h(l,` forecasting network is known to produce state-of-the-art results,
		at the time of writing, for univariate time series prediction, with up to an order of magnitude
		lower computational requirement than some competitors. Given my limited budget and its strong
		performance reported in `),b(a.$$.fragment,l),n=h(l,`, it seemed like the natural starting point for a
		network architecture.`)},m(l,p){m(l,t,p),k(e,l,p),m(l,s,p),k(a,l,p),m(l,n,p),o=!0},p:B,i(l){o||(w(e.$$.fragment,l),w(a.$$.fragment,l),o=!0)},o(l){v(e.$$.fragment,l),v(a.$$.fragment,l),o=!1},d(l){l&&(r(t),r(s),r(n)),I(e,l),I(a,l)}}}function bo(f){let t,e,s,a,n,o;return e=new K({props:{id:"oreshkin"}}),a=new K({props:{id:"oreshkin"}}),{c(){t=c("The architectural ideas in N-HiTS build on those of its predecessor, N-BEATS "),_(e.$$.fragment),s=c(`, a neural basis expansion network for time series prediction. The key ideas inherited
		from N-BEATS include the organization of fully connected layers into blocks that output
		basis expansions (linear projections of the preceding fully connected layer's output) and
		the use of both forecast and backcast predictions from each block. The forecast predictions
		from all blocks are summed together to produce the final output of the network, while the
		backcasts are subtracted from the input of the corresponding block to produce a residual
		connection as the input to the next block. The goal of the backcasts is to help the
		downstream blocks by "removing components of their input that are not helpful for
		forecasting" `),_(a.$$.fragment),n=c(".")},l(l){t=h(l,"The architectural ideas in N-HiTS build on those of its predecessor, N-BEATS "),b(e.$$.fragment,l),s=h(l,`, a neural basis expansion network for time series prediction. The key ideas inherited
		from N-BEATS include the organization of fully connected layers into blocks that output
		basis expansions (linear projections of the preceding fully connected layer's output) and
		the use of both forecast and backcast predictions from each block. The forecast predictions
		from all blocks are summed together to produce the final output of the network, while the
		backcasts are subtracted from the input of the corresponding block to produce a residual
		connection as the input to the next block. The goal of the backcasts is to help the
		downstream blocks by "removing components of their input that are not helpful for
		forecasting" `),b(a.$$.fragment,l),n=h(l,".")},m(l,p){m(l,t,p),k(e,l,p),m(l,s,p),k(a,l,p),m(l,n,p),o=!0},p:B,i(l){o||(w(e.$$.fragment,l),w(a.$$.fragment,l),o=!0)},o(l){v(e.$$.fragment,l),v(a.$$.fragment,l),o=!1},d(l){l&&(r(t),r(s),r(n)),I(e,l),I(a,l)}}}function ko(f){let t,e,s,a;return e=new K({props:{id:"challu"}}),{c(){t=c(`The novel ideas from N-HiTS enable the possiblity of modeling increasingly long time
		horizons while keeping computational complexity low. They include the use of pooling layers
		that downsample the inputs to each block and upsampling layers that map a compressed
		representation of the forecast to the output sample rate. In addition to the complexity
		savings, the compressed representations may induce a bias towards a temporal hierarchical
		modeling of the time series across the blocks that allows N-HiTS to exceed the performance
		of competing long-horizon forecasting models while requiring an order of magnitude lower
		computational complexity `),_(e.$$.fragment),s=c(".")},l(n){t=h(n,`The novel ideas from N-HiTS enable the possiblity of modeling increasingly long time
		horizons while keeping computational complexity low. They include the use of pooling layers
		that downsample the inputs to each block and upsampling layers that map a compressed
		representation of the forecast to the output sample rate. In addition to the complexity
		savings, the compressed representations may induce a bias towards a temporal hierarchical
		modeling of the time series across the blocks that allows N-HiTS to exceed the performance
		of competing long-horizon forecasting models while requiring an order of magnitude lower
		computational complexity `),b(e.$$.fragment,n),s=h(n,".")},m(n,o){m(n,t,o),k(e,n,o),m(n,s,o),a=!0},p:B,i(n){a||(w(e.$$.fragment,n),a=!0)},o(n){v(e.$$.fragment,n),a=!1},d(n){n&&(r(t),r(s)),I(e,n)}}}function Io(f){let t;return{c(){t=c("Experiments")},l(e){t=h(e,"Experiments")},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function Mo(f){let t,e,s,a;return e=new K({props:{id:"gilpin"}}),{c(){t=c(`While Gilpin's experiments focus on testing 24 different time-series prediction models on
		over 130 different chaotic systems using a relatively narrow range of hyper parameters for
		tuning `),_(e.$$.fragment),s=c(`, my experiments aim to tune a single model, N-HiTS, on a single
		system, the Lorenz Attractor, to maximize its accuracy for a given, relatively long, fixed
		horizon (aka prediction window length). And more specifically, I aim not only to achieve a
		low average error on the test set but also to limit the worst-case error as much as
		possible, which will likely mean achieving a degree of predictive power over the most
		chaotic regions of the system. Is this a completely naive aspiration given what is known
		about chaotic systems? Maybe, but I'm not really sure yet, and either way this should be a
		fun learning experience...
	`)},l(n){t=h(n,`While Gilpin's experiments focus on testing 24 different time-series prediction models on
		over 130 different chaotic systems using a relatively narrow range of hyper parameters for
		tuning `),b(e.$$.fragment,n),s=h(n,`, my experiments aim to tune a single model, N-HiTS, on a single
		system, the Lorenz Attractor, to maximize its accuracy for a given, relatively long, fixed
		horizon (aka prediction window length). And more specifically, I aim not only to achieve a
		low average error on the test set but also to limit the worst-case error as much as
		possible, which will likely mean achieving a degree of predictive power over the most
		chaotic regions of the system. Is this a completely naive aspiration given what is known
		about chaotic systems? Maybe, but I'm not really sure yet, and either way this should be a
		fun learning experience...
	`)},m(n,o){m(n,t,o),k(e,n,o),m(n,s,o),a=!0},p:B,i(n){a||(w(e.$$.fragment,n),a=!0)},o(n){v(e.$$.fragment,n),a=!1},d(n){n&&(r(t),r(s)),I(e,n)}}}function Ao(f){let t;return{c(){t=c("Data Generation")},l(e){t=h(e,"Data Generation")},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function Eo(f){let t;return{c(){t=c("dysts")},l(e){t=h(e,"dysts")},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function To(f){let t;return{c(){t=c("IVP solver")},l(e){t=h(e,"IVP solver")},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function xo(f){let t;return{c(){t=c("dysts")},l(e){t=h(e,"dysts")},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function jo(f){let t,e="$dt$",s,a,n="$0.015$",o,l,p,d,M,P,L="$dt$",z,A,g,R="after",G,C="$dt$",O,V,J,W,nt="$\\text{first_step} = 0.0001801$",F,H,X;return p=new U({props:{href:"https://github.com/williamgilpin/dysts",$$slots:{default:[Eo]},$$scope:{ctx:f}}}),M=new U({props:{href:"https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.solve_ivp.html",$$slots:{default:[To]},$$scope:{ctx:f}}}),J=new U({props:{href:"https://github.com/williamgilpin/dysts",$$slots:{default:[xo]},$$scope:{ctx:f}}}),{c(){t=c("I begin with a horizon (prediction window) of 100 points, using a "),s=c(e),a=c(` of approximately
		`),o=c(n),l=c(" seconds per point (the default from "),_(p.$$.fragment),d=c(") to sample the solution produced by the "),_(M.$$.fragment),P=c(". Importantly, note that this "),z=c(L),A=c(` is only the one used for sampling the solution
		`),g=x("i"),g.textContent=R,G=c(`
		it is generated by the IVP solver. The actual `),O=c(C),V=c(` used internally by the IVP solver can
		vary dynamically, but the initial target value used by `),_(J.$$.fragment),W=c(" is: "),F=c(nt),H=c(`.
	`)},l(S){t=h(S,"I begin with a horizon (prediction window) of 100 points, using a "),s=h(S,e),a=h(S,` of approximately
		`),o=h(S,n),l=h(S," seconds per point (the default from "),b(p.$$.fragment,S),d=h(S,") to sample the solution produced by the "),b(M.$$.fragment,S),P=h(S,". Importantly, note that this "),z=h(S,L),A=h(S,` is only the one used for sampling the solution
		`),g=j(S,"I",{"data-svelte-h":!0}),tt(g)!=="svelte-10nlrz4"&&(g.textContent=R),G=h(S,`
		it is generated by the IVP solver. The actual `),O=h(S,C),V=h(S,` used internally by the IVP solver can
		vary dynamically, but the initial target value used by `),b(J.$$.fragment,S),W=h(S," is: "),F=h(S,nt),H=h(S,`.
	`)},m(S,q){m(S,t,q),m(S,s,q),m(S,a,q),m(S,o,q),m(S,l,q),k(p,S,q),m(S,d,q),k(M,S,q),m(S,P,q),m(S,z,q),m(S,A,q),m(S,g,q),m(S,G,q),m(S,O,q),m(S,V,q),k(J,S,q),m(S,W,q),m(S,F,q),m(S,H,q),X=!0},p(S,q){const at={};q&1&&(at.$$scope={dirty:q,ctx:S}),p.$set(at);const Y={};q&1&&(Y.$$scope={dirty:q,ctx:S}),M.$set(Y);const ln={};q&1&&(ln.$$scope={dirty:q,ctx:S}),J.$set(ln)},i(S){X||(w(p.$$.fragment,S),w(M.$$.fragment,S),w(J.$$.fragment,S),X=!0)},o(S){v(p.$$.fragment,S),v(M.$$.fragment,S),v(J.$$.fragment,S),X=!1},d(S){S&&(r(t),r(s),r(a),r(o),r(l),r(d),r(P),r(z),r(A),r(g),r(G),r(O),r(V),r(W),r(F),r(H)),I(p,S),I(M,S),I(J,S)}}}function zo(f){let t;return{c(){t=c("Lyapunov exponent")},l(e){t=h(e,"Lyapunov exponent")},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function Po(f){let t;return{c(){t=c("dysts")},l(e){t=h(e,"dysts")},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function Lo(f){let t;return{c(){t=c("Lyapunov time")},l(e){t=h(e,"Lyapunov time")},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function So(f){let t,e,s,a,n,o="$0.8917$",l,p,d,M,P="$1.121s$",L,z,A;return e=new U({props:{href:"https://en.wikipedia.org/wiki/Lyapunov_exponent",$$slots:{default:[zo]},$$scope:{ctx:f}}}),a=new U({props:{href:"https://github.com/williamgilpin/dysts",$$slots:{default:[Po]},$$scope:{ctx:f}}}),d=new U({props:{href:"https://en.wikipedia.org/wiki/Lyapunov_time",$$slots:{default:[Lo]},$$scope:{ctx:f}}}),{c(){t=c(`At this stage, it may also be worth mentioning one of the common metrics for measuring the
		average chaoticity of a system, the maximum
		`),_(e.$$.fragment),s=c(`. As
		reported in
		`),_(a.$$.fragment),n=c(`, the Lyapunov exponent for
		the Lorenz Attractor is approx. `),l=c(o),p=c(", and so the "),_(d.$$.fragment),M=c(` is approx.
		`),L=c(P),z=c(".")},l(g){t=h(g,`At this stage, it may also be worth mentioning one of the common metrics for measuring the
		average chaoticity of a system, the maximum
		`),b(e.$$.fragment,g),s=h(g,`. As
		reported in
		`),b(a.$$.fragment,g),n=h(g,`, the Lyapunov exponent for
		the Lorenz Attractor is approx. `),l=h(g,o),p=h(g,", and so the "),b(d.$$.fragment,g),M=h(g,` is approx.
		`),L=h(g,P),z=h(g,".")},m(g,R){m(g,t,R),k(e,g,R),m(g,s,R),k(a,g,R),m(g,n,R),m(g,l,R),m(g,p,R),k(d,g,R),m(g,M,R),m(g,L,R),m(g,z,R),A=!0},p(g,R){const G={};R&1&&(G.$$scope={dirty:R,ctx:g}),e.$set(G);const C={};R&1&&(C.$$scope={dirty:R,ctx:g}),a.$set(C);const O={};R&1&&(O.$$scope={dirty:R,ctx:g}),d.$set(O)},i(g){A||(w(e.$$.fragment,g),w(a.$$.fragment,g),w(d.$$.fragment,g),A=!0)},o(g){v(e.$$.fragment,g),v(a.$$.fragment,g),v(d.$$.fragment,g),A=!1},d(g){g&&(r(t),r(s),r(n),r(l),r(p),r(M),r(L),r(z)),I(e,g),I(a,g),I(d,g)}}}function Ro(f){let t,e,s="on average",a,n="$e$",o,l,p="$1.121$",d,M,P="$\\frac{4}{3}$",L,z;return{c(){t=c("This tells us that, "),e=x("i"),e.textContent=s,a=c(`, the distance between any two trajectories from the
		Lorenz Attractor are expected to diverge by a factor of `),o=c(n),l=c(` after
		`),d=c(p),M=c(` seconds. Note that with these parameters, the horizon covers a time period of about
		`),L=c(P),z=c(`
		of the Lyapunov time.
	`)},l(A){t=h(A,"This tells us that, "),e=j(A,"I",{"data-svelte-h":!0}),tt(e)!=="svelte-ggx8py"&&(e.textContent=s),a=h(A,`, the distance between any two trajectories from the
		Lorenz Attractor are expected to diverge by a factor of `),o=h(A,n),l=h(A,` after
		`),d=h(A,p),M=h(A,` seconds. Note that with these parameters, the horizon covers a time period of about
		`),L=h(A,P),z=h(A,`
		of the Lyapunov time.
	`)},m(A,g){m(A,t,g),m(A,e,g),m(A,a,g),m(A,o,g),m(A,l,g),m(A,d,g),m(A,M,g),m(A,L,g),m(A,z,g)},p:B,d(A){A&&(r(t),r(e),r(a),r(o),r(l),r(d),r(M),r(L),r(z))}}}function No(f){let t,e="$[-9.79, -15.04, 20.53]$",s,a,n="$[0.99,1.01]$",o,l;return{c(){t=c(`The train and test sets are comprised of many trajectories with initial conditions all
		centered at approx. `),s=c(e),a=c(` and multiplied by a random perturbation uniformly
		sampled from the interval `),o=c(n),l=c(".")},l(p){t=h(p,`The train and test sets are comprised of many trajectories with initial conditions all
		centered at approx. `),s=h(p,e),a=h(p,` and multiplied by a random perturbation uniformly
		sampled from the interval `),o=h(p,n),l=h(p,".")},m(p,d){m(p,t,d),m(p,s,d),m(p,a,d),m(p,o,d),m(p,l,d)},p:B,d(p){p&&(r(t),r(s),r(a),r(o),r(l))}}}function Do(f){let t,e="$3*100 = 300$",s,a,n="$3 * (500 + 100) = 1800$",o,l;return{c(){t=c(`The input to the N-HiTs model is a lookback window of the previous series values whose
		length is typically some multiple of the horizon window. I went with the default value from
		the N-HiTS paper of 5 times the horizon window length, or 500 points, making each training
		sample a total of 600 points. (Note that because N-HiTs is a univariate model, while the
		Lorenz System is three-dimensional, the data points must be flattened into one dimension.
		Therefore, the horizon window length is actually `),s=c(e),a=c(`, and each training
		sample's length is `),o=c(n),l=c(").")},l(p){t=h(p,`The input to the N-HiTs model is a lookback window of the previous series values whose
		length is typically some multiple of the horizon window. I went with the default value from
		the N-HiTS paper of 5 times the horizon window length, or 500 points, making each training
		sample a total of 600 points. (Note that because N-HiTs is a univariate model, while the
		Lorenz System is three-dimensional, the data points must be flattened into one dimension.
		Therefore, the horizon window length is actually `),s=h(p,e),a=h(p,`, and each training
		sample's length is `),o=h(p,n),l=h(p,").")},m(p,d){m(p,t,d),m(p,s,d),m(p,a,d),m(p,o,d),m(p,l,d)},p:B,d(p){p&&(r(t),r(s),r(a),r(o),r(l))}}}function Go(f){let t,e=`$10,000 - 600 +
		1 = 9401$`,s,a;return{c(){t=c(`I choose, somewhat arbitrarily, to generate 10,000 points per series, and in order to
		increase data efficiency, I select each training sample by sliding the 600-point window
		along the series with a one-point stride. Each series, therefore, contributes `),s=c(e),a=c(` training samples. For the initial experiment, I generate 25 series with unique initial conditions,
		and train on 19 of them, and hold out 3 series for validation and 3 series for testing.`)},l(n){t=h(n,`I choose, somewhat arbitrarily, to generate 10,000 points per series, and in order to
		increase data efficiency, I select each training sample by sliding the 600-point window
		along the series with a one-point stride. Each series, therefore, contributes `),s=h(n,e),a=h(n,` training samples. For the initial experiment, I generate 25 series with unique initial conditions,
		and train on 19 of them, and hold out 3 series for validation and 3 series for testing.`)},m(n,o){m(n,t,o),m(n,s,o),m(n,a,o)},p:B,d(n){n&&(r(t),r(s),r(a))}}}function qo(f){let t;return{c(){t=c("Model 1")},l(e){t=h(e,"Model 1")},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function Co(f){let t;return{c(){t=c("The full set of N-HiTS hyperparameters for the first model configuration is:")},l(e){t=h(e,"The full set of N-HiTS hyperparameters for the first model configuration is:")},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function Ho(f){let t,e,s,a,n,o;return e=new K({props:{id:"challu"}}),a=new K({props:{id:"gilpin"}}),{c(){t=c("The model is optimized with MAE loss, consistent with the default loss from "),_(e.$$.fragment),s=c(`. For evaluation, I use the symmetric mean absolute percentage error (sMAPE) as defined in
		`),_(a.$$.fragment),n=c(":")},l(l){t=h(l,"The model is optimized with MAE loss, consistent with the default loss from "),b(e.$$.fragment,l),s=h(l,`. For evaluation, I use the symmetric mean absolute percentage error (sMAPE) as defined in
		`),b(a.$$.fragment,l),n=h(l,":")},m(l,p){m(l,t,p),k(e,l,p),m(l,s,p),k(a,l,p),m(l,n,p),o=!0},p:B,i(l){o||(w(e.$$.fragment,l),w(a.$$.fragment,l),o=!0)},o(l){v(e.$$.fragment,l),v(a.$$.fragment,l),o=!1},d(l){l&&(r(t),r(s),r(n)),I(e,l),I(a,l)}}}function Fo(f){let t;return{c(){t=c(`In this formulation, sMAPE is bound to the interval [0, 200]. The distribution of average
		window errors and its CDF on the test set are shown below. Note that the left y axis is
		log-scaled.`)},l(e){t=h(e,`In this formulation, sMAPE is bound to the interval [0, 200]. The distribution of average
		window errors and its CDF on the test set are shown below. Note that the left y axis is
		log-scaled.`)},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function Vo(f){let t;return{c(){t=c("Model 1 - sMAPE error distribution on the test set")},l(e){t=h(e,"Model 1 - sMAPE error distribution on the test set")},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function Uo(f){let t;return{c(){t=c(`To gain a more intuitive understanding of the magnitude of these errors, we can plot
		individual window predictions against the references:`)},l(e){t=h(e,`To gain a more intuitive understanding of the magnitude of these errors, we can plot
		individual window predictions against the references:`)},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function Bo(f){let t;return{c(){t=c(`Samples of three different 100-point predictions from Model 1 with small, medium, and
			large sMAPE errors`)},l(e){t=h(e,`Samples of three different 100-point predictions from Model 1 with small, medium, and
			large sMAPE errors`)},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function Oo(f){let t;return{c(){t=c(`One interesting observation in all three graphs is that there appears to be a kind of
		"point of divergence" on the prediction before which the average error is very low and after
		which the error grows quickly. In the first graph, this point is about in the middle of the
		prediction, in the second it is maybe one third of the way into the prediction, and in the
		third it is near the beginning. If we look at the predictions of adjacent windows, we see
		that the behavior at this point is consisent across the windows, indicating that there is
		something about the system's behavior in this region that is very difficult for this model
		to fit, regardless of its alignment within the prediction window.`)},l(e){t=h(e,`One interesting observation in all three graphs is that there appears to be a kind of
		"point of divergence" on the prediction before which the average error is very low and after
		which the error grows quickly. In the first graph, this point is about in the middle of the
		prediction, in the second it is maybe one third of the way into the prediction, and in the
		third it is near the beginning. If we look at the predictions of adjacent windows, we see
		that the behavior at this point is consisent across the windows, indicating that there is
		something about the system's behavior in this region that is very difficult for this model
		to fit, regardless of its alignment within the prediction window.`)},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function Wo(f){let t;return{c(){t=c(`The behavior of the model near the origin, which is a critical point of the system, for
			an especially challenging case. In contrast to all other regions of this trajectory, the
			model seems highly uncertain of how the trajectory will evolve shortly after passing
			near the origin.`)},l(e){t=h(e,`The behavior of the model near the origin, which is a critical point of the system, for
			an especially challenging case. In contrast to all other regions of this trajectory, the
			model seems highly uncertain of how the trajectory will evolve shortly after passing
			near the origin.`)},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function Ko(f){let t;return{c(){t=c("critical points")},l(e){t=h(e,"critical points")},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function Jo(f){let t,e,s,a,n,o,l,p;return e=new U({props:{href:"https://en.wikipedia.org/wiki/Critical_point_(mathematics)",$$slots:{default:[Ko]},$$scope:{ctx:f}}}),a=new K({props:{id:"osinga"}}),o=new K({props:{id:"osinga"}}),{c(){t=c(`For anyone familiar with dynamical systems theory, it won't be a surprise that this point
		coincides with one of the three `),_(e.$$.fragment),s=c(` of the Lorenz system--in this case, the origin. And in this parameterization, the origin is
		known to be a saddle point comprised of the intersection of a stable 2D manifold and an unstable
		1D manifold. "Stable" here means that trajectories near the critical point tend to move towards
		it even if they are perturbed slightly away from it by other forces, while "unstable" implies
		the opposite. (See `),_(a.$$.fragment),n=c(` for some excellent visualizations of these manifolds.)
		Near the origin, the unstable manifold is a line that is approximately perpendicular to the Z
		axis and parallel to the lengthwise orientation of the Attractor, which is why the trajectories
		always diverge at the near 90-degree angles that we see in the animations as they approach the
		origin. And the (incredibly complex) topography of the stable 2D manifold determines towards
		which of the other two critical points a trajectory will be deflected `),_(o.$$.fragment),l=c(`.
		From this I conclude that, technically, we can say that the primary goal of the neural
		network is to learn the topography of the origin's stable 2D manifold. As the manifold
		defines a boundary across which trajectories can never pass, we can confine the past and
		future trajectory of any point to be within the boundaries of the manifold.
	`)},l(d){t=h(d,`For anyone familiar with dynamical systems theory, it won't be a surprise that this point
		coincides with one of the three `),b(e.$$.fragment,d),s=h(d,` of the Lorenz system--in this case, the origin. And in this parameterization, the origin is
		known to be a saddle point comprised of the intersection of a stable 2D manifold and an unstable
		1D manifold. "Stable" here means that trajectories near the critical point tend to move towards
		it even if they are perturbed slightly away from it by other forces, while "unstable" implies
		the opposite. (See `),b(a.$$.fragment,d),n=h(d,` for some excellent visualizations of these manifolds.)
		Near the origin, the unstable manifold is a line that is approximately perpendicular to the Z
		axis and parallel to the lengthwise orientation of the Attractor, which is why the trajectories
		always diverge at the near 90-degree angles that we see in the animations as they approach the
		origin. And the (incredibly complex) topography of the stable 2D manifold determines towards
		which of the other two critical points a trajectory will be deflected `),b(o.$$.fragment,d),l=h(d,`.
		From this I conclude that, technically, we can say that the primary goal of the neural
		network is to learn the topography of the origin's stable 2D manifold. As the manifold
		defines a boundary across which trajectories can never pass, we can confine the past and
		future trajectory of any point to be within the boundaries of the manifold.
	`)},m(d,M){m(d,t,M),k(e,d,M),m(d,s,M),k(a,d,M),m(d,n,M),k(o,d,M),m(d,l,M),p=!0},p(d,M){const P={};M&1&&(P.$$scope={dirty:M,ctx:d}),e.$set(P)},i(d){p||(w(e.$$.fragment,d),w(a.$$.fragment,d),w(o.$$.fragment,d),p=!0)},o(d){v(e.$$.fragment,d),v(a.$$.fragment,d),v(o.$$.fragment,d),p=!1},d(d){d&&(r(t),r(s),r(n),r(l)),I(e,d),I(a,d),I(o,d)}}}function Yo(f){let t,e="$f(t) = \\exp(\\lambda t)x_0$",s,a,n="$\\lambda$",o,l,p="$x_0$",d,M,P="$11.8$",L,z;return{c(){t=c(`We can estimate how unstable the 1D manifold is by calculating the eigenvalues of the
		Jacobian matrix of the system at the origin and assuming the dynamics are approximately
		linear in this region. When we do this, we get three eigenvalues, two of which have negative
		real components and are associated with the stable 2D manifold, and the third which has
		positive real component and is associated with the unstable 1D manifold. The dynamics along
		the manifolds near the origin can be approximated by the expression `),s=c(e),a=c(`,
		where `),o=c(n),l=c(" equals the eigenvalue and "),d=c(p),M=c(` is the starting point. For the Lorenz
		Attractor, the eigenvalue associated with the unstable manifold is `),L=c(P),z=c(`, so
		trajectories will be rapidly deflected away from the origin along the unstable manifold, as
		we see in the below animation:`)},l(A){t=h(A,`We can estimate how unstable the 1D manifold is by calculating the eigenvalues of the
		Jacobian matrix of the system at the origin and assuming the dynamics are approximately
		linear in this region. When we do this, we get three eigenvalues, two of which have negative
		real components and are associated with the stable 2D manifold, and the third which has
		positive real component and is associated with the unstable 1D manifold. The dynamics along
		the manifolds near the origin can be approximated by the expression `),s=h(A,e),a=h(A,`,
		where `),o=h(A,n),l=h(A," equals the eigenvalue and "),d=h(A,p),M=h(A,` is the starting point. For the Lorenz
		Attractor, the eigenvalue associated with the unstable manifold is `),L=h(A,P),z=h(A,`, so
		trajectories will be rapidly deflected away from the origin along the unstable manifold, as
		we see in the below animation:`)},m(A,g){m(A,t,g),m(A,s,g),m(A,a,g),m(A,o,g),m(A,l,g),m(A,d,g),m(A,M,g),m(A,L,g),m(A,z,g)},p:B,d(A){A&&(r(t),r(s),r(a),r(o),r(l),r(d),r(M),r(L),r(z))}}}function Xo(f){let t;return{c(){t=c(`The trajectories from the training set all begin at nearly the same point but quickly
			diverge as they approach the critical point at the origin.`)},l(e){t=h(e,`The trajectories from the training set all begin at nearly the same point but quickly
			diverge as they approach the critical point at the origin.`)},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function Zo(f){let t;return{c(){t=c(`Given all of this background, it is now unsurprising that the model is struggling to
		predict the behavior of the system near the origin. But we should also note that the model
		does quite well at predicting basically every other region of the system. We just have to
		figure out a way to improve the predictions near the origin, and then we should have a model
		with an overall very robust representation of the Lorenz Attractor. As this model and its
		training set are relatively modest in size, the next most obvious step to try is to
		signifcantly increase both the amount of training data and the model's capacity, and see if
		those changes alone are enough to resolve the weaknesses of Model 1.`)},l(e){t=h(e,`Given all of this background, it is now unsurprising that the model is struggling to
		predict the behavior of the system near the origin. But we should also note that the model
		does quite well at predicting basically every other region of the system. We just have to
		figure out a way to improve the predictions near the origin, and then we should have a model
		with an overall very robust representation of the Lorenz Attractor. As this model and its
		training set are relatively modest in size, the next most obvious step to try is to
		signifcantly increase both the amount of training data and the model's capacity, and see if
		those changes alone are enough to resolve the weaknesses of Model 1.`)},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function Qo(f){let t;return{c(){t=c("Model 2")},l(e){t=h(e,"Model 2")},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function tr(f){let t;return{c(){t=c(`For the next model, I increased the number of unique initial conditions from 25 to 10000,
		and held out 100 for validation and 200 for testing, leaving 9700 unique initial conditions,
		each of length 10,000 points, or about 150 seconds, in the training set. I also expanded the
		range of hyperparameters for tuning to include significantly larger models, both in depth
		and width. After tuning, I arrived at the following settings:`)},l(e){t=h(e,`For the next model, I increased the number of unique initial conditions from 25 to 10000,
		and held out 100 for validation and 200 for testing, leaving 9700 unique initial conditions,
		each of length 10,000 points, or about 150 seconds, in the training set. I also expanded the
		range of hyperparameters for tuning to include significantly larger models, both in depth
		and width. After tuning, I arrived at the following settings:`)},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function er(f){let t;return{c(){t=c("Model 2 vs Model 1 - sMAPE error distribution.")},l(e){t=h(e,"Model 2 vs Model 1 - sMAPE error distribution.")},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function nr(f){let t;return{c(){t=c(`From the plot, we see a significant increase in the first bin and a reduction in every
		subsequent bin of the per-window error histogram relative to Model 1, so the larger dataset
		and new hyperparameter tunings have a definite and significant positive impact. 99% of
		windows from Model 2 have a sMAPE less than 6, compared to only 74% for Model 1, and 99.9%
		have a sMAPE less than 40, compared to 98% for Model 1.`)},l(e){t=h(e,`From the plot, we see a significant increase in the first bin and a reduction in every
		subsequent bin of the per-window error histogram relative to Model 1, so the larger dataset
		and new hyperparameter tunings have a definite and significant positive impact. 99% of
		windows from Model 2 have a sMAPE less than 6, compared to only 74% for Model 1, and 99.9%
		have a sMAPE less than 40, compared to 98% for Model 1.`)},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function sr(f){let t;return{c(){t=c(`There are, however, still a handful of windows with very large sMAPE errors. We can
		visualize these errors slightly differently to get a better sense of how they are
		distributed within and across the test series:`)},l(e){t=h(e,`There are, however, still a handful of windows with very large sMAPE errors. We can
		visualize these errors slightly differently to get a better sense of how they are
		distributed within and across the test series:`)},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function ar(f){let t;return{c(){t=c("Model 2 - sMAPE errors per series per window in the test set.")},l(e){t=h(e,"Model 2 - sMAPE errors per series per window in the test set.")},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function ir(f){let t;return{c(){t=c(`We see that very large errors occur quite rarely and briefly, with the predictions spending
		most of the time near the ground truth. Let's check the animation for one of the large
		spikes with a sMAPE greater than 100:`)},l(e){t=h(e,`We see that very large errors occur quite rarely and briefly, with the predictions spending
		most of the time near the ground truth. Let's check the animation for one of the large
		spikes with a sMAPE greater than 100:`)},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function or(f){let t;return{c(){t=c(`Model 2 - a trajectory with one of the largest sMAPE errors from the test set. DFO =
			'distance from origin'`)},l(e){t=h(e,`Model 2 - a trajectory with one of the largest sMAPE errors from the test set. DFO =
			'distance from origin'`)},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function rr(f){let t;return{c(){t=c(`Not surprisingly, this trajectory passes very close to the origin, and we immediately see
		how similar this failure case is to the one from Model 1. Despite the average improvement
		across all error magnitudes, has the model's ability to predict the behavior near the
		unstable origin actually improved significantly relative to Model 1? Let's check:`)},l(e){t=h(e,`Not surprisingly, this trajectory passes very close to the origin, and we immediately see
		how similar this failure case is to the one from Model 1. Despite the average improvement
		across all error magnitudes, has the model's ability to predict the behavior near the
		unstable origin actually improved significantly relative to Model 1? Let's check:`)},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function lr(f){let t,e="$n$",s,a,n="$n-1$",o,l,p="$n+1$",d,M;return{c(){t=c(`Each local minimum distance from the origin on the ground truth trajectories is
			calculated, and the corresponding maximum sMAPE error among the windows that included
			the minimum in its target is shown. A local minimum is defined as a point at time `),s=c(e),a=c(`
			that is closer to the origin than the points at `),o=c(n),l=c(" and "),d=c(p),M=c(".")},l(P){t=h(P,`Each local minimum distance from the origin on the ground truth trajectories is
			calculated, and the corresponding maximum sMAPE error among the windows that included
			the minimum in its target is shown. A local minimum is defined as a point at time `),s=h(P,e),a=h(P,`
			that is closer to the origin than the points at `),o=h(P,n),l=h(P," and "),d=h(P,p),M=h(P,".")},m(P,L){m(P,t,L),m(P,s,L),m(P,a,L),m(P,o,L),m(P,l,L),m(P,d,L),m(P,M,L)},p:B,d(P){P&&(r(t),r(s),r(a),r(o),r(l),r(d),r(M))}}}function fr(f){let t;return{c(){t=c(`As we can clearly see from the plot, Model 2 is able to predict points that are closer to
		the origin significantly more accurately than Model 1. So although Model 2 is not able to
		avoid catastraphic failure for all points, it has indeed reduced the number of points for
		which these failures occur.`)},l(e){t=h(e,`As we can clearly see from the plot, Model 2 is able to predict points that are closer to
		the origin significantly more accurately than Model 1. So although Model 2 is not able to
		avoid catastraphic failure for all points, it has indeed reduced the number of points for
		which these failures occur.`)},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function cr(f){let t,e="$dt$",s,a,n="$dt$",o,l,p="$\\approx0.015$",d,M,P="$\\approx0.003$",L,z,A="$\\approx1.5$",g,R;return{c(){t=c(`So we've drastically increased both model capacity and dataset size, and we have only
		achieved marginal improvement on the most chaotic trajectories. To continue to make
		progress, we probably need to try a different approach. One idea is to increase the temporal
		resolution of the model by using a smaller `),s=c(e),a=c(` to generate our data points. So far we've
		used a `),o=c(n),l=c(" of "),d=c(p),M=c(". Let's try reducing that by a factor of 5 to "),L=c(P),z=c(`,
		and in order to keep the prediction task equally difficult, we'll also increase the horizon
		window, and lookback window, by a factor of 5 to 500 and 2500 respectively, so that the
		total amount of time being predicted is still `),g=c(A),R=c(` seconds. We'll call this Model
		3.`)},l(G){t=h(G,`So we've drastically increased both model capacity and dataset size, and we have only
		achieved marginal improvement on the most chaotic trajectories. To continue to make
		progress, we probably need to try a different approach. One idea is to increase the temporal
		resolution of the model by using a smaller `),s=h(G,e),a=h(G,` to generate our data points. So far we've
		used a `),o=h(G,n),l=h(G," of "),d=h(G,p),M=h(G,". Let's try reducing that by a factor of 5 to "),L=h(G,P),z=h(G,`,
		and in order to keep the prediction task equally difficult, we'll also increase the horizon
		window, and lookback window, by a factor of 5 to 500 and 2500 respectively, so that the
		total amount of time being predicted is still `),g=h(G,A),R=h(G,` seconds. We'll call this Model
		3.`)},m(G,C){m(G,t,C),m(G,s,C),m(G,a,C),m(G,o,C),m(G,l,C),m(G,d,C),m(G,M,C),m(G,L,C),m(G,z,C),m(G,g,C),m(G,R,C)},p:B,d(G){G&&(r(t),r(s),r(a),r(o),r(l),r(d),r(M),r(L),r(z),r(g),r(R))}}}function hr(f){let t;return{c(){t=c("Model 3")},l(e){t=h(e,"Model 3")},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function ur(f){let t;return{c(){t=c("The new hyperparmeters for Model 3 are:")},l(e){t=h(e,"The new hyperparmeters for Model 3 are:")},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function mr(f){let t;return{c(){t=c("FSDP Strategy")},l(e){t=h(e,"FSDP Strategy")},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function pr(f){let t,e,s,a,n,o;return a=new U({props:{href:"https://lightning.ai/docs/pytorch/stable/advanced/model_parallel/fsdp.html",$$slots:{default:[mr]},$$scope:{ctx:f}}}),{c(){t=c("A sidenote on the practicality of training this model: "),e=x("p"),s=c(`Although we have not increased the number of parameters relative to Model 2, by
			increasing the input size and horizon length by a factor of 5, we have significantly
			increased the memory requirement for training this model. Now in order to fit the model
			on two GPUs with 16 GB of RAM each, I have to use Lightning's `),_(a.$$.fragment),n=c(` to distribute the model across both GPUs in order to get the per-GPU memory requirement
			to be just a hair under 16 GB. This also means that the model trains significantly more slowly,
			taking about 40 hours to converge, compared to about 16 hours for Model 2.`),this.h()},l(l){t=h(l,"A sidenote on the practicality of training this model: "),e=j(l,"P",{class:!0});var p=N(e);s=h(p,`Although we have not increased the number of parameters relative to Model 2, by
			increasing the input size and horizon length by a factor of 5, we have significantly
			increased the memory requirement for training this model. Now in order to fit the model
			on two GPUs with 16 GB of RAM each, I have to use Lightning's `),b(a.$$.fragment,p),n=h(p,` to distribute the model across both GPUs in order to get the per-GPU memory requirement
			to be just a hair under 16 GB. This also means that the model trains significantly more slowly,
			taking about 40 hours to converge, compared to about 16 hours for Model 2.`),p.forEach(r),this.h()},h(){y(e,"class","ms-8")},m(l,p){m(l,t,p),m(l,e,p),$(e,s),k(a,e,null),$(e,n),o=!0},p(l,p){const d={};p&1&&(d.$$scope={dirty:p,ctx:l}),a.$set(d)},i(l){o||(w(a.$$.fragment,l),o=!0)},o(l){v(a.$$.fragment,l),o=!1},d(l){l&&(r(t),r(e)),I(a)}}}function dr(f){let t;return{c(){t=c("Model 3 vs. Model 2 - sMAPE error distribution.")},l(e){t=h(e,"Model 3 vs. Model 2 - sMAPE error distribution.")},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function $r(f){let t;return{c(){t=c(`The maximum-error trajectory from the test set for Model 3. Although there is still lots
			of room for improvement, the predictions now at least roughly track the general contour
			of the ground truth.`)},l(e){t=h(e,`The maximum-error trajectory from the test set for Model 3. Although there is still lots
			of room for improvement, the predictions now at least roughly track the general contour
			of the ground truth.`)},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function gr(f){let t,e="$dt \\approx 0.015$",s,a,n="$dt \\approx 0.003$",o,l;return{c(){t=c(`Based on the results of Model 3, we can conclude that a primary limiting factor with
		previous models was the temporal resolution of the trajectory's history; the information
		required to make an accurate prediction for the most challenging trajectories is apparently
		not contained in trajectories with a sample period of `),s=c(e),a=c(`, but much more
		of it is contained in trajectories with a sample period of `),o=c(n),l=c(".")},l(p){t=h(p,`Based on the results of Model 3, we can conclude that a primary limiting factor with
		previous models was the temporal resolution of the trajectory's history; the information
		required to make an accurate prediction for the most challenging trajectories is apparently
		not contained in trajectories with a sample period of `),s=h(p,e),a=h(p,`, but much more
		of it is contained in trajectories with a sample period of `),o=h(p,n),l=h(p,".")},m(p,d){m(p,t,d),m(p,s,d),m(p,a,d),m(p,o,d),m(p,l,d)},p:B,d(p){p&&(r(t),r(s),r(a),r(o),r(l))}}}function wr(f){let t;return{c(){t=c("Autoregressive Generation")},l(e){t=h(e,"Autoregressive Generation")},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function vr(f){let t,e="$\\approx 7.5$",s,a;return{c(){t=c("Now that we have a model that adequately approximates the ODE given the last "),s=c(e),a=c(`
		seconds of the IVP solver's output, the next test is to measure how well the model continues
		to predict the trajectory given its own past predictions. To do this, for each trajectory in
		the test set, we will begin by using the first 2500 points to produce the model's prediction
		for points 2501-3000. Then we'll feed those 500 points back into the input to predict points
		3001-3500, and continue in this way for all 10,000 points in each trajectory. Then we can compare
		how closely the predicted trajectories match the ones produced by the IVP solver.
	`)},l(n){t=h(n,"Now that we have a model that adequately approximates the ODE given the last "),s=h(n,e),a=h(n,`
		seconds of the IVP solver's output, the next test is to measure how well the model continues
		to predict the trajectory given its own past predictions. To do this, for each trajectory in
		the test set, we will begin by using the first 2500 points to produce the model's prediction
		for points 2501-3000. Then we'll feed those 500 points back into the input to predict points
		3001-3500, and continue in this way for all 10,000 points in each trajectory. Then we can compare
		how closely the predicted trajectories match the ones produced by the IVP solver.
	`)},m(n,o){m(n,t,o),m(n,s,o),m(n,a,o)},p:B,d(n){n&&(r(t),r(s),r(a))}}}function yr(f){let t,e="$\\approx7.2$",s,a;return{c(){t=c("When we do this, we find that Model 3 is, on average, able to predict the first "),s=c(e),a=c(`
		seconds of the trajectory before it begins to diverge significantly from the reference (I arrived
		at this by calculating the mean time at which the maximum L2 distance between corresponding points
		on the trajectories exceeds 3). But we also note that, although there are clearly visible differences
		between the reference and the prediction, the full 10,000-point trajectories that Model 3 predicts
		are, to the naked eye at least, more or less indistinguishable from the typical trajectories
		of the Lorenz Attractor. In other words, they look like entirely plausible trajectories even
		if they eventually diverge significantly from the ones produced by the IVP solver for the same
		initial conditions.`)},l(n){t=h(n,"When we do this, we find that Model 3 is, on average, able to predict the first "),s=h(n,e),a=h(n,`
		seconds of the trajectory before it begins to diverge significantly from the reference (I arrived
		at this by calculating the mean time at which the maximum L2 distance between corresponding points
		on the trajectories exceeds 3). But we also note that, although there are clearly visible differences
		between the reference and the prediction, the full 10,000-point trajectories that Model 3 predicts
		are, to the naked eye at least, more or less indistinguishable from the typical trajectories
		of the Lorenz Attractor. In other words, they look like entirely plausible trajectories even
		if they eventually diverge significantly from the ones produced by the IVP solver for the same
		initial conditions.`)},m(n,o){m(n,t,o),m(n,s,o),m(n,a,o)},p:B,d(n){n&&(r(t),r(s),r(a))}}}function _r(f){let t;return{c(){t=c(`Comparison of trajectories generated by the IVP solver (left) and auto-regressively
			generated by Model 3 (right). Each row uses the same initial conditions.`)},l(e){t=h(e,`Comparison of trajectories generated by the IVP solver (left) and auto-regressively
			generated by Model 3 (right). Each row uses the same initial conditions.`)},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function br(f){let t;return{c(){t=c("shadowing lemma")},l(e){t=h(e,"shadowing lemma")},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function kr(f){let t,e,s,a;return e=new U({props:{href:"https://en.wikipedia.org/wiki/Shadowing_lemma",$$slots:{default:[br]},$$scope:{ctx:f}}}),{c(){t=c(`Is there a way to confirm this observation more rigorously than with the eye test alone?
		I'm not sure, and I'll have to leave that question for future work. But it's also crucial to
		note that different IVP solvers also produce diverging trajectories in much the same manner
		as this. In fact, all numerical solutions to chaotic equations are known to diverge from the
		true solution due to the rounding error introduced by finite precision. The `),_(e.$$.fragment),s=c(` tells us that, in spite of this, the trajectories produced by IVP solvers still remain arbitrarily
		close to real trajectories from the ODE even if they do not exactly represent the ones that would
		be produced by the given initial conditions.
	`)},l(n){t=h(n,`Is there a way to confirm this observation more rigorously than with the eye test alone?
		I'm not sure, and I'll have to leave that question for future work. But it's also crucial to
		note that different IVP solvers also produce diverging trajectories in much the same manner
		as this. In fact, all numerical solutions to chaotic equations are known to diverge from the
		true solution due to the rounding error introduced by finite precision. The `),b(e.$$.fragment,n),s=h(n,` tells us that, in spite of this, the trajectories produced by IVP solvers still remain arbitrarily
		close to real trajectories from the ODE even if they do not exactly represent the ones that would
		be produced by the given initial conditions.
	`)},m(n,o){m(n,t,o),k(e,n,o),m(n,s,o),a=!0},p(n,o){const l={};o&1&&(l.$$scope={dirty:o,ctx:n}),e.$set(l)},i(n){a||(w(e.$$.fragment,n),a=!0)},o(n){v(e.$$.fragment,n),a=!1},d(n){n&&(r(t),r(s)),I(e,n)}}}function Ir(f){let t;return{c(){t=c("dysts")},l(e){t=h(e,"dysts")},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function Mr(f){let t;return{c(){t=c("Radau")},l(e){t=h(e,"Radau")},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function Ar(f){let t;return{c(){t=c("RK45")},l(e){t=h(e,"RK45")},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function Er(f){let t,e,s,a,n,o,l,p;return e=new U({props:{href:"https://github.com/williamgilpin/dysts",$$slots:{default:[Ir]},$$scope:{ctx:f}}}),a=new U({props:{href:"https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.Radau.html",$$slots:{default:[Mr]},$$scope:{ctx:f}}}),o=new U({props:{href:"https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.RK45.html#scipy.integrate.RK45",$$slots:{default:[Ar]},$$scope:{ctx:f}}}),{c(){t=c(`In light of this, another way to evaluate the autoregressive output of the model is to
		compare it with the output from a different IVP solver with similar error constraints. `),_(e.$$.fragment),s=c(" uses the "),_(a.$$.fragment),n=c(" solver by default, and this is what I used to generate the dataset. "),_(o.$$.fragment),l=c(` has similar error constraints to Radau, so let's compare the autoregressive output against
		Radau relative to RK45's output against Radau:`)},l(d){t=h(d,`In light of this, another way to evaluate the autoregressive output of the model is to
		compare it with the output from a different IVP solver with similar error constraints. `),b(e.$$.fragment,d),s=h(d," uses the "),b(a.$$.fragment,d),n=h(d," solver by default, and this is what I used to generate the dataset. "),b(o.$$.fragment,d),l=h(d,` has similar error constraints to Radau, so let's compare the autoregressive output against
		Radau relative to RK45's output against Radau:`)},m(d,M){m(d,t,M),k(e,d,M),m(d,s,M),k(a,d,M),m(d,n,M),k(o,d,M),m(d,l,M),p=!0},p(d,M){const P={};M&1&&(P.$$scope={dirty:M,ctx:d}),e.$set(P);const L={};M&1&&(L.$$scope={dirty:M,ctx:d}),a.$set(L);const z={};M&1&&(z.$$scope={dirty:M,ctx:d}),o.$set(z)},i(d){p||(w(e.$$.fragment,d),w(a.$$.fragment,d),w(o.$$.fragment,d),p=!0)},o(d){v(e.$$.fragment,d),v(a.$$.fragment,d),v(o.$$.fragment,d),p=!1},d(d){d&&(r(t),r(s),r(n),r(l)),I(e,d),I(a,d),I(o,d)}}}function Tr(f){let t;return{c(){t=c("solve_ivp")},l(e){t=h(e,"solve_ivp")},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function xr(f){let t,e,s,a;return e=new U({props:{href:"https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.solve_ivp.html",$$slots:{default:[Tr]},$$scope:{ctx:f}}}),{c(){t=c(`Comparing the mean absolute error per timestep between Radau and Model 3 (blue) and
			Radua and RK45 (orange). The error is averaged across 200 different trajectories.
			Scipy's `),_(e.$$.fragment),s=c(" is used to produce the IVP solver outputs.")},l(n){t=h(n,`Comparing the mean absolute error per timestep between Radau and Model 3 (blue) and
			Radua and RK45 (orange). The error is averaged across 200 different trajectories.
			Scipy's `),b(e.$$.fragment,n),s=h(n," is used to produce the IVP solver outputs.")},m(n,o){m(n,t,o),k(e,n,o),m(n,s,o),a=!0},p(n,o){const l={};o&1&&(l.$$scope={dirty:o,ctx:n}),e.$set(l)},i(n){a||(w(e.$$.fragment,n),a=!0)},o(n){v(e.$$.fragment,n),a=!1},d(n){n&&(r(t),r(s)),I(e,n)}}}function jr(f){let t;return{c(){t=c(`So we can say that the model is approximating the output of Radau more closely than another
		high-quality IVP solver. Ultimately, all three solvers diverge chaotically from each other,
		but in the short term, Model 3 remains closer to Radau than RK45. From this I tenatively
		conclude that the model is an effective IVP solver of the Lorenz Attractor.`)},l(e){t=h(e,`So we can say that the model is approximating the output of Radau more closely than another
		high-quality IVP solver. Ultimately, all three solvers diverge chaotically from each other,
		but in the short term, Model 3 remains closer to Radau than RK45. From this I tenatively
		conclude that the model is an effective IVP solver of the Lorenz Attractor.`)},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function zr(f){let t;return{c(){t=c("Discussion")},l(e){t=h(e,"Discussion")},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function Pr(f){let t,e,s,a,n,o="$5H$",l,p,d="$H$",M,P,L;return e=new K({props:{id:"gilpin"}}),a=new K({props:{id:"challu"}}),{c(){t=c("Inspired by recent research ("),_(e.$$.fragment),s=c(`) that supports the potential for generic
		neural architectures to match or exceed the performance of domain-specific models at the
		task of predicting chaotic systems, this project demonstrated the strong potential of at
		least one generic neural architecture (`),_(a.$$.fragment),n=c(`) to qualitatively match the
		performance of state-of-the-art IVP solvers, such as Radau, at integrating the ODE for at
		least one specific system--Lorenz--using only examples of solutions, with no explicit
		representation of the underlying ODE, to build up a model of the entire dynamics of the
		system. Given `),l=c(o),p=c(` points of an initial trajectory and at a high enough temporal resolution,
		the neural model demonstrated the ability to predict the subsequent `),M=c(d),P=c(` outputs of the Radau
		solver with, in most cases, high accuracy, and in the worst case, marginal accuracy, for all
		trajectories in a test set that uniformly sampled the phase space of the system. When used autoregressively,
		the model demonstrated the potential to generate arbitrarily long trajectories that are visually
		indistinguishable from typical trajectories of the system and that match the output of the Radau
		solver at least as well as other state-of-the-art IVP solvers such as RK45.`)},l(z){t=h(z,"Inspired by recent research ("),b(e.$$.fragment,z),s=h(z,`) that supports the potential for generic
		neural architectures to match or exceed the performance of domain-specific models at the
		task of predicting chaotic systems, this project demonstrated the strong potential of at
		least one generic neural architecture (`),b(a.$$.fragment,z),n=h(z,`) to qualitatively match the
		performance of state-of-the-art IVP solvers, such as Radau, at integrating the ODE for at
		least one specific system--Lorenz--using only examples of solutions, with no explicit
		representation of the underlying ODE, to build up a model of the entire dynamics of the
		system. Given `),l=h(z,o),p=h(z,` points of an initial trajectory and at a high enough temporal resolution,
		the neural model demonstrated the ability to predict the subsequent `),M=h(z,d),P=h(z,` outputs of the Radau
		solver with, in most cases, high accuracy, and in the worst case, marginal accuracy, for all
		trajectories in a test set that uniformly sampled the phase space of the system. When used autoregressively,
		the model demonstrated the potential to generate arbitrarily long trajectories that are visually
		indistinguishable from typical trajectories of the system and that match the output of the Radau
		solver at least as well as other state-of-the-art IVP solvers such as RK45.`)},m(z,A){m(z,t,A),k(e,z,A),m(z,s,A),k(a,z,A),m(z,n,A),m(z,l,A),m(z,p,A),m(z,M,A),m(z,P,A),L=!0},p:B,i(z){L||(w(e.$$.fragment,z),w(a.$$.fragment,z),L=!0)},o(z){v(e.$$.fragment,z),v(a.$$.fragment,z),L=!1},d(z){z&&(r(t),r(s),r(n),r(l),r(p),r(M),r(P)),I(e,z),I(a,z)}}}function Lr(f){let t;return{c(){t=c(`It must be noted, however, that the amount of data and model capacity used to achieve these
		results was substantial. Roughly 100 million data points from the Lorenz Attractor were used
		to train a model with over half a billion parameters for 40 hours using two GPUs. Although
		these numbers are modest compared to many of the most successful deep learning applications
		today, they are likely still far from trivial, in my opinion. For how many real-world
		chaotic systems with no known ODE representation is it feasible to gather 100 million data
		points? And could such a large model be optimized to run predictions in real-time for
		systems that require it to? I certainly do not know, but it seems plausible that such
		requirements could pose a significant barrier in many real-world cases. Having said all of
		that, it must also be noted that maximizing data and model efficiency was not a focus of
		this project, and so the potential for optimization is an open question.`)},l(e){t=h(e,`It must be noted, however, that the amount of data and model capacity used to achieve these
		results was substantial. Roughly 100 million data points from the Lorenz Attractor were used
		to train a model with over half a billion parameters for 40 hours using two GPUs. Although
		these numbers are modest compared to many of the most successful deep learning applications
		today, they are likely still far from trivial, in my opinion. For how many real-world
		chaotic systems with no known ODE representation is it feasible to gather 100 million data
		points? And could such a large model be optimized to run predictions in real-time for
		systems that require it to? I certainly do not know, but it seems plausible that such
		requirements could pose a significant barrier in many real-world cases. Having said all of
		that, it must also be noted that maximizing data and model efficiency was not a focus of
		this project, and so the potential for optimization is an open question.`)},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function Sr(f){let t;return{c(){t=c("shadowing lemma")},l(e){t=h(e,"shadowing lemma")},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function Rr(f){let t,e,s,a,n,o,l,p;return e=new U({props:{href:"https://en.wikipedia.org/wiki/Shadowing_lemma",$$slots:{default:[Sr]},$$scope:{ctx:f}}}),a=new K({props:{id:"seo"}}),o=new K({props:{id:"degrave"}}),{c(){t=c(`Another, and possibly more critical, open question I have is, given that this model (and
		all IVP solvers) cannot actually predict the true solutions but instead can only predict
		'shadows' of true solutions (see `),_(e.$$.fragment),s=c(`), how useful can these predictions actually be in real-world applications? Is there any
		practical use for such a system, or are projects like this merely academic exercises? Is the
		true potential of deep neural networks as applied to chaotic systems more in their ability
		to prevent systems from entering chaotic regimes, as is explored in `),_(a.$$.fragment),n=c(`
		and `),_(o.$$.fragment),l=c(`, rather than to actually predict how chaotic dynamics will unfold?
		I suspect the answer is 'yes', although I am again far from certain. In either case, these
		results show a definite ability of generic deep neural networks to mimic the dynamics of a
		chaotic system, which may not amount to predicting future states but may still be enough to
		enable control systems to effectively manage those future states.`)},l(d){t=h(d,`Another, and possibly more critical, open question I have is, given that this model (and
		all IVP solvers) cannot actually predict the true solutions but instead can only predict
		'shadows' of true solutions (see `),b(e.$$.fragment,d),s=h(d,`), how useful can these predictions actually be in real-world applications? Is there any
		practical use for such a system, or are projects like this merely academic exercises? Is the
		true potential of deep neural networks as applied to chaotic systems more in their ability
		to prevent systems from entering chaotic regimes, as is explored in `),b(a.$$.fragment,d),n=h(d,`
		and `),b(o.$$.fragment,d),l=h(d,`, rather than to actually predict how chaotic dynamics will unfold?
		I suspect the answer is 'yes', although I am again far from certain. In either case, these
		results show a definite ability of generic deep neural networks to mimic the dynamics of a
		chaotic system, which may not amount to predicting future states but may still be enough to
		enable control systems to effectively manage those future states.`)},m(d,M){m(d,t,M),k(e,d,M),m(d,s,M),k(a,d,M),m(d,n,M),k(o,d,M),m(d,l,M),p=!0},p(d,M){const P={};M&1&&(P.$$scope={dirty:M,ctx:d}),e.$set(P)},i(d){p||(w(e.$$.fragment,d),w(a.$$.fragment,d),w(o.$$.fragment,d),p=!0)},o(d){v(e.$$.fragment,d),v(a.$$.fragment,d),v(o.$$.fragment,d),p=!1},d(d){d&&(r(t),r(s),r(n),r(l)),I(e,d),I(a,d),I(o,d)}}}function Nr(f){let t;return{c(){t=c("References")},l(e){t=h(e,"References")},m(e,s){m(e,t,s)},d(e){e&&r(t)}}}function Dr(f){let t,e,s="Modeling Chaotic Dynamics with Deep Learning: A Case Study on the Lorenz Attractor",a,n,o="Michael Horgan",l,p,d,M,P,L,z,A,g,R,G,C,O,V,J,W,nt,F,H=`<sup id="paperspace">2. All of my experiments were run on a Paperspace VM using two RTX 5000s, each with
				16 GB of RAM.</sup>`,X,S,q,at,Y,ln,Bn,_t,On,bt,Wn,fn,De,Ge,kt,_a,Kn,It,Jn,Mt,Yn,cn,yn,ba=Qi+"",Xn,Zn,At,Qn,hn,_n,ka=to+"",ts,es,Et,ns,Tt,ss,xt,as,jt,is,zt,os,Pt,rs,Lt,St,Rt,Nt,ls,un,bn,Ia=eo+"",fs,cs,Dt,Gt,hs,qt,us,Ct,ms,Ht,ps,Ft,ds,qe,$s,Vt,gs,kn,Ma=no+"",ws,vs,Ut,ys,Bt,it,Aa,_s,Ot,bs,Wt,ks,Kt,Ce,Ea=`<img class="" src="${`${Z}/Model1SmallErr.png`}" alt="" width="300" height="300"/> <img class="" src="${`${Z}/Model1MediumErr.png`}" alt="" width="300" height="300"/> <img class="" src="${`${Z}/Model1LargeErr.png`}" alt="" width="300" height="300"/>`,Is,Jt,Ms,Yt,As,Xt,ot,Ta,Es,Zt,Ts,Qt,te,xs,ee,rt,xa,js,ne,zs,se,Ps,ae,Ls,ie,Ss,He,Rs,Fe,ja=`Note that Model 2 has roughly 32x the number of trainable parameters as Model 1. I've
		increased the depth (number of stacks, blocks per stack) and width (mlp layer size) of the
		network, and I've also significantly increased the amount of compression in the initial
		stacks. Because the network is much deeper, I also added layer normalization after each
		block to try to help reduce convergence time. Lastly, I increased the number of training
		steps and reduced the initial learning rate by an order of magnitude, and I modified the
		learning rate schedule to reduce by half whenever the validation loss does not decrease from
		the previous validation step.`,Ns,Ve,lt,za,oe,Ds,re,le,Gs,Ue,ft,Pa,fe,qs,ce,Cs,Be,ct,La,Hs,he,Fs,ue,Vs,Oe,ht,Sa,me,Us,pe,Bs,de,Os,$e,Ws,ge,Ks,We,Js,we,Ys,st,Xs,Ra="$dt \\approx 0.003$",Zs,Qs,Na="$\\lt80$",ta,ea,na,Ke,ut,Da,ve,sa,mn,Ga=`The most challenging trajectories from the test set are significantly improved, although
		still far from perfect:`,aa,ye,mt,qa,ia,_e,oa,be,ra,ke,la,Ie,Me,fa,Je,pt,Ca,Ae,ca,Ee,Te,ha,xe,dt,Ha,ua,je,ma,ze,pa,Pe,da,Le,$a,Se,Re,ga,Ne,wa,Ye,Fn;return L=new U({props:{href:"https://github.com/nrxszvo/mochaNN",$$slots:{default:[oo]},$$scope:{ctx:f}}}),g=new D({props:{$$slots:{default:[ro]},$$scope:{ctx:f}}}),R=new D({props:{$$slots:{default:[lo]},$$scope:{ctx:f}}}),W=new U({props:{href:"https://www.youtube.com/@SabineHossenfelder",$$slots:{default:[fo]},$$scope:{ctx:f}}}),Y=new U({props:{href:"https://www.youtube.com/playlist?list=PLMrJAkhIeNNTYaOnVI3QpH7jgULnAmvPA",$$slots:{default:[co]},$$scope:{ctx:f}}}),_t=new Rn({props:{$$slots:{default:[ho]},$$scope:{ctx:f}}}),bt=new D({props:{$$slots:{default:[mo]},$$scope:{ctx:f}}}),It=new Q({props:{$$slots:{default:[po]},$$scope:{ctx:f}}}),Mt=new D({props:{$$slots:{default:[$o]},$$scope:{ctx:f}}}),At=new D({props:{$$slots:{default:[go]},$$scope:{ctx:f}}}),Et=new D({props:{$$slots:{default:[vo]},$$scope:{ctx:f}}}),Tt=new Rn({props:{$$slots:{default:[yo]},$$scope:{ctx:f}}}),xt=new D({props:{$$slots:{default:[_o]},$$scope:{ctx:f}}}),jt=new D({props:{$$slots:{default:[bo]},$$scope:{ctx:f}}}),zt=new D({props:{$$slots:{default:[ko]},$$scope:{ctx:f}}}),Pt=new Rn({props:{$$slots:{default:[Io]},$$scope:{ctx:f}}}),Lt=new D({props:{$$slots:{default:[Mo]},$$scope:{ctx:f}}}),St=new Nn({props:{$$slots:{default:[Ao]},$$scope:{ctx:f}}}),Rt=new D({props:{$$slots:{default:[jo]},$$scope:{ctx:f}}}),Nt=new D({props:{$$slots:{default:[So]},$$scope:{ctx:f}}}),Dt=new D({props:{indent:"indent-0",$$slots:{default:[Ro]},$$scope:{ctx:f}}}),Gt=new D({props:{$$slots:{default:[No]},$$scope:{ctx:f}}}),qt=new D({props:{indent:"indent-0",$$slots:{default:[Do]},$$scope:{ctx:f}}}),Ct=new D({props:{$$slots:{default:[Go]},$$scope:{ctx:f}}}),Ht=new Nn({props:{$$slots:{default:[qo]},$$scope:{ctx:f}}}),Ft=new D({props:{$$slots:{default:[Co]},$$scope:{ctx:f}}}),qe=new va({props:{hps:so}}),Vt=new D({props:{indent:"indent-0",$$slots:{default:[Ho]},$$scope:{ctx:f}}}),Ut=new D({props:{$$slots:{default:[Fo]},$$scope:{ctx:f}}}),Ot=new Q({props:{$$slots:{default:[Vo]},$$scope:{ctx:f}}}),Wt=new D({props:{$$slots:{default:[Uo]},$$scope:{ctx:f}}}),Jt=new Q({props:{$$slots:{default:[Bo]},$$scope:{ctx:f}}}),Yt=new D({props:{$$slots:{default:[Oo]},$$scope:{ctx:f}}}),Zt=new Q({props:{$$slots:{default:[Wo]},$$scope:{ctx:f}}}),Qt=new D({props:{$$slots:{default:[Jo]},$$scope:{ctx:f}}}),te=new D({props:{$$slots:{default:[Yo]},$$scope:{ctx:f}}}),ne=new Q({props:{$$slots:{default:[Xo]},$$scope:{ctx:f}}}),se=new D({props:{$$slots:{default:[Zo]},$$scope:{ctx:f}}}),ae=new Nn({props:{$$slots:{default:[Qo]},$$scope:{ctx:f}}}),ie=new D({props:{$$slots:{default:[tr]},$$scope:{ctx:f}}}),He=new va({props:{hps:ao}}),oe=new Q({props:{$$slots:{default:[er]},$$scope:{ctx:f}}}),re=new D({props:{$$slots:{default:[nr]},$$scope:{ctx:f}}}),le=new D({props:{$$slots:{default:[sr]},$$scope:{ctx:f}}}),fe=new Q({props:{$$slots:{default:[ar]},$$scope:{ctx:f}}}),ce=new D({props:{$$slots:{default:[ir]},$$scope:{ctx:f}}}),he=new Q({props:{$$slots:{default:[or]},$$scope:{ctx:f}}}),ue=new D({props:{$$slots:{default:[rr]},$$scope:{ctx:f}}}),me=new Q({props:{$$slots:{default:[lr]},$$scope:{ctx:f}}}),pe=new D({props:{$$slots:{default:[fr]},$$scope:{ctx:f}}}),de=new D({props:{$$slots:{default:[cr]},$$scope:{ctx:f}}}),$e=new Nn({props:{$$slots:{default:[hr]},$$scope:{ctx:f}}}),ge=new D({props:{$$slots:{default:[ur]},$$scope:{ctx:f}}}),We=new va({props:{hps:io}}),we=new D({props:{style:"my-4 text-xs font-serif",$$slots:{default:[pr]},$$scope:{ctx:f}}}),ve=new Q({props:{$$slots:{default:[dr]},$$scope:{ctx:f}}}),_e=new Q({props:{$$slots:{default:[$r]},$$scope:{ctx:f}}}),be=new D({props:{$$slots:{default:[gr]},$$scope:{ctx:f}}}),ke=new Nn({props:{$$slots:{default:[wr]},$$scope:{ctx:f}}}),Ie=new D({props:{$$slots:{default:[vr]},$$scope:{ctx:f}}}),Me=new D({props:{$$slots:{default:[yr]},$$scope:{ctx:f}}}),Ae=new Q({props:{$$slots:{default:[_r]},$$scope:{ctx:f}}}),Ee=new D({props:{$$slots:{default:[kr]},$$scope:{ctx:f}}}),Te=new D({props:{$$slots:{default:[Er]},$$scope:{ctx:f}}}),je=new Q({props:{$$slots:{default:[xr]},$$scope:{ctx:f}}}),ze=new D({props:{$$slots:{default:[jr]},$$scope:{ctx:f}}}),Pe=new Rn({props:{$$slots:{default:[zr]},$$scope:{ctx:f}}}),Le=new D({props:{$$slots:{default:[Pr]},$$scope:{ctx:f}}}),Se=new D({props:{$$slots:{default:[Lr]},$$scope:{ctx:f}}}),Re=new D({props:{$$slots:{default:[Rr]},$$scope:{ctx:f}}}),Ne=new Rn({props:{$$slots:{default:[Nr]},$$scope:{ctx:f}}}),Ye=new qi({}),{c(){t=x("div"),e=x("div"),e.textContent=s,a=E(),n=x("div"),n.textContent=o,l=E(),p=x("br"),d=E(),M=x("div"),P=c("(All code used in this project is available in the github repo: "),_(L.$$.fragment),z=c(")"),A=E(),_(g.$$.fragment),_(R.$$.fragment),G=E(),C=x("div"),O=x("p"),V=x("sup"),J=c(`1. For a quick and entertaining way to stay informed of new developments in the
				world of DL for dynamical systems modeling, I highly recommend Sabine Hossenfelder's `),_(W.$$.fragment),nt=E(),F=x("p"),F.innerHTML=H,X=E(),S=x("p"),q=x("sup"),at=c(`3. If you would like to refresh your background on dynamical systems theory, I
				highly recommend Steve Brunton's free `),_(Y.$$.fragment),ln=c(" on the subject"),Bn=E(),_(_t.$$.fragment),On=E(),_(bt.$$.fragment),Wn=E(),fn=x("div"),De=x("a"),Ge=x("figure"),kt=x("img"),Kn=E(),_(It.$$.fragment),Jn=E(),_(Mt.$$.fragment),Yn=E(),cn=x("div"),yn=x("p"),Xn=c(ba),Zn=E(),_(At.$$.fragment),Qn=E(),hn=x("div"),_n=x("p"),ts=c(ka),es=E(),_(Et.$$.fragment),ns=E(),_(Tt.$$.fragment),ss=E(),_(xt.$$.fragment),as=E(),_(jt.$$.fragment),is=E(),_(zt.$$.fragment),os=E(),_(Pt.$$.fragment),rs=E(),_(Lt.$$.fragment),_(St.$$.fragment),_(Rt.$$.fragment),_(Nt.$$.fragment),ls=E(),un=x("div"),bn=x("p"),fs=c(Ia),cs=E(),_(Dt.$$.fragment),_(Gt.$$.fragment),hs=E(),_(qt.$$.fragment),us=E(),_(Ct.$$.fragment),ms=E(),_(Ht.$$.fragment),ps=E(),_(Ft.$$.fragment),ds=E(),_(qe.$$.fragment),$s=E(),_(Vt.$$.fragment),gs=E(),kn=x("div"),ws=c(Ma),vs=E(),_(Ut.$$.fragment),ys=E(),Bt=x("figure"),it=x("img"),_s=E(),_(Ot.$$.fragment),bs=E(),_(Wt.$$.fragment),ks=E(),Kt=x("figure"),Ce=x("div"),Ce.innerHTML=Ea,Is=E(),_(Jt.$$.fragment),Ms=E(),_(Yt.$$.fragment),As=E(),Xt=x("figure"),ot=x("img"),Es=E(),_(Zt.$$.fragment),Ts=E(),_(Qt.$$.fragment),_(te.$$.fragment),xs=E(),ee=x("figure"),rt=x("img"),js=E(),_(ne.$$.fragment),zs=E(),_(se.$$.fragment),Ps=E(),_(ae.$$.fragment),Ls=E(),_(ie.$$.fragment),Ss=E(),_(He.$$.fragment),Rs=E(),Fe=x("p"),Fe.textContent=ja,Ns=E(),Ve=x("figure"),lt=x("img"),_(oe.$$.fragment),Ds=E(),_(re.$$.fragment),_(le.$$.fragment),Gs=E(),Ue=x("figure"),ft=x("img"),_(fe.$$.fragment),qs=E(),_(ce.$$.fragment),Cs=E(),Be=x("figure"),ct=x("img"),Hs=E(),_(he.$$.fragment),Fs=E(),_(ue.$$.fragment),Vs=E(),Oe=x("figure"),ht=x("img"),_(me.$$.fragment),Us=E(),_(pe.$$.fragment),Bs=E(),_(de.$$.fragment),Os=E(),_($e.$$.fragment),Ws=E(),_(ge.$$.fragment),Ks=E(),_(We.$$.fragment),Js=E(),_(we.$$.fragment),Ys=E(),st=x("p"),Xs=c(`After retraining Model 2 with a new dataset that samples the Lorenz Attractor trajectories
		with `),Zs=c(Ra),Qs=c(`, we see that we are now able to predict all regions of the test
		set with sMAPE error `),ta=c(Na),ea=c(":"),na=E(),Ke=x("figure"),ut=x("img"),_(ve.$$.fragment),sa=E(),mn=x("p"),mn.textContent=Ga,aa=E(),ye=x("figure"),mt=x("img"),ia=E(),_(_e.$$.fragment),oa=E(),_(be.$$.fragment),ra=E(),_(ke.$$.fragment),la=E(),_(Ie.$$.fragment),_(Me.$$.fragment),fa=E(),Je=x("figure"),pt=x("img"),_(Ae.$$.fragment),ca=E(),_(Ee.$$.fragment),_(Te.$$.fragment),ha=E(),xe=x("figure"),dt=x("img"),ua=E(),_(je.$$.fragment),ma=E(),_(ze.$$.fragment),pa=E(),_(Pe.$$.fragment),da=E(),_(Le.$$.fragment),$a=E(),_(Se.$$.fragment),_(Re.$$.fragment),ga=E(),_(Ne.$$.fragment),wa=E(),_(Ye.$$.fragment),this.h()},l(u){t=j(u,"DIV",{class:!0});var i=N(t);e=j(i,"DIV",{class:!0,"data-svelte-h":!0}),tt(e)!=="svelte-py3wln"&&(e.textContent=s),a=T(i),n=j(i,"DIV",{class:!0,"data-svelte-h":!0}),tt(n)!=="svelte-hve5fy"&&(n.textContent=o),l=T(i),p=j(i,"BR",{}),d=T(i),M=j(i,"DIV",{class:!0});var Xe=N(M);P=h(Xe,"(All code used in this project is available in the github repo: "),b(L.$$.fragment,Xe),z=h(Xe,")"),Xe.forEach(r),A=T(i),b(g.$$.fragment,i),b(R.$$.fragment,i),G=T(i),C=j(i,"DIV",{class:!0});var $t=N(C);O=j($t,"P",{});var In=N(O);V=j(In,"SUP",{id:!0});var pn=N(V);J=h(pn,`1. For a quick and entertaining way to stay informed of new developments in the
				world of DL for dynamical systems modeling, I highly recommend Sabine Hossenfelder's `),b(W.$$.fragment,pn),pn.forEach(r),In.forEach(r),nt=T($t),F=j($t,"P",{"data-svelte-h":!0}),tt(F)!=="svelte-1u51tc0"&&(F.innerHTML=H),X=T($t),S=j($t,"P",{});var Mn=N(S);q=j(Mn,"SUP",{id:!0});var Ze=N(q);at=h(Ze,`3. If you would like to refresh your background on dynamical systems theory, I
				highly recommend Steve Brunton's free `),b(Y.$$.fragment,Ze),ln=h(Ze," on the subject"),Ze.forEach(r),Mn.forEach(r),$t.forEach(r),Bn=T(i),b(_t.$$.fragment,i),On=T(i),b(bt.$$.fragment,i),Wn=T(i),fn=j(i,"DIV",{class:!0});var An=N(fn);De=j(An,"A",{title:!0,href:!0});var En=N(De);Ge=j(En,"FIGURE",{});var Qe=N(Ge);kt=j(Qe,"IMG",{class:!0,width:!0,alt:!0,src:!0}),Kn=T(Qe),b(It.$$.fragment,Qe),Qe.forEach(r),En.forEach(r),An.forEach(r),Jn=T(i),b(Mt.$$.fragment,i),Yn=T(i),cn=j(i,"DIV",{class:!0});var Tn=N(cn);yn=j(Tn,"P",{});var xn=N(yn);Xn=h(xn,ba),xn.forEach(r),Tn.forEach(r),Zn=T(i),b(At.$$.fragment,i),Qn=T(i),hn=j(i,"DIV",{class:!0});var jn=N(hn);_n=j(jn,"P",{});var zn=N(_n);ts=h(zn,ka),zn.forEach(r),jn.forEach(r),es=T(i),b(Et.$$.fragment,i),ns=T(i),b(Tt.$$.fragment,i),ss=T(i),b(xt.$$.fragment,i),as=T(i),b(jt.$$.fragment,i),is=T(i),b(zt.$$.fragment,i),os=T(i),b(Pt.$$.fragment,i),rs=T(i),b(Lt.$$.fragment,i),b(St.$$.fragment,i),b(Rt.$$.fragment,i),b(Nt.$$.fragment,i),ls=T(i),un=j(i,"DIV",{class:!0});var Pn=N(un);bn=j(Pn,"P",{});var Ln=N(bn);fs=h(Ln,Ia),Ln.forEach(r),Pn.forEach(r),cs=T(i),b(Dt.$$.fragment,i),b(Gt.$$.fragment,i),hs=T(i),b(qt.$$.fragment,i),us=T(i),b(Ct.$$.fragment,i),ms=T(i),b(Ht.$$.fragment,i),ps=T(i),b(Ft.$$.fragment,i),ds=T(i),b(qe.$$.fragment,i),$s=T(i),b(Vt.$$.fragment,i),gs=T(i),kn=j(i,"DIV",{});var Sn=N(kn);ws=h(Sn,Ma),Sn.forEach(r),vs=T(i),b(Ut.$$.fragment,i),ys=T(i),Bt=j(i,"FIGURE",{class:!0});var tn=N(Bt);it=j(tn,"IMG",{class:!0,src:!0,alt:!0,width:!0,height:!0}),_s=T(tn),b(Ot.$$.fragment,tn),tn.forEach(r),bs=T(i),b(Wt.$$.fragment,i),ks=T(i),Kt=j(i,"FIGURE",{class:!0});var en=N(Kt);Ce=j(en,"DIV",{class:!0,"data-svelte-h":!0}),tt(Ce)!=="svelte-36bm4b"&&(Ce.innerHTML=Ea),Is=T(en),b(Jt.$$.fragment,en),en.forEach(r),Ms=T(i),b(Yt.$$.fragment,i),As=T(i),Xt=j(i,"FIGURE",{class:!0});var nn=N(Xt);ot=j(nn,"IMG",{class:!0,src:!0,alt:!0,width:!0,height:!0}),Es=T(nn),b(Zt.$$.fragment,nn),nn.forEach(r),Ts=T(i),b(Qt.$$.fragment,i),b(te.$$.fragment,i),xs=T(i),ee=j(i,"FIGURE",{class:!0});var sn=N(ee);rt=j(sn,"IMG",{class:!0,src:!0,alt:!0,width:!0,height:!0}),js=T(sn),b(ne.$$.fragment,sn),sn.forEach(r),zs=T(i),b(se.$$.fragment,i),Ps=T(i),b(ae.$$.fragment,i),Ls=T(i),b(ie.$$.fragment,i),Ss=T(i),b(He.$$.fragment,i),Rs=T(i),Fe=j(i,"P",{class:!0,"data-svelte-h":!0}),tt(Fe)!=="svelte-1dsw2lx"&&(Fe.textContent=ja),Ns=T(i),Ve=j(i,"FIGURE",{class:!0});var dn=N(Ve);lt=j(dn,"IMG",{class:!0,src:!0,alt:!0,width:!0,height:!0}),b(oe.$$.fragment,dn),dn.forEach(r),Ds=T(i),b(re.$$.fragment,i),b(le.$$.fragment,i),Gs=T(i),Ue=j(i,"FIGURE",{class:!0});var $n=N(Ue);ft=j($n,"IMG",{class:!0,src:!0,alt:!0,width:!0,height:!0}),b(fe.$$.fragment,$n),$n.forEach(r),qs=T(i),b(ce.$$.fragment,i),Cs=T(i),Be=j(i,"FIGURE",{});var an=N(Be);ct=j(an,"IMG",{class:!0,src:!0,alt:!0,width:!0,height:!0}),Hs=T(an),b(he.$$.fragment,an),an.forEach(r),Fs=T(i),b(ue.$$.fragment,i),Vs=T(i),Oe=j(i,"FIGURE",{class:!0});var gn=N(Oe);ht=j(gn,"IMG",{class:!0,src:!0,alt:!0,width:!0,height:!0}),b(me.$$.fragment,gn),gn.forEach(r),Us=T(i),b(pe.$$.fragment,i),Bs=T(i),b(de.$$.fragment,i),Os=T(i),b($e.$$.fragment,i),Ws=T(i),b(ge.$$.fragment,i),Ks=T(i),b(We.$$.fragment,i),Js=T(i),b(we.$$.fragment,i),Ys=T(i),st=j(i,"P",{class:!0});var gt=N(st);Xs=h(gt,`After retraining Model 2 with a new dataset that samples the Lorenz Attractor trajectories
		with `),Zs=h(gt,Ra),Qs=h(gt,`, we see that we are now able to predict all regions of the test
		set with sMAPE error `),ta=h(gt,Na),ea=h(gt,":"),gt.forEach(r),na=T(i),Ke=j(i,"FIGURE",{class:!0});var wn=N(Ke);ut=j(wn,"IMG",{class:!0,src:!0,alt:!0,width:!0,height:!0}),b(ve.$$.fragment,wn),wn.forEach(r),sa=T(i),mn=j(i,"P",{"data-svelte-h":!0}),tt(mn)!=="svelte-142kkmj"&&(mn.textContent=Ga),aa=T(i),ye=j(i,"FIGURE",{class:!0});var on=N(ye);mt=j(on,"IMG",{class:!0,src:!0,alt:!0,width:!0,height:!0}),ia=T(on),b(_e.$$.fragment,on),on.forEach(r),oa=T(i),b(be.$$.fragment,i),ra=T(i),b(ke.$$.fragment,i),la=T(i),b(Ie.$$.fragment,i),b(Me.$$.fragment,i),fa=T(i),Je=j(i,"FIGURE",{class:!0});var vn=N(Je);pt=j(vn,"IMG",{class:!0,src:!0,alt:!0,width:!0,height:!0}),b(Ae.$$.fragment,vn),vn.forEach(r),ca=T(i),b(Ee.$$.fragment,i),b(Te.$$.fragment,i),ha=T(i),xe=j(i,"FIGURE",{class:!0});var rn=N(xe);dt=j(rn,"IMG",{class:!0,src:!0,alt:!0,width:!0,height:!0}),ua=T(rn),b(je.$$.fragment,rn),rn.forEach(r),ma=T(i),b(ze.$$.fragment,i),pa=T(i),b(Pe.$$.fragment,i),da=T(i),b(Le.$$.fragment,i),$a=T(i),b(Se.$$.fragment,i),b(Re.$$.fragment,i),ga=T(i),b(Ne.$$.fragment,i),wa=T(i),b(Ye.$$.fragment,i),i.forEach(r),this.h()},h(){y(e,"class","mt-8 text-2xl text-center"),y(n,"class","mt-2 text-sm text-center"),y(M,"class","text-sm text-center font-serif mb-4"),y(V,"id","sabine"),y(q,"id","brunton"),y(C,"class","my-4 ms-4 -indent-4 font-serif leading-4"),y(kt,"class","m-auto"),y(kt,"width","128"),y(kt,"alt","A Trajectory Through Phase Space in a Lorenz Attractor"),et(kt.src,_a="https://upload.wikimedia.org/wikipedia/commons/1/13/A_Trajectory_Through_Phase_Space_in_a_Lorenz_Attractor.gif")||y(kt,"src",_a),y(De,"title","Dan Quinn, CC BY-SA 3.0 <https://creativecommons.org/licenses/by-sa/3.0>, via Wikimedia Commons"),y(De,"href","https://commons.wikimedia.org/wiki/File:A_Trajectory_Through_Phase_Space_in_a_Lorenz_Attractor.gif"),y(fn,"class","my-2 self-center"),y(cn,"class","self-center"),y(hn,"class","self-center"),y(un,"class","self-center"),y(it,"class","m-auto"),et(it.src,Aa=`${Z}/Model1ErrDist.png`)||y(it,"src",Aa),y(it,"alt",""),y(it,"width","600"),y(it,"height","600"),y(Bt,"class","mb-6 self-center"),y(Ce,"class","flex flex-wrap justify-center"),y(Kt,"class","mt-6 mb-6 self-center"),y(ot,"class","m-auto"),et(ot.src,Ta=`${Z}/model-1-pod.gif`)||y(ot,"src",Ta),y(ot,"alt","prediction point of divergence"),y(ot,"width","450"),y(ot,"height","350"),y(Xt,"class","mt-6 mb-6 self-center"),y(rt,"class","m-auto"),et(rt.src,xa=`${Z}/trajectories.gif`)||y(rt,"src",xa),y(rt,"alt","trajectories approaching origin"),y(rt,"width","350"),y(rt,"height","300"),y(ee,"class","mt-6 mb-6 self-center"),y(Fe,"class","mt-2"),y(lt,"class","m-auto"),et(lt.src,za=`${Z}/Model2ErrDist.png`)||y(lt,"src",za),y(lt,"alt",""),y(lt,"width","600"),y(lt,"height","600"),y(Ve,"class","mb-2 self-center"),y(ft,"class","m-auto"),et(ft.src,Pa=`${Z}/Model2Err3d.png`)||y(ft,"src",Pa),y(ft,"alt",""),y(ft,"width","600"),y(ft,"height","600"),y(Ue,"class","-mt-2 mb-2 self-center"),y(ct,"class","m-auto"),et(ct.src,La=`${Z}/model-2-pod.gif`)||y(ct,"src",La),y(ct,"alt","model 2 point of divergence"),y(ct,"width","400"),y(ct,"height","340"),y(ht,"class","m-auto"),et(ht.src,Sa=`${Z}/Model2DFO.png`)||y(ht,"src",Sa),y(ht,"alt","distance from origin vs. sMAPE"),y(ht,"width","800"),y(ht,"height","600"),y(Oe,"class","-mt-2 mb-2 self-center"),y(st,"class","mt-4"),y(ut,"class","m-auto"),et(ut.src,Da=`${Z}/Model2vModel3.png`)||y(ut,"src",Da),y(ut,"alt",""),y(ut,"width","600"),y(ut,"height","600"),y(Ke,"class","mb-8 self-center"),y(mt,"class","m-auto"),et(mt.src,qa=`${Z}/model-3-low-dfo.gif`)||y(mt,"src",qa),y(mt,"alt","Model 3 trajectory example"),y(mt,"width","800"),y(mt,"height","800"),y(ye,"class","my-8 self-center"),y(pt,"class","m-auto"),et(pt.src,Ca=`${Z}/ref_v_ar.png`)||y(pt,"src",Ca),y(pt,"alt","Reference vs. Autoregressive Trajectories"),y(pt,"width","450"),y(pt,"height","500"),y(Je,"class","mb-2 self-center"),y(dt,"class","m-auto"),et(dt.src,Ha=`${Z}/rk45_vs_nhits.png`)||y(dt,"src",Ha),y(dt,"alt","Comparison of RK45 and predictions to Radua's solutions"),y(dt,"width","600"),y(dt,"height","600"),y(xe,"class","mb-2 self-center"),y(t,"class","flex flex-col mx-4 sm:mx-16")},m(u,i){m(u,t,i),$(t,e),$(t,a),$(t,n),$(t,l),$(t,p),$(t,d),$(t,M),$(M,P),k(L,M,null),$(M,z),$(t,A),k(g,t,null),k(R,t,null),$(t,G),$(t,C),$(C,O),$(O,V),$(V,J),k(W,V,null),$(C,nt),$(C,F),$(C,X),$(C,S),$(S,q),$(q,at),k(Y,q,null),$(q,ln),$(t,Bn),k(_t,t,null),$(t,On),k(bt,t,null),$(t,Wn),$(t,fn),$(fn,De),$(De,Ge),$(Ge,kt),$(Ge,Kn),k(It,Ge,null),$(t,Jn),k(Mt,t,null),$(t,Yn),$(t,cn),$(cn,yn),$(yn,Xn),$(t,Zn),k(At,t,null),$(t,Qn),$(t,hn),$(hn,_n),$(_n,ts),$(t,es),k(Et,t,null),$(t,ns),k(Tt,t,null),$(t,ss),k(xt,t,null),$(t,as),k(jt,t,null),$(t,is),k(zt,t,null),$(t,os),k(Pt,t,null),$(t,rs),k(Lt,t,null),k(St,t,null),k(Rt,t,null),k(Nt,t,null),$(t,ls),$(t,un),$(un,bn),$(bn,fs),$(t,cs),k(Dt,t,null),k(Gt,t,null),$(t,hs),k(qt,t,null),$(t,us),k(Ct,t,null),$(t,ms),k(Ht,t,null),$(t,ps),k(Ft,t,null),$(t,ds),k(qe,t,null),$(t,$s),k(Vt,t,null),$(t,gs),$(t,kn),$(kn,ws),$(t,vs),k(Ut,t,null),$(t,ys),$(t,Bt),$(Bt,it),$(Bt,_s),k(Ot,Bt,null),$(t,bs),k(Wt,t,null),$(t,ks),$(t,Kt),$(Kt,Ce),$(Kt,Is),k(Jt,Kt,null),$(t,Ms),k(Yt,t,null),$(t,As),$(t,Xt),$(Xt,ot),$(Xt,Es),k(Zt,Xt,null),$(t,Ts),k(Qt,t,null),k(te,t,null),$(t,xs),$(t,ee),$(ee,rt),$(ee,js),k(ne,ee,null),$(t,zs),k(se,t,null),$(t,Ps),k(ae,t,null),$(t,Ls),k(ie,t,null),$(t,Ss),k(He,t,null),$(t,Rs),$(t,Fe),$(t,Ns),$(t,Ve),$(Ve,lt),k(oe,Ve,null),$(t,Ds),k(re,t,null),k(le,t,null),$(t,Gs),$(t,Ue),$(Ue,ft),k(fe,Ue,null),$(t,qs),k(ce,t,null),$(t,Cs),$(t,Be),$(Be,ct),$(Be,Hs),k(he,Be,null),$(t,Fs),k(ue,t,null),$(t,Vs),$(t,Oe),$(Oe,ht),k(me,Oe,null),$(t,Us),k(pe,t,null),$(t,Bs),k(de,t,null),$(t,Os),k($e,t,null),$(t,Ws),k(ge,t,null),$(t,Ks),k(We,t,null),$(t,Js),k(we,t,null),$(t,Ys),$(t,st),$(st,Xs),$(st,Zs),$(st,Qs),$(st,ta),$(st,ea),$(t,na),$(t,Ke),$(Ke,ut),k(ve,Ke,null),$(t,sa),$(t,mn),$(t,aa),$(t,ye),$(ye,mt),$(ye,ia),k(_e,ye,null),$(t,oa),k(be,t,null),$(t,ra),k(ke,t,null),$(t,la),k(Ie,t,null),k(Me,t,null),$(t,fa),$(t,Je),$(Je,pt),k(Ae,Je,null),$(t,ca),k(Ee,t,null),k(Te,t,null),$(t,ha),$(t,xe),$(xe,dt),$(xe,ua),k(je,xe,null),$(t,ma),k(ze,t,null),$(t,pa),k(Pe,t,null),$(t,da),k(Le,t,null),$(t,$a),k(Se,t,null),k(Re,t,null),$(t,ga),k(Ne,t,null),$(t,wa),k(Ye,t,null),Fn=!0},p(u,[i]){const Xe={};i&1&&(Xe.$$scope={dirty:i,ctx:u}),L.$set(Xe);const $t={};i&1&&($t.$$scope={dirty:i,ctx:u}),g.$set($t);const In={};i&1&&(In.$$scope={dirty:i,ctx:u}),R.$set(In);const pn={};i&1&&(pn.$$scope={dirty:i,ctx:u}),W.$set(pn);const Mn={};i&1&&(Mn.$$scope={dirty:i,ctx:u}),Y.$set(Mn);const Ze={};i&1&&(Ze.$$scope={dirty:i,ctx:u}),_t.$set(Ze);const An={};i&1&&(An.$$scope={dirty:i,ctx:u}),bt.$set(An);const En={};i&1&&(En.$$scope={dirty:i,ctx:u}),It.$set(En);const Qe={};i&1&&(Qe.$$scope={dirty:i,ctx:u}),Mt.$set(Qe);const Tn={};i&1&&(Tn.$$scope={dirty:i,ctx:u}),At.$set(Tn);const xn={};i&1&&(xn.$$scope={dirty:i,ctx:u}),Et.$set(xn);const jn={};i&1&&(jn.$$scope={dirty:i,ctx:u}),Tt.$set(jn);const zn={};i&1&&(zn.$$scope={dirty:i,ctx:u}),xt.$set(zn);const Pn={};i&1&&(Pn.$$scope={dirty:i,ctx:u}),jt.$set(Pn);const Ln={};i&1&&(Ln.$$scope={dirty:i,ctx:u}),zt.$set(Ln);const Sn={};i&1&&(Sn.$$scope={dirty:i,ctx:u}),Pt.$set(Sn);const tn={};i&1&&(tn.$$scope={dirty:i,ctx:u}),Lt.$set(tn);const en={};i&1&&(en.$$scope={dirty:i,ctx:u}),St.$set(en);const nn={};i&1&&(nn.$$scope={dirty:i,ctx:u}),Rt.$set(nn);const sn={};i&1&&(sn.$$scope={dirty:i,ctx:u}),Nt.$set(sn);const dn={};i&1&&(dn.$$scope={dirty:i,ctx:u}),Dt.$set(dn);const $n={};i&1&&($n.$$scope={dirty:i,ctx:u}),Gt.$set($n);const an={};i&1&&(an.$$scope={dirty:i,ctx:u}),qt.$set(an);const gn={};i&1&&(gn.$$scope={dirty:i,ctx:u}),Ct.$set(gn);const gt={};i&1&&(gt.$$scope={dirty:i,ctx:u}),Ht.$set(gt);const wn={};i&1&&(wn.$$scope={dirty:i,ctx:u}),Ft.$set(wn);const on={};i&1&&(on.$$scope={dirty:i,ctx:u}),Vt.$set(on);const vn={};i&1&&(vn.$$scope={dirty:i,ctx:u}),Ut.$set(vn);const rn={};i&1&&(rn.$$scope={dirty:i,ctx:u}),Ot.$set(rn);const Fa={};i&1&&(Fa.$$scope={dirty:i,ctx:u}),Wt.$set(Fa);const Va={};i&1&&(Va.$$scope={dirty:i,ctx:u}),Jt.$set(Va);const Ua={};i&1&&(Ua.$$scope={dirty:i,ctx:u}),Yt.$set(Ua);const Ba={};i&1&&(Ba.$$scope={dirty:i,ctx:u}),Zt.$set(Ba);const Oa={};i&1&&(Oa.$$scope={dirty:i,ctx:u}),Qt.$set(Oa);const Wa={};i&1&&(Wa.$$scope={dirty:i,ctx:u}),te.$set(Wa);const Ka={};i&1&&(Ka.$$scope={dirty:i,ctx:u}),ne.$set(Ka);const Ja={};i&1&&(Ja.$$scope={dirty:i,ctx:u}),se.$set(Ja);const Ya={};i&1&&(Ya.$$scope={dirty:i,ctx:u}),ae.$set(Ya);const Xa={};i&1&&(Xa.$$scope={dirty:i,ctx:u}),ie.$set(Xa);const Za={};i&1&&(Za.$$scope={dirty:i,ctx:u}),oe.$set(Za);const Qa={};i&1&&(Qa.$$scope={dirty:i,ctx:u}),re.$set(Qa);const ti={};i&1&&(ti.$$scope={dirty:i,ctx:u}),le.$set(ti);const ei={};i&1&&(ei.$$scope={dirty:i,ctx:u}),fe.$set(ei);const ni={};i&1&&(ni.$$scope={dirty:i,ctx:u}),ce.$set(ni);const si={};i&1&&(si.$$scope={dirty:i,ctx:u}),he.$set(si);const ai={};i&1&&(ai.$$scope={dirty:i,ctx:u}),ue.$set(ai);const ii={};i&1&&(ii.$$scope={dirty:i,ctx:u}),me.$set(ii);const oi={};i&1&&(oi.$$scope={dirty:i,ctx:u}),pe.$set(oi);const ri={};i&1&&(ri.$$scope={dirty:i,ctx:u}),de.$set(ri);const li={};i&1&&(li.$$scope={dirty:i,ctx:u}),$e.$set(li);const fi={};i&1&&(fi.$$scope={dirty:i,ctx:u}),ge.$set(fi);const ci={};i&1&&(ci.$$scope={dirty:i,ctx:u}),we.$set(ci);const hi={};i&1&&(hi.$$scope={dirty:i,ctx:u}),ve.$set(hi);const ui={};i&1&&(ui.$$scope={dirty:i,ctx:u}),_e.$set(ui);const mi={};i&1&&(mi.$$scope={dirty:i,ctx:u}),be.$set(mi);const pi={};i&1&&(pi.$$scope={dirty:i,ctx:u}),ke.$set(pi);const di={};i&1&&(di.$$scope={dirty:i,ctx:u}),Ie.$set(di);const $i={};i&1&&($i.$$scope={dirty:i,ctx:u}),Me.$set($i);const gi={};i&1&&(gi.$$scope={dirty:i,ctx:u}),Ae.$set(gi);const wi={};i&1&&(wi.$$scope={dirty:i,ctx:u}),Ee.$set(wi);const vi={};i&1&&(vi.$$scope={dirty:i,ctx:u}),Te.$set(vi);const yi={};i&1&&(yi.$$scope={dirty:i,ctx:u}),je.$set(yi);const _i={};i&1&&(_i.$$scope={dirty:i,ctx:u}),ze.$set(_i);const bi={};i&1&&(bi.$$scope={dirty:i,ctx:u}),Pe.$set(bi);const ki={};i&1&&(ki.$$scope={dirty:i,ctx:u}),Le.$set(ki);const Ii={};i&1&&(Ii.$$scope={dirty:i,ctx:u}),Se.$set(Ii);const Mi={};i&1&&(Mi.$$scope={dirty:i,ctx:u}),Re.$set(Mi);const Ai={};i&1&&(Ai.$$scope={dirty:i,ctx:u}),Ne.$set(Ai)},i(u){Fn||(w(L.$$.fragment,u),w(g.$$.fragment,u),w(R.$$.fragment,u),w(W.$$.fragment,u),w(Y.$$.fragment,u),w(_t.$$.fragment,u),w(bt.$$.fragment,u),w(It.$$.fragment,u),w(Mt.$$.fragment,u),w(At.$$.fragment,u),w(Et.$$.fragment,u),w(Tt.$$.fragment,u),w(xt.$$.fragment,u),w(jt.$$.fragment,u),w(zt.$$.fragment,u),w(Pt.$$.fragment,u),w(Lt.$$.fragment,u),w(St.$$.fragment,u),w(Rt.$$.fragment,u),w(Nt.$$.fragment,u),w(Dt.$$.fragment,u),w(Gt.$$.fragment,u),w(qt.$$.fragment,u),w(Ct.$$.fragment,u),w(Ht.$$.fragment,u),w(Ft.$$.fragment,u),w(qe.$$.fragment,u),w(Vt.$$.fragment,u),w(Ut.$$.fragment,u),w(Ot.$$.fragment,u),w(Wt.$$.fragment,u),w(Jt.$$.fragment,u),w(Yt.$$.fragment,u),w(Zt.$$.fragment,u),w(Qt.$$.fragment,u),w(te.$$.fragment,u),w(ne.$$.fragment,u),w(se.$$.fragment,u),w(ae.$$.fragment,u),w(ie.$$.fragment,u),w(He.$$.fragment,u),w(oe.$$.fragment,u),w(re.$$.fragment,u),w(le.$$.fragment,u),w(fe.$$.fragment,u),w(ce.$$.fragment,u),w(he.$$.fragment,u),w(ue.$$.fragment,u),w(me.$$.fragment,u),w(pe.$$.fragment,u),w(de.$$.fragment,u),w($e.$$.fragment,u),w(ge.$$.fragment,u),w(We.$$.fragment,u),w(we.$$.fragment,u),w(ve.$$.fragment,u),w(_e.$$.fragment,u),w(be.$$.fragment,u),w(ke.$$.fragment,u),w(Ie.$$.fragment,u),w(Me.$$.fragment,u),w(Ae.$$.fragment,u),w(Ee.$$.fragment,u),w(Te.$$.fragment,u),w(je.$$.fragment,u),w(ze.$$.fragment,u),w(Pe.$$.fragment,u),w(Le.$$.fragment,u),w(Se.$$.fragment,u),w(Re.$$.fragment,u),w(Ne.$$.fragment,u),w(Ye.$$.fragment,u),Fn=!0)},o(u){v(L.$$.fragment,u),v(g.$$.fragment,u),v(R.$$.fragment,u),v(W.$$.fragment,u),v(Y.$$.fragment,u),v(_t.$$.fragment,u),v(bt.$$.fragment,u),v(It.$$.fragment,u),v(Mt.$$.fragment,u),v(At.$$.fragment,u),v(Et.$$.fragment,u),v(Tt.$$.fragment,u),v(xt.$$.fragment,u),v(jt.$$.fragment,u),v(zt.$$.fragment,u),v(Pt.$$.fragment,u),v(Lt.$$.fragment,u),v(St.$$.fragment,u),v(Rt.$$.fragment,u),v(Nt.$$.fragment,u),v(Dt.$$.fragment,u),v(Gt.$$.fragment,u),v(qt.$$.fragment,u),v(Ct.$$.fragment,u),v(Ht.$$.fragment,u),v(Ft.$$.fragment,u),v(qe.$$.fragment,u),v(Vt.$$.fragment,u),v(Ut.$$.fragment,u),v(Ot.$$.fragment,u),v(Wt.$$.fragment,u),v(Jt.$$.fragment,u),v(Yt.$$.fragment,u),v(Zt.$$.fragment,u),v(Qt.$$.fragment,u),v(te.$$.fragment,u),v(ne.$$.fragment,u),v(se.$$.fragment,u),v(ae.$$.fragment,u),v(ie.$$.fragment,u),v(He.$$.fragment,u),v(oe.$$.fragment,u),v(re.$$.fragment,u),v(le.$$.fragment,u),v(fe.$$.fragment,u),v(ce.$$.fragment,u),v(he.$$.fragment,u),v(ue.$$.fragment,u),v(me.$$.fragment,u),v(pe.$$.fragment,u),v(de.$$.fragment,u),v($e.$$.fragment,u),v(ge.$$.fragment,u),v(We.$$.fragment,u),v(we.$$.fragment,u),v(ve.$$.fragment,u),v(_e.$$.fragment,u),v(be.$$.fragment,u),v(ke.$$.fragment,u),v(Ie.$$.fragment,u),v(Me.$$.fragment,u),v(Ae.$$.fragment,u),v(Ee.$$.fragment,u),v(Te.$$.fragment,u),v(je.$$.fragment,u),v(ze.$$.fragment,u),v(Pe.$$.fragment,u),v(Le.$$.fragment,u),v(Se.$$.fragment,u),v(Re.$$.fragment,u),v(Ne.$$.fragment,u),v(Ye.$$.fragment,u),Fn=!1},d(u){u&&r(t),I(L),I(g),I(R),I(W),I(Y),I(_t),I(bt),I(It),I(Mt),I(At),I(Et),I(Tt),I(xt),I(jt),I(zt),I(Pt),I(Lt),I(St),I(Rt),I(Nt),I(Dt),I(Gt),I(qt),I(Ct),I(Ht),I(Ft),I(qe),I(Vt),I(Ut),I(Ot),I(Wt),I(Jt),I(Yt),I(Zt),I(Qt),I(te),I(ne),I(se),I(ae),I(ie),I(He),I(oe),I(re),I(le),I(fe),I(ce),I(he),I(ue),I(me),I(pe),I(de),I($e),I(ge),I(We),I(we),I(ve),I(_e),I(be),I(ke),I(Ie),I(Me),I(Ae),I(Ee),I(Te),I(je),I(ze),I(Pe),I(Le),I(Se),I(Re),I(Ne),I(Ye)}}}function Gr(f){return Pi(()=>{let t=document.createElement("script");t.src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js",t.async=!0,document.head.append(t),window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},processEscapes:!0}}),[]}class Fr extends vt{constructor(t){super(),yt(this,t,Gr,Dr,wt,{})}}export{Fr as component};
