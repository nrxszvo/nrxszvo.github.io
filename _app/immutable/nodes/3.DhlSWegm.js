import{s as dt,a as Pn,u as Ln,g as Sn,b as Rn,n as H,x as et,o as di}from"../chunks/scheduler.BeaK0CkN.js";import{S as gt,i as wt,e as x,c as P,h as R,f,b as k,d as m,m as g,n as w,r as gi,p as wi,z as $i,t as h,v,j as u,w as y,k as d,x as _,y as b,l as ra,s as j,a as z,g as st}from"../chunks/index.D9G72pLO.js";import{b as Z}from"../chunks/paths.C8n8Gvc_.js";function Gn(r){return(r==null?void 0:r.length)!==void 0?r:Array.from(r)}function vi(r){let t,e,s;const a=r[4].default,n=Pn(a,r,r[3],null);return{c(){t=x("a"),n&&n.c(),this.h()},l(o){t=P(o,"A",{href:!0,class:!0,target:!0,rel:!0});var l=R(t);n&&n.l(l),l.forEach(f),this.h()},h(){k(t,"href",r[0]),k(t,"class",e="font-medium text-blue-600 dark:text-blue-500 hover:underline "+r[1]),k(t,"target",r[2]),k(t,"rel","noopener noreferrer")},m(o,l){m(o,t,l),n&&n.m(t,null),s=!0},p(o,[l]){n&&n.p&&(!s||l&8)&&Ln(n,a,o,o[3],s?Rn(a,o[3],l,null):Sn(o[3]),null),(!s||l&1)&&k(t,"href",o[0]),(!s||l&2&&e!==(e="font-medium text-blue-600 dark:text-blue-500 hover:underline "+o[1]))&&k(t,"class",e),(!s||l&4)&&k(t,"target",o[2])},i(o){s||(g(n,o),s=!0)},o(o){w(n,o),s=!1},d(o){o&&f(t),n&&n.d(o)}}}function yi(r,t,e){let{$$slots:s={},$$scope:a}=t,{href:n}=t,{styling:o=""}=t,{target:l="_blank"}=t;return r.$$set=p=>{"href"in p&&e(0,n=p.href),"styling"in p&&e(1,o=p.styling),"target"in p&&e(2,l=p.target),"$$scope"in p&&e(3,a=p.$$scope)},[n,o,l,a,s]}class V extends gt{constructor(t){super(),wt(this,t,yi,vi,dt,{href:0,styling:1,target:2})}}function hi(r,t,e){const s=r.slice();return s[0]=t[e],s}function _i(r){let t;return{c(){t=h("link")},l(e){t=u(e,"link")},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function ui(r){let t,e,s=r[0].author+"",a,n,o,l=r[0].title+"",p,$,I,A=r[0].publisher+"",M,E,S,T=r[0].year+"",q,N,C,B,W,U,tt,J;return W=new V({props:{href:r[0].link,$$slots:{default:[_i]},$$scope:{ctx:r}}}),{c(){t=x("li"),e=x("span"),a=h(s),n=h(", "),o=x("span"),p=h(l),$=h(`,
			`),I=x("span"),M=h(A),E=h(`,
			`),S=x("span"),q=h(T),N=h(", "),C=x("span"),B=h("["),v(W.$$.fragment),U=h("]"),tt=h(`.
		`),this.h()},l(O){t=P(O,"LI",{});var F=R(t);e=P(F,"SPAN",{class:!0});var X=R(e);a=u(X,s),X.forEach(f),n=u(F,", "),o=P(F,"SPAN",{});var L=R(o);p=u(L,l),L.forEach(f),$=u(F,`,
			`),I=P(F,"SPAN",{});var D=R(I);M=u(D,A),D.forEach(f),E=u(F,`,
			`),S=P(F,"SPAN",{});var at=R(S);q=u(at,T),at.forEach(f),N=u(F,", "),C=P(F,"SPAN",{});var Y=R(C);B=u(Y,"["),y(W.$$.fragment,Y),U=u(Y,"]"),Y.forEach(f),tt=u(F,`.
		`),F.forEach(f),this.h()},h(){k(e,"class","ms-4")},m(O,F){m(O,t,F),d(t,e),d(e,a),d(t,n),d(t,o),d(o,p),d(t,$),d(t,I),d(I,M),d(t,E),d(t,S),d(S,q),d(t,N),d(t,C),d(C,B),_(W,C,null),d(C,U),d(t,tt),J=!0},p(O,F){const X={};F&8&&(X.$$scope={dirty:F,ctx:O}),W.$set(X)},i(O){J||(g(W.$$.fragment,O),J=!0)},o(O){w(W.$$.fragment,O),J=!1},d(O){O&&f(t),b(W)}}}function bi(r){let t,e,s=Gn(xn),a=[];for(let o=0;o<s.length;o+=1)a[o]=ui(hi(r,s,o));const n=o=>w(a[o],1,1,()=>{a[o]=null});return{c(){t=x("ol");for(let o=0;o<a.length;o+=1)a[o].c();this.h()},l(o){t=P(o,"OL",{class:!0});var l=R(t);for(let p=0;p<a.length;p+=1)a[p].l(l);l.forEach(f),this.h()},h(){k(t,"class","pl-5 my-2 text-xs list-decimal")},m(o,l){m(o,t,l);for(let p=0;p<a.length;p+=1)a[p]&&a[p].m(t,null);e=!0},p(o,[l]){if(l&0){s=Gn(xn);let p;for(p=0;p<s.length;p+=1){const $=hi(o,s,p);a[p]?(a[p].p($,l),g(a[p],1)):(a[p]=ui($),a[p].c(),g(a[p],1),a[p].m(t,null))}for(gi(),p=s.length;p<a.length;p+=1)n(p);wi()}},i(o){if(!e){for(let l=0;l<s.length;l+=1)g(a[l]);e=!0}},o(o){a=a.filter(Boolean);for(let l=0;l<a.length;l+=1)w(a[l]);e=!1},d(o){o&&f(t),$i(a,o)}}}const qn=r=>{for(let t=0;t<xn.length;t++)if(xn[t].id==r)return{index:t+1,link:xn[t].link};throw new Error},xn=[{id:"gilpin",author:"William Gilpin",title:"Model scale versus domain knoweldge in statistical forecasting of chaotic systems",publisher:"Phys. Rev. Res., vol. 5, pp. 043252, Dec",year:2023,link:"https://link.aps.org/doi/10.1103/PhysRevResearch.5.043252"},{id:"seo",author:"Seo, J., Kim, S., Jalalvand, A. et al.",title:"Avoiding fusion plasma tearing instability with deep reinforcement learning",publisher:"Nature",year:"2024",link:"https://doi.org/10.1038/s41586-024-07024-9"},{id:"degrave",author:"Jonas Degrave, Federico Felici, Jonas Buchli, Michael Neunert, Brendan Tracey, Francesco Carpanese, Timo Ewalds, Roland Hafner, et. al.",title:"Magnetic control of tokamak plasmas through deep reinforcement learning",publisher:"Nature",year:"2021",link:"https://doi.org/10.1038/s41586-021-04301-9"},{id:"challu",author:"Cristian Challu, Kin G. Olivares, Boris N. Oreshkin, Federico Garza, Max Mergenthaler-Canseco, Artur Dubrawski",title:"N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting",publisher:"arXiv:2201.12886",year:"2022",link:"https://arxiv.org/abs/2201.12886"},{id:"oreshkin",author:"Boris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, Yoshua Bengio",title:"N-BEATS: Neural Basis Expansion Analaysis for Interpretable Time Series Forecasting",publisher:"arXiv:1905.10437",year:"2019",link:"https://arxiv.org/abs/1905.10437"},{id:"osinga",author:"Hinke M. Osinga",title:"Understanding the geometry of dynamics: the stable manifold of the Lorenz system",publisher:"Journal of the Royal Society of New Zealand",year:"2018",link:"https://doi.org/10.1080/03036758.2018.1434802"}];class ki extends gt{constructor(t){super(),wt(this,t,null,bi,dt,{})}}function Ii(r){let t=qn(r[0]).index+"",e;return{c(){e=h(t)},l(s){e=u(s,t)},m(s,a){m(s,e,a)},p(s,a){a&1&&t!==(t=qn(s[0]).index+"")&&ra(e,t)},d(s){s&&f(e)}}}function Mi(r){let t,e,s,a;return e=new V({props:{href:qn(r[0]).link,$$slots:{default:[Ii]},$$scope:{ctx:r}}}),{c(){t=h("["),v(e.$$.fragment),s=h("]")},l(n){t=u(n,"["),y(e.$$.fragment,n),s=u(n,"]")},m(n,o){m(n,t,o),_(e,n,o),m(n,s,o),a=!0},p(n,[o]){const l={};o&1&&(l.href=qn(n[0]).link),o&3&&(l.$$scope={dirty:o,ctx:n}),e.$set(l)},i(n){a||(g(e.$$.fragment,n),a=!0)},o(n){w(e.$$.fragment,n),a=!1},d(n){n&&(f(t),f(s)),b(e,n)}}}function Ai(r,t,e){let{id:s}=t;return r.$$set=a=>{"id"in a&&e(0,s=a.id)},[s]}class K extends gt{constructor(t){super(),wt(this,t,Ai,Mi,dt,{id:0})}}function Ei(r){let t,e;const s=r[1].default,a=Pn(s,r,r[0],null);return{c(){t=x("div"),a&&a.c(),this.h()},l(n){t=P(n,"DIV",{class:!0});var o=R(t);a&&a.l(o),o.forEach(f),this.h()},h(){k(t,"class","text-2xl text-center mt-4 mb-2")},m(n,o){m(n,t,o),a&&a.m(t,null),e=!0},p(n,[o]){a&&a.p&&(!e||o&1)&&Ln(a,s,n,n[0],e?Rn(s,n[0],o,null):Sn(n[0]),null)},i(n){e||(g(a,n),e=!0)},o(n){w(a,n),e=!1},d(n){n&&f(t),a&&a.d(n)}}}function ji(r,t,e){let{$$slots:s={},$$scope:a}=t;return r.$$set=n=>{"$$scope"in n&&e(0,a=n.$$scope)},[a,s]}class ia extends gt{constructor(t){super(),wt(this,t,ji,Ei,dt,{})}}function zi(r){let t,e;const s=r[1].default,a=Pn(s,r,r[0],null);return{c(){t=x("div"),a&&a.c(),this.h()},l(n){t=P(n,"DIV",{class:!0});var o=R(t);a&&a.l(o),o.forEach(f),this.h()},h(){k(t,"class","text-xl text-center mt-4 mb-2")},m(n,o){m(n,t,o),a&&a.m(t,null),e=!0},p(n,[o]){a&&a.p&&(!e||o&1)&&Ln(a,s,n,n[0],e?Rn(s,n[0],o,null):Sn(n[0]),null)},i(n){e||(g(a,n),e=!0)},o(n){w(a,n),e=!1},d(n){n&&f(t),a&&a.d(n)}}}function Ti(r,t,e){let{$$slots:s={},$$scope:a}=t;return r.$$set=n=>{"$$scope"in n&&e(0,a=n.$$scope)},[a,s]}class an extends gt{constructor(t){super(),wt(this,t,Ti,zi,dt,{})}}function xi(r){let t,e,s;const a=r[3].default,n=Pn(a,r,r[2],null);return{c(){t=x("p"),n&&n.c(),this.h()},l(o){t=P(o,"P",{class:!0});var l=R(t);n&&n.l(l),l.forEach(f),this.h()},h(){k(t,"class",e="my-2 "+r[0]+" "+r[1])},m(o,l){m(o,t,l),n&&n.m(t,null),s=!0},p(o,[l]){n&&n.p&&(!s||l&4)&&Ln(n,a,o,o[2],s?Rn(a,o[2],l,null):Sn(o[2]),null),(!s||l&3&&e!==(e="my-2 "+o[0]+" "+o[1]))&&k(t,"class",e)},i(o){s||(g(n,o),s=!0)},o(o){w(n,o),s=!1},d(o){o&&f(t),n&&n.d(o)}}}function Pi(r,t,e){let{$$slots:s={},$$scope:a}=t,{indent:n="indent-8"}=t,{style:o=""}=t;return r.$$set=l=>{"indent"in l&&e(0,n=l.indent),"style"in l&&e(1,o=l.style),"$$scope"in l&&e(2,a=l.$$scope)},[n,o,a,s]}class G extends gt{constructor(t){super(),wt(this,t,Pi,xi,dt,{indent:0,style:1})}}function mi(r,t,e){const s=r.slice();return s[1]=t[e],s}function pi(r){let t,e,s=r[1].desc+"",a,n,o,l,p=r[1].val+"",$,I;return{c(){t=x("li"),e=x("div"),a=h(s),n=j(),o=x("div"),l=x("span"),$=h(p),I=j(),this.h()},l(A){t=P(A,"LI",{class:!0});var M=R(t);e=P(M,"DIV",{class:!0});var E=R(e);a=u(E,s),E.forEach(f),n=z(M),o=P(M,"DIV",{class:!0});var S=R(o);l=P(S,"SPAN",{class:!0});var T=R(l);$=u(T,p),T.forEach(f),S.forEach(f),I=z(M),M.forEach(f),this.h()},h(){k(e,"class","flex-none w-32 sm:w-64"),k(l,"class","rounded-0.5 p-1 font-mono"),k(o,"class","w-fit flex-wrap"),k(t,"class","flex items-center justify-left")},m(A,M){m(A,t,M),d(t,e),d(e,a),d(t,n),d(t,o),d(o,l),d(l,$),d(t,I)},p(A,M){M&1&&s!==(s=A[1].desc+"")&&ra(a,s),M&1&&p!==(p=A[1].val+"")&&ra($,p)},d(A){A&&f(t)}}}function Li(r){let t,e,s=Gn(r[0]),a=[];for(let n=0;n<s.length;n+=1)a[n]=pi(mi(r,s,n));return{c(){t=x("section"),e=x("ul");for(let n=0;n<a.length;n+=1)a[n].c();this.h()},l(n){t=P(n,"SECTION",{class:!0});var o=R(t);e=P(o,"UL",{class:!0});var l=R(e);for(let p=0;p<a.length;p+=1)a[p].l(l);l.forEach(f),o.forEach(f),this.h()},h(){k(e,"class","flex flex-col m-auto ps-2 rounded gap-0.5 bg-gray-100 divide-y divide-gray-200 w-fit"),k(t,"class","relative block my-4")},m(n,o){m(n,t,o),d(t,e);for(let l=0;l<a.length;l+=1)a[l]&&a[l].m(e,null)},p(n,[o]){if(o&1){s=Gn(n[0]);let l;for(l=0;l<s.length;l+=1){const p=mi(n,s,l);a[l]?a[l].p(p,o):(a[l]=pi(p),a[l].c(),a[l].m(e,null))}for(;l<a.length;l+=1)a[l].d(1);a.length=s.length}},i:H,o:H,d(n){n&&f(t),$i(a,n)}}}function Si(r,t,e){let{hps:s=[]}=t;return r.$$set=a=>{"hps"in a&&e(0,s=a.hps)},[s]}class oa extends gt{constructor(t){super(),wt(this,t,Si,Li,dt,{hps:0})}}function Ri(r){let t,e;const s=r[1].default,a=Pn(s,r,r[0],null);return{c(){t=x("figcaption"),a&&a.c(),this.h()},l(n){t=P(n,"FIGCAPTION",{class:!0});var o=R(t);a&&a.l(o),o.forEach(f),this.h()},h(){k(t,"class","text-center text-xs mt-2 mx-0 sm:mx-36")},m(n,o){m(n,t,o),a&&a.m(t,null),e=!0},p(n,[o]){a&&a.p&&(!e||o&1)&&Ln(a,s,n,n[0],e?Rn(s,n[0],o,null):Sn(n[0]),null)},i(n){e||(g(a,n),e=!0)},o(n){w(a,n),e=!1},d(n){n&&f(t),a&&a.d(n)}}}function Ni(r,t,e){let{$$slots:s={},$$scope:a}=t;return r.$$set=n=>{"$$scope"in n&&e(0,a=n.$$scope)},[a,s]}class Q extends gt{constructor(t){super(),wt(this,t,Ni,Ri,dt,{})}}function Gi(r){let t,e,s="Model scale versus domain knowledge in statistical forecasting of chaotic systems",a,n,o,l,p,$,I,A;return n=new K({props:{id:"gilpin"}}),l=new K({props:{id:"seo"}}),$=new K({props:{id:"degrave"}}),{c(){t=h(`This project is inspired by several recent publications involving the use of deep learning
		to predict or control chaotic dynamical systems, in particular William Gilpin's paper, `),e=x("i"),e.textContent=s,a=j(),v(n.$$.fragment),o=h(`. Gilpin found that, given enough training data, generic neural
		architectures can match or exceed the performance of state-of-the-art domain-specific
		choatic forecasting models such as reservoir computers and neural ODEs. Although I have
		never studied dynamical systems in depth, I have recently become highly intrigued by the
		prospect of applying deep learning to prediction tasks involving chaotic systems, as I
		explore ways to contribute to the efforts to find technical solutions to climate change.
		Along with Gilpin's paper, there have been several recent publications on the subject that
		were especially exciting to me, particularly the ones applying deep learning to tokamak
		control in nuclear fusion reactors (see e.g. `),v(l.$$.fragment),p=h(", "),v($.$$.fragment),I=h(`).
	`)},l(M){t=u(M,`This project is inspired by several recent publications involving the use of deep learning
		to predict or control chaotic dynamical systems, in particular William Gilpin's paper, `),e=P(M,"I",{"data-svelte-h":!0}),st(e)!=="svelte-1orr88l"&&(e.textContent=s),a=z(M),y(n.$$.fragment,M),o=u(M,`. Gilpin found that, given enough training data, generic neural
		architectures can match or exceed the performance of state-of-the-art domain-specific
		choatic forecasting models such as reservoir computers and neural ODEs. Although I have
		never studied dynamical systems in depth, I have recently become highly intrigued by the
		prospect of applying deep learning to prediction tasks involving chaotic systems, as I
		explore ways to contribute to the efforts to find technical solutions to climate change.
		Along with Gilpin's paper, there have been several recent publications on the subject that
		were especially exciting to me, particularly the ones applying deep learning to tokamak
		control in nuclear fusion reactors (see e.g. `),y(l.$$.fragment,M),p=u(M,", "),y($.$$.fragment,M),I=u(M,`).
	`)},m(M,E){m(M,t,E),m(M,e,E),m(M,a,E),_(n,M,E),m(M,o,E),_(l,M,E),m(M,p,E),_($,M,E),m(M,I,E),A=!0},p:H,i(M){A||(g(n.$$.fragment,M),g(l.$$.fragment,M),g($.$$.fragment,M),A=!0)},o(M){w(n.$$.fragment,M),w(l.$$.fragment,M),w($.$$.fragment,M),A=!1},d(M){M&&(f(t),f(e),f(a),f(o),f(p),f(I)),b(n,M),b(l,M),b($,M)}}}function qi(r){let t;return{c(){t=h(`My goal with this project is to get some hands-on experience with a chaotic system and
		probe deeper into Gilpin's findings by testing the limits of a neural network's ability to
		model a single chaotic system (within the computational constraints imposed by my limited
		budget*). I'll start with what is probably the most well known chaotic system, the Lorenz
		Attractor. As my dynamical systems background is a bit rusty, I will be (re)discovering many
		of the properties of the Lorenz System, and dynamical systems in general, as I go, often
		using the results of my experiments to guide my investigation. What exactly makes the Lorenz
		Attractor chaotic? And what constraints will that impose on the ability of a deep neural
		network to model it? Let's find out!`)},l(e){t=u(e,`My goal with this project is to get some hands-on experience with a chaotic system and
		probe deeper into Gilpin's findings by testing the limits of a neural network's ability to
		model a single chaotic system (within the computational constraints imposed by my limited
		budget*). I'll start with what is probably the most well known chaotic system, the Lorenz
		Attractor. As my dynamical systems background is a bit rusty, I will be (re)discovering many
		of the properties of the Lorenz System, and dynamical systems in general, as I go, often
		using the results of my experiments to guide my investigation. What exactly makes the Lorenz
		Attractor chaotic? And what constraints will that impose on the ability of a deep neural
		network to model it? Let's find out!`)},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function Di(r){let t;return{c(){t=h("mochaNN")},l(e){t=u(e,"mochaNN")},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function Ci(r){let t;return{c(){t=h("Youtube channel")},l(e){t=u(e,"Youtube channel")},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function Fi(r){let t;return{c(){t=h("lecture series")},l(e){t=u(e,"lecture series")},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function Vi(r){let t;return{c(){t=h("The Lorenz Attractor")},l(e){t=u(e,"The Lorenz Attractor")},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function Hi(r){let t;return{c(){t=h("Lorenz Attractor")},l(e){t=u(e,"Lorenz Attractor")},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function Bi(r){let t,e,s,a;return e=new V({props:{href:"https://en.wikipedia.org/wiki/Lorenz_system",$$slots:{default:[Hi]},$$scope:{ctx:r}}}),{c(){t=h(`The
		`),v(e.$$.fragment),s=h(` was developed
		by Edward Lorenz et. al. in 1963 as a simplified model of atmospheric convection.`)},l(n){t=u(n,`The
		`),y(e.$$.fragment,n),s=u(n,` was developed
		by Edward Lorenz et. al. in 1963 as a simplified model of atmospheric convection.`)},m(n,o){m(n,t,o),_(e,n,o),m(n,s,o),a=!0},p(n,o){const l={};o&128&&(l.$$scope={dirty:o,ctx:n}),e.$set(l)},i(n){a||(g(e.$$.fragment,n),a=!0)},o(n){w(e.$$.fragment,n),a=!1},d(n){n&&(f(t),f(s)),b(e,n)}}}function Oi(r){let t;return{c(){t=h("The Lorenz Attractor")},l(e){t=u(e,"The Lorenz Attractor")},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function Ui(r){let t;return{c(){t=h(`The Lorenz system is comprised of three ordinary differential equations representing the
		properties of convection and horizontal and vertical temperature in a two-dimensional fluid
		layer:`)},l(e){t=u(e,`The Lorenz system is comprised of three ordinary differential equations representing the
		properties of convection and horizontal and vertical temperature in a two-dimensional fluid
		layer:`)},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function Wi(r){let t,e,s="Attractor",a;return{c(){t=h("The Lorenz "),e=x("i"),e.textContent=s,a=h(" refers to a set of chaotic solutions to the system, most commonly:")},l(n){t=u(n,"The Lorenz "),e=P(n,"I",{"data-svelte-h":!0}),st(e)!=="svelte-7jrnvq"&&(e.textContent=s),a=u(n," refers to a set of chaotic solutions to the system, most commonly:")},m(n,o){m(n,t,o),m(n,e,o),m(n,a,o)},p:H,d(n){n&&(f(t),f(e),f(a))}}}function Ki(r){let t;return{c(){t=h("dysts")},l(e){t=u(e,"dysts")},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function Ji(r){let t,e,s,a;return e=new V({props:{href:"https://github.com/williamgilpin/dysts",$$slots:{default:[Ki]},$$scope:{ctx:r}}}),{c(){t=h("I used Gilpin's "),v(e.$$.fragment),s=h(` python module
		to generate the training data for this solution.`)},l(n){t=u(n,"I used Gilpin's "),y(e.$$.fragment,n),s=u(n,` python module
		to generate the training data for this solution.`)},m(n,o){m(n,t,o),_(e,n,o),m(n,s,o),a=!0},p(n,o){const l={};o&128&&(l.$$scope={dirty:o,ctx:n}),e.$set(l)},i(n){a||(g(e.$$.fragment,n),a=!0)},o(n){w(e.$$.fragment,n),a=!1},d(n){n&&(f(t),f(s)),b(e,n)}}}function Yi(r){let t;return{c(){t=h("Neural Architecture: N-HiTS")},l(e){t=u(e,"Neural Architecture: N-HiTS")},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function Xi(r){let t,e,s,a,n,o;return e=new K({props:{id:"challu"}}),a=new K({props:{id:"gilpin"}}),{c(){t=h("The N-HiTS "),v(e.$$.fragment),s=h(` forecasting network is known to produce state-of-the-art results,
		at the time of writing, for univariate time series prediction, with up to an order of magnitude
		lower computational requirement than some competitors. Given my limited budget and its strong
		performance reported in `),v(a.$$.fragment),n=h(`, it seemed like the natural starting point for a
		network architecture.`)},l(l){t=u(l,"The N-HiTS "),y(e.$$.fragment,l),s=u(l,` forecasting network is known to produce state-of-the-art results,
		at the time of writing, for univariate time series prediction, with up to an order of magnitude
		lower computational requirement than some competitors. Given my limited budget and its strong
		performance reported in `),y(a.$$.fragment,l),n=u(l,`, it seemed like the natural starting point for a
		network architecture.`)},m(l,p){m(l,t,p),_(e,l,p),m(l,s,p),_(a,l,p),m(l,n,p),o=!0},p:H,i(l){o||(g(e.$$.fragment,l),g(a.$$.fragment,l),o=!0)},o(l){w(e.$$.fragment,l),w(a.$$.fragment,l),o=!1},d(l){l&&(f(t),f(s),f(n)),b(e,l),b(a,l)}}}function Zi(r){let t,e,s,a,n,o;return e=new K({props:{id:"oreshkin"}}),a=new K({props:{id:"oreshkin"}}),{c(){t=h("The architectural ideas in N-HiTS build on those of its predecessor, N-BEATS "),v(e.$$.fragment),s=h(`, a neural basis expansion network for time series prediction. The key ideas inherited
		from N-BEATS include the organization of fully connected layers into blocks that output
		basis expansions (linear projections of the preceding fully connected layer's output) and
		the use of both forecast and backcast predictions from each block. The forecast predictions
		from all blocks are summed together to produce the final output of the network, while the
		backcasts are subtracted from the input of the corresponding block to produce a residual
		connection as the input to the next block. The goal of the backcasts is to help the
		downstream blocks by "removing components of their input that are not helpful for
		forecasting" `),v(a.$$.fragment),n=h(".")},l(l){t=u(l,"The architectural ideas in N-HiTS build on those of its predecessor, N-BEATS "),y(e.$$.fragment,l),s=u(l,`, a neural basis expansion network for time series prediction. The key ideas inherited
		from N-BEATS include the organization of fully connected layers into blocks that output
		basis expansions (linear projections of the preceding fully connected layer's output) and
		the use of both forecast and backcast predictions from each block. The forecast predictions
		from all blocks are summed together to produce the final output of the network, while the
		backcasts are subtracted from the input of the corresponding block to produce a residual
		connection as the input to the next block. The goal of the backcasts is to help the
		downstream blocks by "removing components of their input that are not helpful for
		forecasting" `),y(a.$$.fragment,l),n=u(l,".")},m(l,p){m(l,t,p),_(e,l,p),m(l,s,p),_(a,l,p),m(l,n,p),o=!0},p:H,i(l){o||(g(e.$$.fragment,l),g(a.$$.fragment,l),o=!0)},o(l){w(e.$$.fragment,l),w(a.$$.fragment,l),o=!1},d(l){l&&(f(t),f(s),f(n)),b(e,l),b(a,l)}}}function Qi(r){let t,e,s,a;return e=new K({props:{id:"challu"}}),{c(){t=h(`The novel ideas from N-HiTS enable the possiblity of modeling increasingly long time
		horizons while keeping computational complexity low. They include the use of pooling layers
		that downsample the inputs to each block and upsampling layers that map a compressed
		representation of the forecast to the output sample rate. In addition to the complexity
		savings, the compressed representations may induce a bias towards a temporal hierarchical
		modeling of the time series across the blocks that allows N-HiTS to exceed the performance
		of competing long-horizon forecasting models while requiring an order of magnitude lower
		computational complexity `),v(e.$$.fragment),s=h(".")},l(n){t=u(n,`The novel ideas from N-HiTS enable the possiblity of modeling increasingly long time
		horizons while keeping computational complexity low. They include the use of pooling layers
		that downsample the inputs to each block and upsampling layers that map a compressed
		representation of the forecast to the output sample rate. In addition to the complexity
		savings, the compressed representations may induce a bias towards a temporal hierarchical
		modeling of the time series across the blocks that allows N-HiTS to exceed the performance
		of competing long-horizon forecasting models while requiring an order of magnitude lower
		computational complexity `),y(e.$$.fragment,n),s=u(n,".")},m(n,o){m(n,t,o),_(e,n,o),m(n,s,o),a=!0},p:H,i(n){a||(g(e.$$.fragment,n),a=!0)},o(n){w(e.$$.fragment,n),a=!1},d(n){n&&(f(t),f(s)),b(e,n)}}}function to(r){let t;return{c(){t=h("Experiments")},l(e){t=u(e,"Experiments")},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function eo(r){let t,e,s,a;return e=new K({props:{id:"gilpin"}}),{c(){t=h(`While Gilpin's experiments focus on testing 24 different time-series prediction models on
		over 130 different chaotic systems using a relatively narrow range of hyper parameters for
		tuning `),v(e.$$.fragment),s=h(`, my experiments aim to tune a single model, N-HiTS, on a single
		system, the Lorenz Attractor, to maximize its accuracy for a given, relatively long, fixed
		horizon (aka prediction window length). And more specifically, I aim not only to achieve a
		low average error on the test set but also to limit the worst-case error as much as
		possible, which will likely mean achieving a degree of predictive power over the most
		chaotic regions of the system. Is this a completely naive aspiration given what is known
		about chaotic systems? Maybe, but I'm not really sure yet, and either way this should be a
		fun learning experience...
	`)},l(n){t=u(n,`While Gilpin's experiments focus on testing 24 different time-series prediction models on
		over 130 different chaotic systems using a relatively narrow range of hyper parameters for
		tuning `),y(e.$$.fragment,n),s=u(n,`, my experiments aim to tune a single model, N-HiTS, on a single
		system, the Lorenz Attractor, to maximize its accuracy for a given, relatively long, fixed
		horizon (aka prediction window length). And more specifically, I aim not only to achieve a
		low average error on the test set but also to limit the worst-case error as much as
		possible, which will likely mean achieving a degree of predictive power over the most
		chaotic regions of the system. Is this a completely naive aspiration given what is known
		about chaotic systems? Maybe, but I'm not really sure yet, and either way this should be a
		fun learning experience...
	`)},m(n,o){m(n,t,o),_(e,n,o),m(n,s,o),a=!0},p:H,i(n){a||(g(e.$$.fragment,n),a=!0)},o(n){w(e.$$.fragment,n),a=!1},d(n){n&&(f(t),f(s)),b(e,n)}}}function no(r){let t;return{c(){t=h("Data Generation")},l(e){t=u(e,"Data Generation")},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function so(r){let t;return{c(){t=h("dysts")},l(e){t=u(e,"dysts")},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function ao(r){let t;return{c(){t=h("IVP solver")},l(e){t=u(e,"IVP solver")},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function io(r){let t;return{c(){t=h("dysts")},l(e){t=u(e,"dysts")},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function oo(r){let t,e="$dt$",s,a,n="$0.015$",o,l,p,$,I,A,M="$dt$",E,S,T,q="after",N,C="$dt$",B,W,U,tt,J="$\\text{first_step} = 0.0001801$",O,F,X;return p=new V({props:{href:"https://github.com/williamgilpin/dysts",$$slots:{default:[so]},$$scope:{ctx:r}}}),I=new V({props:{href:"https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.solve_ivp.html",$$slots:{default:[ao]},$$scope:{ctx:r}}}),U=new V({props:{href:"https://github.com/williamgilpin/dysts",$$slots:{default:[io]},$$scope:{ctx:r}}}),{c(){t=h("I begin with a horizon (prediction window) of 100 points, using a "),s=h(e),a=h(` of approximately
		`),o=h(n),l=h(" seconds per point (the default from "),v(p.$$.fragment),$=h(") to sample the solution produced by the "),v(I.$$.fragment),A=h(". Importantly, note that this "),E=h(M),S=h(` is only the one used for sampling the solution
		`),T=x("i"),T.textContent=q,N=h(`
		it is generated by the IVP solver. The actual `),B=h(C),W=h(` used internally by the IVP solver can
		vary dynamically, but the initial target value used by `),v(U.$$.fragment),tt=h(" is "),O=h(J),F=h(`.
	`)},l(L){t=u(L,"I begin with a horizon (prediction window) of 100 points, using a "),s=u(L,e),a=u(L,` of approximately
		`),o=u(L,n),l=u(L," seconds per point (the default from "),y(p.$$.fragment,L),$=u(L,") to sample the solution produced by the "),y(I.$$.fragment,L),A=u(L,". Importantly, note that this "),E=u(L,M),S=u(L,` is only the one used for sampling the solution
		`),T=P(L,"I",{"data-svelte-h":!0}),st(T)!=="svelte-10nlrz4"&&(T.textContent=q),N=u(L,`
		it is generated by the IVP solver. The actual `),B=u(L,C),W=u(L,` used internally by the IVP solver can
		vary dynamically, but the initial target value used by `),y(U.$$.fragment,L),tt=u(L," is "),O=u(L,J),F=u(L,`.
	`)},m(L,D){m(L,t,D),m(L,s,D),m(L,a,D),m(L,o,D),m(L,l,D),_(p,L,D),m(L,$,D),_(I,L,D),m(L,A,D),m(L,E,D),m(L,S,D),m(L,T,D),m(L,N,D),m(L,B,D),m(L,W,D),_(U,L,D),m(L,tt,D),m(L,O,D),m(L,F,D),X=!0},p(L,D){const at={};D&128&&(at.$$scope={dirty:D,ctx:L}),p.$set(at);const Y={};D&128&&(Y.$$scope={dirty:D,ctx:L}),I.$set(Y);const on={};D&128&&(on.$$scope={dirty:D,ctx:L}),U.$set(on)},i(L){X||(g(p.$$.fragment,L),g(I.$$.fragment,L),g(U.$$.fragment,L),X=!0)},o(L){w(p.$$.fragment,L),w(I.$$.fragment,L),w(U.$$.fragment,L),X=!1},d(L){L&&(f(t),f(s),f(a),f(o),f(l),f($),f(A),f(E),f(S),f(T),f(N),f(B),f(W),f(tt),f(O),f(F)),b(p,L),b(I,L),b(U,L)}}}function ro(r){let t;return{c(){t=h("Lyapunov exponent")},l(e){t=u(e,"Lyapunov exponent")},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function lo(r){let t;return{c(){t=h("dysts")},l(e){t=u(e,"dysts")},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function fo(r){let t;return{c(){t=h("Lyapunov time")},l(e){t=u(e,"Lyapunov time")},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function co(r){let t,e,s,a,n,o="$0.8917$",l,p,$,I,A="$1.121s$",M,E,S;return e=new V({props:{href:"https://en.wikipedia.org/wiki/Lyapunov_exponent",$$slots:{default:[ro]},$$scope:{ctx:r}}}),a=new V({props:{href:"https://github.com/williamgilpin/dysts",$$slots:{default:[lo]},$$scope:{ctx:r}}}),$=new V({props:{href:"https://en.wikipedia.org/wiki/Lyapunov_time",$$slots:{default:[fo]},$$scope:{ctx:r}}}),{c(){t=h(`At this stage, it may also be worth mentioning one of the common metrics for measuring the
		average chaoticity of a system, the maximum
		`),v(e.$$.fragment),s=h(`. As
		reported in
		`),v(a.$$.fragment),n=h(`, the Lyapunov exponent for
		the Lorenz Attractor is approx. `),l=h(o),p=h(", and so the "),v($.$$.fragment),I=h(` is approx.
		`),M=h(A),E=h(".")},l(T){t=u(T,`At this stage, it may also be worth mentioning one of the common metrics for measuring the
		average chaoticity of a system, the maximum
		`),y(e.$$.fragment,T),s=u(T,`. As
		reported in
		`),y(a.$$.fragment,T),n=u(T,`, the Lyapunov exponent for
		the Lorenz Attractor is approx. `),l=u(T,o),p=u(T,", and so the "),y($.$$.fragment,T),I=u(T,` is approx.
		`),M=u(T,A),E=u(T,".")},m(T,q){m(T,t,q),_(e,T,q),m(T,s,q),_(a,T,q),m(T,n,q),m(T,l,q),m(T,p,q),_($,T,q),m(T,I,q),m(T,M,q),m(T,E,q),S=!0},p(T,q){const N={};q&128&&(N.$$scope={dirty:q,ctx:T}),e.$set(N);const C={};q&128&&(C.$$scope={dirty:q,ctx:T}),a.$set(C);const B={};q&128&&(B.$$scope={dirty:q,ctx:T}),$.$set(B)},i(T){S||(g(e.$$.fragment,T),g(a.$$.fragment,T),g($.$$.fragment,T),S=!0)},o(T){w(e.$$.fragment,T),w(a.$$.fragment,T),w($.$$.fragment,T),S=!1},d(T){T&&(f(t),f(s),f(n),f(l),f(p),f(I),f(M),f(E)),b(e,T),b(a,T),b($,T)}}}function ho(r){let t,e,s="on average",a,n="$e$",o,l,p="$1.121$",$,I,A="$\\frac{4}{3}$",M,E;return{c(){t=h("This tells us that, "),e=x("i"),e.textContent=s,a=h(`, the distance between any two trajectories from the
		Lorenz Attractor are expected to diverge by a factor of `),o=h(n),l=h(` after
		`),$=h(p),I=h(` seconds. Note that with these parameters, the horizon covers a time period of about
		`),M=h(A),E=h(`
		of the Lyapunov time.
	`)},l(S){t=u(S,"This tells us that, "),e=P(S,"I",{"data-svelte-h":!0}),st(e)!=="svelte-ggx8py"&&(e.textContent=s),a=u(S,`, the distance between any two trajectories from the
		Lorenz Attractor are expected to diverge by a factor of `),o=u(S,n),l=u(S,` after
		`),$=u(S,p),I=u(S,` seconds. Note that with these parameters, the horizon covers a time period of about
		`),M=u(S,A),E=u(S,`
		of the Lyapunov time.
	`)},m(S,T){m(S,t,T),m(S,e,T),m(S,a,T),m(S,o,T),m(S,l,T),m(S,$,T),m(S,I,T),m(S,M,T),m(S,E,T)},p:H,d(S){S&&(f(t),f(e),f(a),f(o),f(l),f($),f(I),f(M),f(E))}}}function uo(r){let t,e="$[-9.79, -15.04, 20.53]$",s,a,n="$[0.99,1.01]$",o,l;return{c(){t=h(`The train and test sets are comprised of many trajectories with initial conditions all
		centered at approx. `),s=h(e),a=h(` and multiplied by a random perturbation uniformly
		sampled from the interval `),o=h(n),l=h(".")},l(p){t=u(p,`The train and test sets are comprised of many trajectories with initial conditions all
		centered at approx. `),s=u(p,e),a=u(p,` and multiplied by a random perturbation uniformly
		sampled from the interval `),o=u(p,n),l=u(p,".")},m(p,$){m(p,t,$),m(p,s,$),m(p,a,$),m(p,o,$),m(p,l,$)},p:H,d(p){p&&(f(t),f(s),f(a),f(o),f(l))}}}function mo(r){let t,e="$3*100 = 300$",s,a,n="$3 * (500 + 100) = 1800$",o,l;return{c(){t=h(`The input to the N-HiTs model is a lookback window of the previous series values whose
		length is typically some multiple of the horizon window. I went with the default value from
		the N-HiTS paper of 5 times the horizon window length, or 500 points, making each training
		sample a total of 600 points. (Note that because N-HiTs is a univariate model, while the
		Lorenz System is three-dimensional, the data points must be flattened into one dimension.
		Therefore, the horizon window length is actually `),s=h(e),a=h(`, and each training
		sample's length is `),o=h(n),l=h(").")},l(p){t=u(p,`The input to the N-HiTs model is a lookback window of the previous series values whose
		length is typically some multiple of the horizon window. I went with the default value from
		the N-HiTS paper of 5 times the horizon window length, or 500 points, making each training
		sample a total of 600 points. (Note that because N-HiTs is a univariate model, while the
		Lorenz System is three-dimensional, the data points must be flattened into one dimension.
		Therefore, the horizon window length is actually `),s=u(p,e),a=u(p,`, and each training
		sample's length is `),o=u(p,n),l=u(p,").")},m(p,$){m(p,t,$),m(p,s,$),m(p,a,$),m(p,o,$),m(p,l,$)},p:H,d(p){p&&(f(t),f(s),f(a),f(o),f(l))}}}function po(r){let t,e=`$10,000 - 600 +
		1 = 9401$`,s,a;return{c(){t=h(`I choose, somewhat arbitrarily, to generate 10,000 points per series, and in order to
		increase data efficiency, I select each training sample by sliding the 600-point window
		along the series with a one-point stride. Each series, therefore, contributes `),s=h(e),a=h(` training samples. For the initial experiment, I generate 25 series with unique initial conditions,
		and train on 19 of them, and hold out 3 series for validation and 3 series for testing.`)},l(n){t=u(n,`I choose, somewhat arbitrarily, to generate 10,000 points per series, and in order to
		increase data efficiency, I select each training sample by sliding the 600-point window
		along the series with a one-point stride. Each series, therefore, contributes `),s=u(n,e),a=u(n,` training samples. For the initial experiment, I generate 25 series with unique initial conditions,
		and train on 19 of them, and hold out 3 series for validation and 3 series for testing.`)},m(n,o){m(n,t,o),m(n,s,o),m(n,a,o)},p:H,d(n){n&&(f(t),f(s),f(a))}}}function $o(r){let t;return{c(){t=h("Model 1")},l(e){t=u(e,"Model 1")},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function go(r){let t,e,s,a,n,o;return e=new K({props:{id:"challu"}}),a=new K({props:{id:"gilpin"}}),{c(){t=h("The model is optimized with MAE loss, consistent with the default loss from "),v(e.$$.fragment),s=h(`. For evaluation, I use the symmetric mean absolute percentage error (sMAPE) as defined in
		`),v(a.$$.fragment),n=h(":")},l(l){t=u(l,"The model is optimized with MAE loss, consistent with the default loss from "),y(e.$$.fragment,l),s=u(l,`. For evaluation, I use the symmetric mean absolute percentage error (sMAPE) as defined in
		`),y(a.$$.fragment,l),n=u(l,":")},m(l,p){m(l,t,p),_(e,l,p),m(l,s,p),_(a,l,p),m(l,n,p),o=!0},p:H,i(l){o||(g(e.$$.fragment,l),g(a.$$.fragment,l),o=!0)},o(l){w(e.$$.fragment,l),w(a.$$.fragment,l),o=!1},d(l){l&&(f(t),f(s),f(n)),b(e,l),b(a,l)}}}function wo(r){let t;return{c(){t=h(`In this formulation, sMAPE is bound to the interval [0, 200]. The distribution of average
		window errors and its CDF on the test set are shown below. Note that the left y axis is
		log-scaled.`)},l(e){t=u(e,`In this formulation, sMAPE is bound to the interval [0, 200]. The distribution of average
		window errors and its CDF on the test set are shown below. Note that the left y axis is
		log-scaled.`)},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function vo(r){let t;return{c(){t=h("Model 1 - sMAPE error distribution on the test set")},l(e){t=u(e,"Model 1 - sMAPE error distribution on the test set")},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function yo(r){let t;return{c(){t=h(`To gain a more intuitive understanding of the magnitude of these errors, we can plot
		individual window predictions against the references:`)},l(e){t=u(e,`To gain a more intuitive understanding of the magnitude of these errors, we can plot
		individual window predictions against the references:`)},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function _o(r){let t;return{c(){t=h(`Samples of three different 100-point predictions from Model 1 with small, medium, and
			large sMAPE errors`)},l(e){t=u(e,`Samples of three different 100-point predictions from Model 1 with small, medium, and
			large sMAPE errors`)},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function bo(r){let t;return{c(){t=h(`One interesting observation in all three graphs is that there appears to be a kind of
		"point of divergence" on the prediction before which the average error is very low and after
		which the error grows quickly. In the first graph, this point is about in the middle of the
		prediction, in the second it is maybe one third of the way into the prediction, and in the
		third it is near the beginning. If we look at the predictions of adjacent windows, we see
		that the behavior at this point is consisent across the windows, indicating that there is
		something about the system's behavior in this region that is very difficult for this model
		to fit, regardless of its alignment within the prediction window.`)},l(e){t=u(e,`One interesting observation in all three graphs is that there appears to be a kind of
		"point of divergence" on the prediction before which the average error is very low and after
		which the error grows quickly. In the first graph, this point is about in the middle of the
		prediction, in the second it is maybe one third of the way into the prediction, and in the
		third it is near the beginning. If we look at the predictions of adjacent windows, we see
		that the behavior at this point is consisent across the windows, indicating that there is
		something about the system's behavior in this region that is very difficult for this model
		to fit, regardless of its alignment within the prediction window.`)},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function ko(r){let t;return{c(){t=h(`The behavior of the model near the origin, which is a critical point of the system, for
			an especially challenging case. In contrast to all other regions of this trajectory, the
			model seems highly uncertain of how the trajectory will evolve shortly after passing
			near the origin.`)},l(e){t=u(e,`The behavior of the model near the origin, which is a critical point of the system, for
			an especially challenging case. In contrast to all other regions of this trajectory, the
			model seems highly uncertain of how the trajectory will evolve shortly after passing
			near the origin.`)},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function Io(r){let t;return{c(){t=h("critical points")},l(e){t=u(e,"critical points")},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function Mo(r){let t,e,s,a,n,o,l,p;return e=new V({props:{href:"https://en.wikipedia.org/wiki/Critical_point_(mathematics)",$$slots:{default:[Io]},$$scope:{ctx:r}}}),a=new K({props:{id:"osinga"}}),o=new K({props:{id:"osinga"}}),{c(){t=h(`For anyone familiar with dynamical systems theory, it won't be a surprise that this point
		coincides with one of the three `),v(e.$$.fragment),s=h(` of the Lorenz system--in this case, the origin. And in this parameterization, the origin is
		known to be a saddle point comprised of the intersection of a stable 2D manifold and an unstable
		1D manifold. "Stable" means that trajectories near the manifold tend to move towards it even
		when they are perturbed slightly away from it by other forces, while "unstable" implies the opposite.
		(See `),v(a.$$.fragment),n=h(` for some excellent visualizations of these manifolds.) Near the origin,
		the unstable manifold is a line that is approximately perpendicular to the Z axis and parallel
		to the lengthwise orientation of the Attractor, which is why the trajectories always diverge
		at the near 90-degree angles that we see in the animations as they approach the origin. And the
		(incredibly complex) topography of the stable 2D manifold determines towards which of the other
		two critical points a trajectory will be deflected `),v(o.$$.fragment),l=h(`. From this I conclude
		that, technically, we can say that the primary goal of our neural network is to learn the
		topography of the origin's stable 2D manifold. As the manifold defines a boundary across
		which trajectories can never pass, we can confine the past and future trajectory of any
		point to be within the boundary.
	`)},l($){t=u($,`For anyone familiar with dynamical systems theory, it won't be a surprise that this point
		coincides with one of the three `),y(e.$$.fragment,$),s=u($,` of the Lorenz system--in this case, the origin. And in this parameterization, the origin is
		known to be a saddle point comprised of the intersection of a stable 2D manifold and an unstable
		1D manifold. "Stable" means that trajectories near the manifold tend to move towards it even
		when they are perturbed slightly away from it by other forces, while "unstable" implies the opposite.
		(See `),y(a.$$.fragment,$),n=u($,` for some excellent visualizations of these manifolds.) Near the origin,
		the unstable manifold is a line that is approximately perpendicular to the Z axis and parallel
		to the lengthwise orientation of the Attractor, which is why the trajectories always diverge
		at the near 90-degree angles that we see in the animations as they approach the origin. And the
		(incredibly complex) topography of the stable 2D manifold determines towards which of the other
		two critical points a trajectory will be deflected `),y(o.$$.fragment,$),l=u($,`. From this I conclude
		that, technically, we can say that the primary goal of our neural network is to learn the
		topography of the origin's stable 2D manifold. As the manifold defines a boundary across
		which trajectories can never pass, we can confine the past and future trajectory of any
		point to be within the boundary.
	`)},m($,I){m($,t,I),_(e,$,I),m($,s,I),_(a,$,I),m($,n,I),_(o,$,I),m($,l,I),p=!0},p($,I){const A={};I&128&&(A.$$scope={dirty:I,ctx:$}),e.$set(A)},i($){p||(g(e.$$.fragment,$),g(a.$$.fragment,$),g(o.$$.fragment,$),p=!0)},o($){w(e.$$.fragment,$),w(a.$$.fragment,$),w(o.$$.fragment,$),p=!1},d($){$&&(f(t),f(s),f(n),f(l)),b(e,$),b(a,$),b(o,$)}}}function Ao(r){let t,e="$f(t) = \\exp(\\lambda t)$",s,a,n="$\\lambda$",o,l,p="$11.8$",$,I;return{c(){t=h(`We can estimate how unstable the 1D manifold is by calculating the eigenvalues of the
		Jacobian matrix of the system at the origin and assuming the dynamics are approximately
		linear in this region. The eigenvalue with the largest real component dominates the
		dynamics, and by definition, the real part of this eigenvalue for an unstable mainfold will
		be positive, with the dynamics along the manifold being approximated by the expression `),s=h(e),a=h(`,
		where `),o=h(n),l=h(` equals the real part of the largest eigenvalue. For the Lorenz Attractor,
		this value is `),$=h(p),I=h(`, so trajectories will be rapidly deflected away from the origin, as
		we see in the below animation:`)},l(A){t=u(A,`We can estimate how unstable the 1D manifold is by calculating the eigenvalues of the
		Jacobian matrix of the system at the origin and assuming the dynamics are approximately
		linear in this region. The eigenvalue with the largest real component dominates the
		dynamics, and by definition, the real part of this eigenvalue for an unstable mainfold will
		be positive, with the dynamics along the manifold being approximated by the expression `),s=u(A,e),a=u(A,`,
		where `),o=u(A,n),l=u(A,` equals the real part of the largest eigenvalue. For the Lorenz Attractor,
		this value is `),$=u(A,p),I=u(A,`, so trajectories will be rapidly deflected away from the origin, as
		we see in the below animation:`)},m(A,M){m(A,t,M),m(A,s,M),m(A,a,M),m(A,o,M),m(A,l,M),m(A,$,M),m(A,I,M)},p:H,d(A){A&&(f(t),f(s),f(a),f(o),f(l),f($),f(I))}}}function Eo(r){let t;return{c(){t=h(`The trajectories from the training set all begin at nearly the same point but quickly
			diverge as they approach the critical point at the origin.`)},l(e){t=u(e,`The trajectories from the training set all begin at nearly the same point but quickly
			diverge as they approach the critical point at the origin.`)},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function jo(r){let t;return{c(){t=h(`Given all of this background, it is now unsurprising that the model is struggling to
		predict the behavior of the system near the origin. But we should also note that the model
		does quite well at predicting basically every other region of the system. We just have to
		figure out a way to improve the predictions near the origin, and then we should have a model
		with an overall very robust representation of the Lorenz Attractor. As this model and its
		training set are relatively modest in size, the next most obvious step to try is to
		signifcantly increase both the amount of training data and the model's capacity, and see if
		those changes alone are enough to resolve the weaknesses of Model 1.`)},l(e){t=u(e,`Given all of this background, it is now unsurprising that the model is struggling to
		predict the behavior of the system near the origin. But we should also note that the model
		does quite well at predicting basically every other region of the system. We just have to
		figure out a way to improve the predictions near the origin, and then we should have a model
		with an overall very robust representation of the Lorenz Attractor. As this model and its
		training set are relatively modest in size, the next most obvious step to try is to
		signifcantly increase both the amount of training data and the model's capacity, and see if
		those changes alone are enough to resolve the weaknesses of Model 1.`)},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function zo(r){let t;return{c(){t=h("Model 2")},l(e){t=u(e,"Model 2")},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function To(r){let t;return{c(){t=h(`For the next model, I increased the number of unique initial conditions from 25 to 10000,
		and held out 100 for validation and 200 for testing, leaving 9700 unique initial conditions,
		each of length 10,000 points, or about 150 seconds, in the training set. I also expanded the
		range of hyperparameters for tuning to include significantly larger models, both in depth
		and width. After tuning, I arrived at the following settings:`)},l(e){t=u(e,`For the next model, I increased the number of unique initial conditions from 25 to 10000,
		and held out 100 for validation and 200 for testing, leaving 9700 unique initial conditions,
		each of length 10,000 points, or about 150 seconds, in the training set. I also expanded the
		range of hyperparameters for tuning to include significantly larger models, both in depth
		and width. After tuning, I arrived at the following settings:`)},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function xo(r){let t;return{c(){t=h("Model 2 vs Model 1 - sMAPE error distribution.")},l(e){t=u(e,"Model 2 vs Model 1 - sMAPE error distribution.")},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function Po(r){let t;return{c(){t=h(`From the plot, we see a significant increase in the first bin and a reduction in every
		subsequent bin of the per-window error histogram relative to Model 1, so the larger dataset
		and new hyperparameter tunings have a definite and significant positive impact. 99% of
		windows from Model 2 have a sMAPE less than 6, compared to only 74% for Model 1, and 99.9%
		have a sMAPE less than 40, compared to 98% for Model 1.`)},l(e){t=u(e,`From the plot, we see a significant increase in the first bin and a reduction in every
		subsequent bin of the per-window error histogram relative to Model 1, so the larger dataset
		and new hyperparameter tunings have a definite and significant positive impact. 99% of
		windows from Model 2 have a sMAPE less than 6, compared to only 74% for Model 1, and 99.9%
		have a sMAPE less than 40, compared to 98% for Model 1.`)},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function Lo(r){let t;return{c(){t=h(`There are, however, still a handful of windows with very large sMAPE errors. We can
		visualize these errors slightly differently to get a better sense of how they are
		distributed within and across the test series:`)},l(e){t=u(e,`There are, however, still a handful of windows with very large sMAPE errors. We can
		visualize these errors slightly differently to get a better sense of how they are
		distributed within and across the test series:`)},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function So(r){let t;return{c(){t=h("Model 2 - sMAPE errors per series per window in the test set.")},l(e){t=u(e,"Model 2 - sMAPE errors per series per window in the test set.")},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function Ro(r){let t;return{c(){t=h(`We see that very large errors occur quite rarely and briefly, with the predictions spending
		most of the time near the ground truth. Let's check the animation for one of the large
		spikes with a sMAPE greater than 100:`)},l(e){t=u(e,`We see that very large errors occur quite rarely and briefly, with the predictions spending
		most of the time near the ground truth. Let's check the animation for one of the large
		spikes with a sMAPE greater than 100:`)},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function No(r){let t;return{c(){t=h(`Model 2 - a trajectory with one of the largest sMAPE errors from the test set. DFO =
			'distance from origin'`)},l(e){t=u(e,`Model 2 - a trajectory with one of the largest sMAPE errors from the test set. DFO =
			'distance from origin'`)},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function Go(r){let t;return{c(){t=h(`Not surprisingly, this trajectory passes very close to the origin, and we immediately see
		how similar this failure case is to the one from Model 1. Despite the average improvement
		across all error magnitudes, has the model's ability to predict the behavior near the
		unstable origin actually improved significantly relative to Model 1? Let's check:`)},l(e){t=u(e,`Not surprisingly, this trajectory passes very close to the origin, and we immediately see
		how similar this failure case is to the one from Model 1. Despite the average improvement
		across all error magnitudes, has the model's ability to predict the behavior near the
		unstable origin actually improved significantly relative to Model 1? Let's check:`)},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function qo(r){let t,e="$n$",s,a,n="$n-1$",o,l,p="$n+1$",$,I;return{c(){t=h(`Each local minimum distance from the origin on the ground truth trajectories is
			calculated, and the corresponding maximum sMAPE error among the windows that included
			the minimum in its target is shown. A local minimum is defined as a point at time `),s=h(e),a=h(`
			that is closer to the origin than the points at `),o=h(n),l=h(" and "),$=h(p),I=h(".")},l(A){t=u(A,`Each local minimum distance from the origin on the ground truth trajectories is
			calculated, and the corresponding maximum sMAPE error among the windows that included
			the minimum in its target is shown. A local minimum is defined as a point at time `),s=u(A,e),a=u(A,`
			that is closer to the origin than the points at `),o=u(A,n),l=u(A," and "),$=u(A,p),I=u(A,".")},m(A,M){m(A,t,M),m(A,s,M),m(A,a,M),m(A,o,M),m(A,l,M),m(A,$,M),m(A,I,M)},p:H,d(A){A&&(f(t),f(s),f(a),f(o),f(l),f($),f(I))}}}function Do(r){let t;return{c(){t=h(`As we can clearly see from the plot, Model 2 is able to predict points that are closer to
		the origin significantly more accurately than Model 1. So although Model 2 is not able to
		avoid catastraphic failure for all points, it has indeed reduced the number of points for
		which these failures occur.`)},l(e){t=u(e,`As we can clearly see from the plot, Model 2 is able to predict points that are closer to
		the origin significantly more accurately than Model 1. So although Model 2 is not able to
		avoid catastraphic failure for all points, it has indeed reduced the number of points for
		which these failures occur.`)},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function Co(r){let t,e="$dt$",s,a,n="$dt$",o,l,p="$\\approx0.015$",$,I,A="$\\approx0.003$",M,E,S="$\\approx1.5$",T,q;return{c(){t=h(`So we've drastically increased both model capacity and dataset size, and we have only
		achieved marginal improvement on the most chaotic trajectories. To continue to make
		progress, we probably need to try a different approach. One idea is to increase the temporal
		resolution of the model by using a smaller `),s=h(e),a=h(` to generate our data points. So far we've
		used a `),o=h(n),l=h(" of "),$=h(p),I=h(". Let's try reducing that by a factor of 5 to "),M=h(A),E=h(`,
		and in order to keep the prediction task equally difficult, we'll also increase the horizon
		window, and lookback window, by a factor of 5 to 500 and 2500 respectively, so that the
		total amount of time being predicted is still `),T=h(S),q=h(` seconds. We'll call this Model
		3.`)},l(N){t=u(N,`So we've drastically increased both model capacity and dataset size, and we have only
		achieved marginal improvement on the most chaotic trajectories. To continue to make
		progress, we probably need to try a different approach. One idea is to increase the temporal
		resolution of the model by using a smaller `),s=u(N,e),a=u(N,` to generate our data points. So far we've
		used a `),o=u(N,n),l=u(N," of "),$=u(N,p),I=u(N,". Let's try reducing that by a factor of 5 to "),M=u(N,A),E=u(N,`,
		and in order to keep the prediction task equally difficult, we'll also increase the horizon
		window, and lookback window, by a factor of 5 to 500 and 2500 respectively, so that the
		total amount of time being predicted is still `),T=u(N,S),q=u(N,` seconds. We'll call this Model
		3.`)},m(N,C){m(N,t,C),m(N,s,C),m(N,a,C),m(N,o,C),m(N,l,C),m(N,$,C),m(N,I,C),m(N,M,C),m(N,E,C),m(N,T,C),m(N,q,C)},p:H,d(N){N&&(f(t),f(s),f(a),f(o),f(l),f($),f(I),f(M),f(E),f(T),f(q))}}}function Fo(r){let t;return{c(){t=h("Model 3")},l(e){t=u(e,"Model 3")},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function Vo(r){let t;return{c(){t=h("FSDP Strategy")},l(e){t=u(e,"FSDP Strategy")},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function Ho(r){let t,e,s,a,n,o;return a=new V({props:{href:"https://lightning.ai/docs/pytorch/stable/advanced/model_parallel/fsdp.html",$$slots:{default:[Vo]},$$scope:{ctx:r}}}),{c(){t=h("A sidenote on the practicality of training this model: "),e=x("p"),s=h(`Although we have not increased the number of parameters relative to Model 2, by
			increasing the input size and horizon length by a factor of 5, we have significantly
			increased the memory requirement for training this model. Now in order to fit the model
			on two GPUs with 16 GB of RAM each, I have to use Lightning's `),v(a.$$.fragment),n=h(` to distribute the model across both GPUs and get the per-GPU memory requirement to be just
			a hair under 16 GB. This also means that the model trains significantly more slowly, taking
			about 40 hours to converge, compared to about 16 hours for Model 2.`),this.h()},l(l){t=u(l,"A sidenote on the practicality of training this model: "),e=P(l,"P",{class:!0});var p=R(e);s=u(p,`Although we have not increased the number of parameters relative to Model 2, by
			increasing the input size and horizon length by a factor of 5, we have significantly
			increased the memory requirement for training this model. Now in order to fit the model
			on two GPUs with 16 GB of RAM each, I have to use Lightning's `),y(a.$$.fragment,p),n=u(p,` to distribute the model across both GPUs and get the per-GPU memory requirement to be just
			a hair under 16 GB. This also means that the model trains significantly more slowly, taking
			about 40 hours to converge, compared to about 16 hours for Model 2.`),p.forEach(f),this.h()},h(){k(e,"class","ms-8")},m(l,p){m(l,t,p),m(l,e,p),d(e,s),_(a,e,null),d(e,n),o=!0},p(l,p){const $={};p&128&&($.$$scope={dirty:p,ctx:l}),a.$set($)},i(l){o||(g(a.$$.fragment,l),o=!0)},o(l){w(a.$$.fragment,l),o=!1},d(l){l&&(f(t),f(e)),b(a)}}}function Bo(r){let t,e="$dt \\approx 0.003$",s,a,n="$\\lt80$",o,l;return{c(){t=h(`After retraining Model 2 with a new dataset that samples the Lorenz Attractor trajectories
		with `),s=h(e),a=h(`, we see that we are now able to predict all regions of the test
		set with sMAPE error `),o=h(n),l=h(":")},l(p){t=u(p,`After retraining Model 2 with a new dataset that samples the Lorenz Attractor trajectories
		with `),s=u(p,e),a=u(p,`, we see that we are now able to predict all regions of the test
		set with sMAPE error `),o=u(p,n),l=u(p,":")},m(p,$){m(p,t,$),m(p,s,$),m(p,a,$),m(p,o,$),m(p,l,$)},p:H,d(p){p&&(f(t),f(s),f(a),f(o),f(l))}}}function Oo(r){let t;return{c(){t=h("Model 3 vs. Model 2 - sMAPE error distribution.")},l(e){t=u(e,"Model 3 vs. Model 2 - sMAPE error distribution.")},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function Uo(r){let t;return{c(){t=h(`The most challenging trajectories from the test set are significantly improved, although
		still far from perfect:`)},l(e){t=u(e,`The most challenging trajectories from the test set are significantly improved, although
		still far from perfect:`)},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function Wo(r){let t;return{c(){t=h(`The maximum-error trajectory from the test set for Model 3. Although there is still lots
			of room for improvement, the predictions now at least roughly track the general contour
			of the ground truth.`)},l(e){t=u(e,`The maximum-error trajectory from the test set for Model 3. Although there is still lots
			of room for improvement, the predictions now at least roughly track the general contour
			of the ground truth.`)},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function Ko(r){let t,e="$dt \\approx 0.015$",s,a,n="$dt \\approx 0.003$",o,l;return{c(){t=h(`Based on the results of Model 3, we can conclude that a primary limiting factor with
		previous models was the temporal resolution of the trajectory's history; the information
		required to make an accurate prediction for the most challenging trajectories is apparently
		not contained in trajectories with a sample period of `),s=h(e),a=h(`, but much more
		of it is contained in trajectories with a sample period of `),o=h(n),l=h(".")},l(p){t=u(p,`Based on the results of Model 3, we can conclude that a primary limiting factor with
		previous models was the temporal resolution of the trajectory's history; the information
		required to make an accurate prediction for the most challenging trajectories is apparently
		not contained in trajectories with a sample period of `),s=u(p,e),a=u(p,`, but much more
		of it is contained in trajectories with a sample period of `),o=u(p,n),l=u(p,".")},m(p,$){m(p,t,$),m(p,s,$),m(p,a,$),m(p,o,$),m(p,l,$)},p:H,d(p){p&&(f(t),f(s),f(a),f(o),f(l))}}}function Jo(r){let t;return{c(){t=h("Autoregressive Generation")},l(e){t=u(e,"Autoregressive Generation")},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function Yo(r){let t,e="$\\approx 7.5$",s,a;return{c(){t=h("Now that we have a model that adequately approximates the ODE given the last "),s=h(e),a=h(`
		seconds of the IVP solver's output, the next test is to measure how well the model continues
		to predict the trajectory given its own past predictions. To do this, for each trajectory in
		the test set, we will begin by using the first 2500 points to produce the model's prediction
		for points 2501-3000. Then we'll feed those 500 points back into the input to predict points
		3001-3500, and continue in this way for all 10,000 points in each trajectory. Then we can compare
		how closely the predicted trajectories match the ones produced by the IVP solver.
	`)},l(n){t=u(n,"Now that we have a model that adequately approximates the ODE given the last "),s=u(n,e),a=u(n,`
		seconds of the IVP solver's output, the next test is to measure how well the model continues
		to predict the trajectory given its own past predictions. To do this, for each trajectory in
		the test set, we will begin by using the first 2500 points to produce the model's prediction
		for points 2501-3000. Then we'll feed those 500 points back into the input to predict points
		3001-3500, and continue in this way for all 10,000 points in each trajectory. Then we can compare
		how closely the predicted trajectories match the ones produced by the IVP solver.
	`)},m(n,o){m(n,t,o),m(n,s,o),m(n,a,o)},p:H,d(n){n&&(f(t),f(s),f(a))}}}function Xo(r){let t,e="$\\approx7.2$",s,a;return{c(){t=h("When we do this, we find that Model 3 is, on average, able to predict the first "),s=h(e),a=h(`
		seconds of the trajectory before it begins to diverge significantly from the reference (I arrived
		at this by calculating the mean time at which the L2 distance between the trajectories exceeds
		3). But we also note that, although there are clearly visible differences between the reference
		and the prediction, the full 10,000-point trajectories that Model 3 predicts are, to the naked
		eye at least, more or less indistinguishable from the typical trajectories of the Lorenz Attractor.
		In other words, they look like entirely plausible trajectories even if they eventually diverge
		significantly from the ones produced by the IVP solver for the same initial conditions.`)},l(n){t=u(n,"When we do this, we find that Model 3 is, on average, able to predict the first "),s=u(n,e),a=u(n,`
		seconds of the trajectory before it begins to diverge significantly from the reference (I arrived
		at this by calculating the mean time at which the L2 distance between the trajectories exceeds
		3). But we also note that, although there are clearly visible differences between the reference
		and the prediction, the full 10,000-point trajectories that Model 3 predicts are, to the naked
		eye at least, more or less indistinguishable from the typical trajectories of the Lorenz Attractor.
		In other words, they look like entirely plausible trajectories even if they eventually diverge
		significantly from the ones produced by the IVP solver for the same initial conditions.`)},m(n,o){m(n,t,o),m(n,s,o),m(n,a,o)},p:H,d(n){n&&(f(t),f(s),f(a))}}}function Zo(r){let t;return{c(){t=h(`Comparison of trajectories generated by the IVP solver (left) and auto-regressively
			generated by Model 3 (right). Each row uses the same initial conditions.`)},l(e){t=u(e,`Comparison of trajectories generated by the IVP solver (left) and auto-regressively
			generated by Model 3 (right). Each row uses the same initial conditions.`)},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function Qo(r){let t;return{c(){t=h("shadowing lemma")},l(e){t=u(e,"shadowing lemma")},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function tr(r){let t,e,s,a;return e=new V({props:{href:"https://en.wikipedia.org/wiki/Shadowing_lemma",$$slots:{default:[Qo]},$$scope:{ctx:r}}}),{c(){t=h(`Is there a way to confirm this observation more rigorously than with the eye test alone?
		I'm not sure, and I'll have to leave that question for future work. But it's also crucial to
		note that different IVP solvers also produce diverging trajectories in much the same manner
		as this. In fact, all numerical solutions to chaotic equations are known to diverge from the
		true solution due to the rounding error introduced by finite precision. The `),v(e.$$.fragment),s=h(` tells us that, in spite of this, the trajectories produced by IVP solvers still remain arbitrarily
		close to real trajectories from the ODE even if they do not exactly represent the ones that would
		be produced by the given initial conditions.
	`)},l(n){t=u(n,`Is there a way to confirm this observation more rigorously than with the eye test alone?
		I'm not sure, and I'll have to leave that question for future work. But it's also crucial to
		note that different IVP solvers also produce diverging trajectories in much the same manner
		as this. In fact, all numerical solutions to chaotic equations are known to diverge from the
		true solution due to the rounding error introduced by finite precision. The `),y(e.$$.fragment,n),s=u(n,` tells us that, in spite of this, the trajectories produced by IVP solvers still remain arbitrarily
		close to real trajectories from the ODE even if they do not exactly represent the ones that would
		be produced by the given initial conditions.
	`)},m(n,o){m(n,t,o),_(e,n,o),m(n,s,o),a=!0},p(n,o){const l={};o&128&&(l.$$scope={dirty:o,ctx:n}),e.$set(l)},i(n){a||(g(e.$$.fragment,n),a=!0)},o(n){w(e.$$.fragment,n),a=!1},d(n){n&&(f(t),f(s)),b(e,n)}}}function er(r){let t;return{c(){t=h("dysts")},l(e){t=u(e,"dysts")},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function nr(r){let t;return{c(){t=h("Radau")},l(e){t=u(e,"Radau")},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function sr(r){let t;return{c(){t=h("RK45")},l(e){t=u(e,"RK45")},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function ar(r){let t,e,s,a,n,o,l,p;return e=new V({props:{href:"https://github.com/williamgilpin/dysts",$$slots:{default:[er]},$$scope:{ctx:r}}}),a=new V({props:{href:"https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.Radau.html",$$slots:{default:[nr]},$$scope:{ctx:r}}}),o=new V({props:{href:"https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.RK45.html#scipy.integrate.RK45",$$slots:{default:[sr]},$$scope:{ctx:r}}}),{c(){t=h(`In light of this, another way to evaluate the autoregressive output of our model is to
		compare it with the output from a different IVP solver with similar error constraints. `),v(e.$$.fragment),s=h(" uses the "),v(a.$$.fragment),n=h(" solver by default, and this is what I used to generate the dataset. "),v(o.$$.fragment),l=h(` has similar error constraints to Radau, so let's compare our autoregressive output against
		Radau relative to RK45's output against Radau:`)},l($){t=u($,`In light of this, another way to evaluate the autoregressive output of our model is to
		compare it with the output from a different IVP solver with similar error constraints. `),y(e.$$.fragment,$),s=u($," uses the "),y(a.$$.fragment,$),n=u($," solver by default, and this is what I used to generate the dataset. "),y(o.$$.fragment,$),l=u($,` has similar error constraints to Radau, so let's compare our autoregressive output against
		Radau relative to RK45's output against Radau:`)},m($,I){m($,t,I),_(e,$,I),m($,s,I),_(a,$,I),m($,n,I),_(o,$,I),m($,l,I),p=!0},p($,I){const A={};I&128&&(A.$$scope={dirty:I,ctx:$}),e.$set(A);const M={};I&128&&(M.$$scope={dirty:I,ctx:$}),a.$set(M);const E={};I&128&&(E.$$scope={dirty:I,ctx:$}),o.$set(E)},i($){p||(g(e.$$.fragment,$),g(a.$$.fragment,$),g(o.$$.fragment,$),p=!0)},o($){w(e.$$.fragment,$),w(a.$$.fragment,$),w(o.$$.fragment,$),p=!1},d($){$&&(f(t),f(s),f(n),f(l)),b(e,$),b(a,$),b(o,$)}}}function ir(r){let t;return{c(){t=h("solve_ivp")},l(e){t=u(e,"solve_ivp")},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function or(r){let t,e,s,a;return e=new V({props:{href:"https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.solve_ivp.html",$$slots:{default:[ir]},$$scope:{ctx:r}}}),{c(){t=h(`Comparing the mean absolute error per timestep between Radau and Model 3 (blue) and
			Radua and RK45 (orange). The error is averaged across 200 different trajectories.
			Scipy's `),v(e.$$.fragment),s=h(" is used to produce the IVP solver outputs.")},l(n){t=u(n,`Comparing the mean absolute error per timestep between Radau and Model 3 (blue) and
			Radua and RK45 (orange). The error is averaged across 200 different trajectories.
			Scipy's `),y(e.$$.fragment,n),s=u(n," is used to produce the IVP solver outputs.")},m(n,o){m(n,t,o),_(e,n,o),m(n,s,o),a=!0},p(n,o){const l={};o&128&&(l.$$scope={dirty:o,ctx:n}),e.$set(l)},i(n){a||(g(e.$$.fragment,n),a=!0)},o(n){w(e.$$.fragment,n),a=!1},d(n){n&&(f(t),f(s)),b(e,n)}}}function rr(r){let t;return{c(){t=h(`So we can say that our model is approximating the output of Radau more closely than another
		high-quality IVP solver. Ultimately, all three solvers diverge chaotically from each other,
		but in the short term, Model 3 remains closer to Radau than RK45. From this I tenatively
		conclude that our model is an effective IVP solver of the Lorenz Attractor.`)},l(e){t=u(e,`So we can say that our model is approximating the output of Radau more closely than another
		high-quality IVP solver. Ultimately, all three solvers diverge chaotically from each other,
		but in the short term, Model 3 remains closer to Radau than RK45. From this I tenatively
		conclude that our model is an effective IVP solver of the Lorenz Attractor.`)},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function lr(r){let t;return{c(){t=h("Conclusion")},l(e){t=u(e,"Conclusion")},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function fr(r){let t,e,s,a,n,o="$5H$",l,p,$="$H$",I,A,M;return e=new K({props:{id:"gilpin"}}),a=new K({props:{id:"challu"}}),{c(){t=h("Inspired by recent research ("),v(e.$$.fragment),s=h(`) that supports the potential for generic
		neural architectures to match or exceed the performance of domain-specific models at the
		task of predicting chaotic systems, this project demonstrated the strong potential of at
		least one generic neural architecture (`),v(a.$$.fragment),n=h(`) to qualitatively match the
		performance of state-of-the-art IVP solvers, such as Radau, at integrating the ODE for at
		least one specific system--Lorenz--using only examples of solutions, with no explicit
		representation of the underlying ODE, to build up a model of the entire dynamics of the
		system. Given `),l=h(o),p=h(` points of an initial trajectory and at a high enough temporal resolution,
		the neural model demonstrated the ability to predict the subsequent `),I=h($),A=h(` outputs of the Radau
		solver with, in most cases, high accuracy, and in the worst case, marginal accuracy, for all
		trajectories in a test set that uniformly sampled the phase space of the system. When used autoregressively,
		the model demonstrated the potential to generate arbitrarily long trajectories that are visually
		indistinguishable from typical trajectories of the system and that match the output of the Radau
		solver at least as well as other state-of-the-art IVP solvers such as RK45.`)},l(E){t=u(E,"Inspired by recent research ("),y(e.$$.fragment,E),s=u(E,`) that supports the potential for generic
		neural architectures to match or exceed the performance of domain-specific models at the
		task of predicting chaotic systems, this project demonstrated the strong potential of at
		least one generic neural architecture (`),y(a.$$.fragment,E),n=u(E,`) to qualitatively match the
		performance of state-of-the-art IVP solvers, such as Radau, at integrating the ODE for at
		least one specific system--Lorenz--using only examples of solutions, with no explicit
		representation of the underlying ODE, to build up a model of the entire dynamics of the
		system. Given `),l=u(E,o),p=u(E,` points of an initial trajectory and at a high enough temporal resolution,
		the neural model demonstrated the ability to predict the subsequent `),I=u(E,$),A=u(E,` outputs of the Radau
		solver with, in most cases, high accuracy, and in the worst case, marginal accuracy, for all
		trajectories in a test set that uniformly sampled the phase space of the system. When used autoregressively,
		the model demonstrated the potential to generate arbitrarily long trajectories that are visually
		indistinguishable from typical trajectories of the system and that match the output of the Radau
		solver at least as well as other state-of-the-art IVP solvers such as RK45.`)},m(E,S){m(E,t,S),_(e,E,S),m(E,s,S),_(a,E,S),m(E,n,S),m(E,l,S),m(E,p,S),m(E,I,S),m(E,A,S),M=!0},p:H,i(E){M||(g(e.$$.fragment,E),g(a.$$.fragment,E),M=!0)},o(E){w(e.$$.fragment,E),w(a.$$.fragment,E),M=!1},d(E){E&&(f(t),f(s),f(n),f(l),f(p),f(I),f(A)),b(e,E),b(a,E)}}}function cr(r){let t;return{c(){t=h(`It must be noted, however, that the amount of data and model capacity used to achieve these
		results was substantial. Roughly 100 million data points from the Lorenz Attractor were used
		to train a model with over half a billion parameters for 40 hours using two GPUs. Although
		these numbers are modest compared to many of the most successful deep learning applications
		today, they are likely still far from trivial, in my opinion. For how many real-world
		chaotic systems with no known ODE representation is it feasible to gather 100 million data
		points? And could such a large model be optimized to run predictions in real-time for
		systems that require it to? I certainly do not know, but it seems plausible that such
		requirements could pose a significant barrier in many real-world cases. Having said all of
		that, it must also be noted that maximizing data and model efficiency was not a focus of
		this project, and so the potential for optimization is an open question.`)},l(e){t=u(e,`It must be noted, however, that the amount of data and model capacity used to achieve these
		results was substantial. Roughly 100 million data points from the Lorenz Attractor were used
		to train a model with over half a billion parameters for 40 hours using two GPUs. Although
		these numbers are modest compared to many of the most successful deep learning applications
		today, they are likely still far from trivial, in my opinion. For how many real-world
		chaotic systems with no known ODE representation is it feasible to gather 100 million data
		points? And could such a large model be optimized to run predictions in real-time for
		systems that require it to? I certainly do not know, but it seems plausible that such
		requirements could pose a significant barrier in many real-world cases. Having said all of
		that, it must also be noted that maximizing data and model efficiency was not a focus of
		this project, and so the potential for optimization is an open question.`)},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function hr(r){let t;return{c(){t=h("shadowing lemma")},l(e){t=u(e,"shadowing lemma")},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function ur(r){let t,e,s,a,n,o,l,p;return e=new V({props:{href:"https://en.wikipedia.org/wiki/Shadowing_lemma",$$slots:{default:[hr]},$$scope:{ctx:r}}}),a=new K({props:{id:"seo"}}),o=new K({props:{id:"degrave"}}),{c(){t=h(`My other, and possibly more critical, open question is, given that this model (and all IVP
		solvers) cannot actually predict the true solution but instead can only predict 'shadows' of
		true solutions (see `),v(e.$$.fragment),s=h(`), how useful can these predictions actually be in real-world applications? Is there any
		practical use for such a system, or are projects like this merely academic exercises? Is the
		true potential of deep neural networks as applied to chaotic systems more in their ability
		to prevent systems from entering chaotic regimes, as is explored in `),v(a.$$.fragment),n=h(`
		and `),v(o.$$.fragment),l=h(`, rather than to actually predict how chaotic dynamics will unfold?
		I suspect the answer is 'yes', although again I am far from certain. In either case, these
		results show a definite ability of generic deep neural networks to mimic the dynamics of a
		chaotic system, which may not amount to predicting future states but may still be enough to
		enable control systems to effectively manage those future states.`)},l($){t=u($,`My other, and possibly more critical, open question is, given that this model (and all IVP
		solvers) cannot actually predict the true solution but instead can only predict 'shadows' of
		true solutions (see `),y(e.$$.fragment,$),s=u($,`), how useful can these predictions actually be in real-world applications? Is there any
		practical use for such a system, or are projects like this merely academic exercises? Is the
		true potential of deep neural networks as applied to chaotic systems more in their ability
		to prevent systems from entering chaotic regimes, as is explored in `),y(a.$$.fragment,$),n=u($,`
		and `),y(o.$$.fragment,$),l=u($,`, rather than to actually predict how chaotic dynamics will unfold?
		I suspect the answer is 'yes', although again I am far from certain. In either case, these
		results show a definite ability of generic deep neural networks to mimic the dynamics of a
		chaotic system, which may not amount to predicting future states but may still be enough to
		enable control systems to effectively manage those future states.`)},m($,I){m($,t,I),_(e,$,I),m($,s,I),_(a,$,I),m($,n,I),_(o,$,I),m($,l,I),p=!0},p($,I){const A={};I&128&&(A.$$scope={dirty:I,ctx:$}),e.$set(A)},i($){p||(g(e.$$.fragment,$),g(a.$$.fragment,$),g(o.$$.fragment,$),p=!0)},o($){w(e.$$.fragment,$),w(a.$$.fragment,$),w(o.$$.fragment,$),p=!1},d($){$&&(f(t),f(s),f(n),f(l)),b(e,$),b(a,$),b(o,$)}}}function mr(r){let t;return{c(){t=h("References")},l(e){t=u(e,"References")},m(e,s){m(e,t,s)},d(e){e&&f(t)}}}function pr(r){let t,e,s="Modeling Chaotic Dynamics with Deep Learning: A Case Study on the Lorenz Attractor",a,n,o="Michael Horgan",l,p,$,I,A,M,E,S,T=`* All of my experiments were run on a Paperspace VM using two RTX 5000s, each with 16 GB
			of RAM.`,q,N,C,B,W,U,tt,J,O,F,X,L,D,at,Y,on,vt,Dn,rn,Le,Se,yt,la,Cn,_t,Fn,bt,Vn,ln,wn,Hn,Bn,kt,On,fn,vn,Un,Wn,It,Kn,Mt,Jn,At,Yn,Et,Xn,jt,Zn,zt,Qn,Tt,xt,Pt,Lt,ts,cn,yn,es,ns,St,Rt,ss,Nt,as,Gt,is,qt,os,Re,fa="The full set of N-HiTS hyperparameters for the first model configuration is:",rs,Ne,ls,Dt,fs,_n,cs,hs,Ct,us,Ft,it,ca,ms,Vt,ps,Ht,$s,Bt,Ge,ha=`<img class="m-auto" src="${`${Z}/Model1SmallErr.png`}" alt="" width="400" height="400"/> <img class="m-auto" src="${`${Z}/Model1MediumErr.png`}" alt="" width="400" height="400"/> <img class="m-auto" src="${`${Z}/Model1LargeErr.png`}" alt="" width="400" height="400"/>`,ds,Ot,gs,Ut,ws,Wt,ot,ua,vs,Kt,ys,Jt,Yt,_s,Xt,rt,ma,bs,Zt,ks,Qt,Is,te,Ms,ee,As,qe,Es,De,pa=`Note that Model 2 has roughly 32x the number of trainable parameters as Model 1. I've
		increased the depth (number of stacks, blocks per stack) and width (mlp layer size) of the
		network, and I've also significantly increased the amount of compression in the initial
		stacks. Because the network is much deeper, I also added layer normalization after each
		block to try to help reduce convergence time. Lastly, I increased the number of training
		steps and reduced the initial learning rate by an order of magnitude, and I modified the
		learning rate schedule to reduce by half whenever the validation loss does not decrease from
		the previous validation step.`,js,Ce,lt,$a,ne,zs,se,ae,Ts,Fe,ft,da,ie,xs,oe,Ps,Ve,ct,ga,Ls,re,Ss,le,Rs,He,ht,wa,fe,Ns,ce,Gs,he,qs,ue,Ds,Be,Cs,me,Fs,pe,Vs,Oe,ut,va,$e,Hs,de,Bs,Ue,mt,ya,Os,ge,Us,we,Ws,ve,Ks,ye,_e,Js,We,pt,_a,be,Ys,ke,Ie,Xs,Me,$t,ba,Zs,Ae,Qs,Ee,ta,je,ea,ze,na,Te,xe,sa,Pe,aa,Ke,Nn;return I=new G({props:{$$slots:{default:[Gi]},$$scope:{ctx:r}}}),A=new G({props:{$$slots:{default:[qi]},$$scope:{ctx:r}}}),B=new V({props:{href:"https://github.com/nrxszvo/mochaNN",$$slots:{default:[Di]},$$scope:{ctx:r}}}),J=new V({props:{href:"https://www.youtube.com/@SabineHossenfelder",$$slots:{default:[Ci]},$$scope:{ctx:r}}}),L=new V({props:{href:"https://www.youtube.com/playlist?list=PLMrJAkhIeNNTYaOnVI3QpH7jgULnAmvPA",$$slots:{default:[Fi]},$$scope:{ctx:r}}}),Y=new ia({props:{$$slots:{default:[Vi]},$$scope:{ctx:r}}}),vt=new G({props:{$$slots:{default:[Bi]},$$scope:{ctx:r}}}),_t=new Q({props:{$$slots:{default:[Oi]},$$scope:{ctx:r}}}),bt=new G({props:{$$slots:{default:[Ui]},$$scope:{ctx:r}}}),kt=new G({props:{$$slots:{default:[Wi]},$$scope:{ctx:r}}}),It=new G({props:{$$slots:{default:[Ji]},$$scope:{ctx:r}}}),Mt=new ia({props:{$$slots:{default:[Yi]},$$scope:{ctx:r}}}),At=new G({props:{$$slots:{default:[Xi]},$$scope:{ctx:r}}}),Et=new G({props:{$$slots:{default:[Zi]},$$scope:{ctx:r}}}),jt=new G({props:{$$slots:{default:[Qi]},$$scope:{ctx:r}}}),zt=new ia({props:{$$slots:{default:[to]},$$scope:{ctx:r}}}),Tt=new G({props:{$$slots:{default:[eo]},$$scope:{ctx:r}}}),xt=new an({props:{$$slots:{default:[no]},$$scope:{ctx:r}}}),Pt=new G({props:{$$slots:{default:[oo]},$$scope:{ctx:r}}}),Lt=new G({props:{$$slots:{default:[co]},$$scope:{ctx:r}}}),St=new G({props:{indent:"indent-0",$$slots:{default:[ho]},$$scope:{ctx:r}}}),Rt=new G({props:{$$slots:{default:[uo]},$$scope:{ctx:r}}}),Nt=new G({props:{indent:"indent-0",$$slots:{default:[mo]},$$scope:{ctx:r}}}),Gt=new G({props:{$$slots:{default:[po]},$$scope:{ctx:r}}}),qt=new an({props:{$$slots:{default:[$o]},$$scope:{ctx:r}}}),Ne=new oa({props:{hps:r[4]}}),Dt=new G({props:{indent:"indent-0",$$slots:{default:[go]},$$scope:{ctx:r}}}),Ct=new G({props:{$$slots:{default:[wo]},$$scope:{ctx:r}}}),Vt=new Q({props:{$$slots:{default:[vo]},$$scope:{ctx:r}}}),Ht=new G({props:{$$slots:{default:[yo]},$$scope:{ctx:r}}}),Ot=new Q({props:{$$slots:{default:[_o]},$$scope:{ctx:r}}}),Ut=new G({props:{$$slots:{default:[bo]},$$scope:{ctx:r}}}),Kt=new Q({props:{$$slots:{default:[ko]},$$scope:{ctx:r}}}),Jt=new G({props:{$$slots:{default:[Mo]},$$scope:{ctx:r}}}),Yt=new G({props:{$$slots:{default:[Ao]},$$scope:{ctx:r}}}),Zt=new Q({props:{$$slots:{default:[Eo]},$$scope:{ctx:r}}}),Qt=new G({props:{$$slots:{default:[jo]},$$scope:{ctx:r}}}),te=new an({props:{$$slots:{default:[zo]},$$scope:{ctx:r}}}),ee=new G({props:{$$slots:{default:[To]},$$scope:{ctx:r}}}),qe=new oa({props:{hps:r[5]}}),ne=new Q({props:{$$slots:{default:[xo]},$$scope:{ctx:r}}}),se=new G({props:{$$slots:{default:[Po]},$$scope:{ctx:r}}}),ae=new G({props:{$$slots:{default:[Lo]},$$scope:{ctx:r}}}),ie=new Q({props:{$$slots:{default:[So]},$$scope:{ctx:r}}}),oe=new G({props:{$$slots:{default:[Ro]},$$scope:{ctx:r}}}),re=new Q({props:{$$slots:{default:[No]},$$scope:{ctx:r}}}),le=new G({props:{$$slots:{default:[Go]},$$scope:{ctx:r}}}),fe=new Q({props:{$$slots:{default:[qo]},$$scope:{ctx:r}}}),ce=new G({props:{$$slots:{default:[Do]},$$scope:{ctx:r}}}),he=new G({props:{$$slots:{default:[Co]},$$scope:{ctx:r}}}),ue=new an({props:{$$slots:{default:[Fo]},$$scope:{ctx:r}}}),Be=new oa({props:{hps:r[6]}}),me=new G({props:{style:"my-4 text-sm font-serif",indent:"indent-0",$$slots:{default:[Ho]},$$scope:{ctx:r}}}),pe=new G({props:{$$slots:{default:[Bo]},$$scope:{ctx:r}}}),$e=new Q({props:{$$slots:{default:[Oo]},$$scope:{ctx:r}}}),de=new G({props:{$$slots:{default:[Uo]},$$scope:{ctx:r}}}),ge=new Q({props:{$$slots:{default:[Wo]},$$scope:{ctx:r}}}),we=new G({props:{$$slots:{default:[Ko]},$$scope:{ctx:r}}}),ve=new an({props:{$$slots:{default:[Jo]},$$scope:{ctx:r}}}),ye=new G({props:{$$slots:{default:[Yo]},$$scope:{ctx:r}}}),_e=new G({props:{$$slots:{default:[Xo]},$$scope:{ctx:r}}}),be=new Q({props:{$$slots:{default:[Zo]},$$scope:{ctx:r}}}),ke=new G({props:{$$slots:{default:[tr]},$$scope:{ctx:r}}}),Ie=new G({props:{$$slots:{default:[ar]},$$scope:{ctx:r}}}),Ae=new Q({props:{$$slots:{default:[or]},$$scope:{ctx:r}}}),Ee=new G({props:{$$slots:{default:[rr]},$$scope:{ctx:r}}}),je=new an({props:{$$slots:{default:[lr]},$$scope:{ctx:r}}}),ze=new G({props:{$$slots:{default:[fr]},$$scope:{ctx:r}}}),Te=new G({props:{$$slots:{default:[cr]},$$scope:{ctx:r}}}),xe=new G({props:{$$slots:{default:[ur]},$$scope:{ctx:r}}}),Pe=new an({props:{$$slots:{default:[mr]},$$scope:{ctx:r}}}),Ke=new ki({}),{c(){t=x("div"),e=x("div"),e.textContent=s,a=j(),n=x("div"),n.textContent=o,l=j(),p=x("br"),$=j(),v(I.$$.fragment),v(A.$$.fragment),M=j(),E=x("div"),S=x("p"),S.textContent=T,q=j(),N=x("p"),C=h("note: All code used in this project is available in my github repo: "),v(B.$$.fragment),W=j(),U=x("p"),tt=h(`note: For a quick and entertaining way to stay informed of new developments in the world
			of DL for dynamical systems modeling, I highly recommend Sabine Hossenfelder's `),v(J.$$.fragment),O=j(),F=x("p"),X=h(`note: If you would like to refresh your background on dynamical systems theory, I highly
			recommend Steve Brunton's free `),v(L.$$.fragment),D=h(" on the subject"),at=j(),v(Y.$$.fragment),on=j(),v(vt.$$.fragment),Dn=j(),rn=x("div"),Le=x("a"),Se=x("figure"),yt=x("img"),Cn=j(),v(_t.$$.fragment),Fn=j(),v(bt.$$.fragment),Vn=j(),ln=x("div"),wn=x("p"),Hn=h(r[0]),Bn=j(),v(kt.$$.fragment),On=j(),fn=x("div"),vn=x("p"),Un=h(r[1]),Wn=j(),v(It.$$.fragment),Kn=j(),v(Mt.$$.fragment),Jn=j(),v(At.$$.fragment),Yn=j(),v(Et.$$.fragment),Xn=j(),v(jt.$$.fragment),Zn=j(),v(zt.$$.fragment),Qn=j(),v(Tt.$$.fragment),v(xt.$$.fragment),v(Pt.$$.fragment),v(Lt.$$.fragment),ts=j(),cn=x("div"),yn=x("p"),es=h(r[2]),ns=j(),v(St.$$.fragment),v(Rt.$$.fragment),ss=j(),v(Nt.$$.fragment),as=j(),v(Gt.$$.fragment),is=j(),v(qt.$$.fragment),os=j(),Re=x("p"),Re.textContent=fa,rs=j(),v(Ne.$$.fragment),ls=j(),v(Dt.$$.fragment),fs=j(),_n=x("div"),cs=h(r[3]),hs=j(),v(Ct.$$.fragment),us=j(),Ft=x("figure"),it=x("img"),ms=j(),v(Vt.$$.fragment),ps=j(),v(Ht.$$.fragment),$s=j(),Bt=x("figure"),Ge=x("div"),Ge.innerHTML=ha,ds=j(),v(Ot.$$.fragment),gs=j(),v(Ut.$$.fragment),ws=j(),Wt=x("figure"),ot=x("img"),vs=j(),v(Kt.$$.fragment),ys=j(),v(Jt.$$.fragment),v(Yt.$$.fragment),_s=j(),Xt=x("figure"),rt=x("img"),bs=j(),v(Zt.$$.fragment),ks=j(),v(Qt.$$.fragment),Is=j(),v(te.$$.fragment),Ms=j(),v(ee.$$.fragment),As=j(),v(qe.$$.fragment),Es=j(),De=x("p"),De.textContent=pa,js=j(),Ce=x("figure"),lt=x("img"),v(ne.$$.fragment),zs=j(),v(se.$$.fragment),v(ae.$$.fragment),Ts=j(),Fe=x("figure"),ft=x("img"),v(ie.$$.fragment),xs=j(),v(oe.$$.fragment),Ps=j(),Ve=x("figure"),ct=x("img"),Ls=j(),v(re.$$.fragment),Ss=j(),v(le.$$.fragment),Rs=j(),He=x("figure"),ht=x("img"),v(fe.$$.fragment),Ns=j(),v(ce.$$.fragment),Gs=j(),v(he.$$.fragment),qs=j(),v(ue.$$.fragment),Ds=j(),v(Be.$$.fragment),Cs=j(),v(me.$$.fragment),Fs=j(),v(pe.$$.fragment),Vs=j(),Oe=x("figure"),ut=x("img"),v($e.$$.fragment),Hs=j(),v(de.$$.fragment),Bs=j(),Ue=x("figure"),mt=x("img"),Os=j(),v(ge.$$.fragment),Us=j(),v(we.$$.fragment),Ws=j(),v(ve.$$.fragment),Ks=j(),v(ye.$$.fragment),v(_e.$$.fragment),Js=j(),We=x("figure"),pt=x("img"),v(be.$$.fragment),Ys=j(),v(ke.$$.fragment),v(Ie.$$.fragment),Xs=j(),Me=x("figure"),$t=x("img"),Zs=j(),v(Ae.$$.fragment),Qs=j(),v(Ee.$$.fragment),ta=j(),v(je.$$.fragment),ea=j(),v(ze.$$.fragment),na=j(),v(Te.$$.fragment),v(xe.$$.fragment),sa=j(),v(Pe.$$.fragment),aa=j(),v(Ke.$$.fragment),this.h()},l(c){t=P(c,"DIV",{class:!0});var i=R(t);e=P(i,"DIV",{class:!0,"data-svelte-h":!0}),st(e)!=="svelte-py3wln"&&(e.textContent=s),a=z(i),n=P(i,"DIV",{class:!0,"data-svelte-h":!0}),st(n)!=="svelte-hve5fy"&&(n.textContent=o),l=z(i),p=P(i,"BR",{}),$=z(i),y(I.$$.fragment,i),y(A.$$.fragment,i),M=z(i),E=P(i,"DIV",{class:!0});var nt=R(E);S=P(nt,"P",{"data-svelte-h":!0}),st(S)!=="svelte-a477ex"&&(S.textContent=T),q=z(nt),N=P(nt,"P",{});var hn=R(N);C=u(hn,"note: All code used in this project is available in my github repo: "),y(B.$$.fragment,hn),hn.forEach(f),W=z(nt),U=P(nt,"P",{});var un=R(U);tt=u(un,`note: For a quick and entertaining way to stay informed of new developments in the world
			of DL for dynamical systems modeling, I highly recommend Sabine Hossenfelder's `),y(J.$$.fragment,un),un.forEach(f),O=z(nt),F=P(nt,"P",{});var Je=R(F);X=u(Je,`note: If you would like to refresh your background on dynamical systems theory, I highly
			recommend Steve Brunton's free `),y(L.$$.fragment,Je),D=u(Je," on the subject"),Je.forEach(f),nt.forEach(f),at=z(i),y(Y.$$.fragment,i),on=z(i),y(vt.$$.fragment,i),Dn=z(i),rn=P(i,"DIV",{class:!0});var bn=R(rn);Le=P(bn,"A",{title:!0,href:!0});var kn=R(Le);Se=P(kn,"FIGURE",{});var Ye=R(Se);yt=P(Ye,"IMG",{class:!0,width:!0,alt:!0,src:!0}),Cn=z(Ye),y(_t.$$.fragment,Ye),Ye.forEach(f),kn.forEach(f),bn.forEach(f),Fn=z(i),y(bt.$$.fragment,i),Vn=z(i),ln=P(i,"DIV",{class:!0});var In=R(ln);wn=P(In,"P",{});var Mn=R(wn);Hn=u(Mn,r[0]),Mn.forEach(f),In.forEach(f),Bn=z(i),y(kt.$$.fragment,i),On=z(i),fn=P(i,"DIV",{class:!0});var An=R(fn);vn=P(An,"P",{});var En=R(vn);Un=u(En,r[1]),En.forEach(f),An.forEach(f),Wn=z(i),y(It.$$.fragment,i),Kn=z(i),y(Mt.$$.fragment,i),Jn=z(i),y(At.$$.fragment,i),Yn=z(i),y(Et.$$.fragment,i),Xn=z(i),y(jt.$$.fragment,i),Zn=z(i),y(zt.$$.fragment,i),Qn=z(i),y(Tt.$$.fragment,i),y(xt.$$.fragment,i),y(Pt.$$.fragment,i),y(Lt.$$.fragment,i),ts=z(i),cn=P(i,"DIV",{class:!0});var jn=R(cn);yn=P(jn,"P",{});var zn=R(yn);es=u(zn,r[2]),zn.forEach(f),jn.forEach(f),ns=z(i),y(St.$$.fragment,i),y(Rt.$$.fragment,i),ss=z(i),y(Nt.$$.fragment,i),as=z(i),y(Gt.$$.fragment,i),is=z(i),y(qt.$$.fragment,i),os=z(i),Re=P(i,"P",{class:!0,"data-svelte-h":!0}),st(Re)!=="svelte-u5zzza"&&(Re.textContent=fa),rs=z(i),y(Ne.$$.fragment,i),ls=z(i),y(Dt.$$.fragment,i),fs=z(i),_n=P(i,"DIV",{});var Tn=R(_n);cs=u(Tn,r[3]),Tn.forEach(f),hs=z(i),y(Ct.$$.fragment,i),us=z(i),Ft=P(i,"FIGURE",{class:!0});var Xe=R(Ft);it=P(Xe,"IMG",{class:!0,src:!0,alt:!0,width:!0,height:!0}),ms=z(Xe),y(Vt.$$.fragment,Xe),Xe.forEach(f),ps=z(i),y(Ht.$$.fragment,i),$s=z(i),Bt=P(i,"FIGURE",{class:!0});var Ze=R(Bt);Ge=P(Ze,"DIV",{class:!0,"data-svelte-h":!0}),st(Ge)!=="svelte-5h4qif"&&(Ge.innerHTML=ha),ds=z(Ze),y(Ot.$$.fragment,Ze),Ze.forEach(f),gs=z(i),y(Ut.$$.fragment,i),ws=z(i),Wt=P(i,"FIGURE",{class:!0});var Qe=R(Wt);ot=P(Qe,"IMG",{class:!0,src:!0,alt:!0,width:!0,height:!0}),vs=z(Qe),y(Kt.$$.fragment,Qe),Qe.forEach(f),ys=z(i),y(Jt.$$.fragment,i),y(Yt.$$.fragment,i),_s=z(i),Xt=P(i,"FIGURE",{class:!0});var tn=R(Xt);rt=P(tn,"IMG",{class:!0,src:!0,alt:!0,width:!0,height:!0}),bs=z(tn),y(Zt.$$.fragment,tn),tn.forEach(f),ks=z(i),y(Qt.$$.fragment,i),Is=z(i),y(te.$$.fragment,i),Ms=z(i),y(ee.$$.fragment,i),As=z(i),y(qe.$$.fragment,i),Es=z(i),De=P(i,"P",{class:!0,"data-svelte-h":!0}),st(De)!=="svelte-1dsw2lx"&&(De.textContent=pa),js=z(i),Ce=P(i,"FIGURE",{class:!0});var mn=R(Ce);lt=P(mn,"IMG",{class:!0,src:!0,alt:!0,width:!0,height:!0}),y(ne.$$.fragment,mn),mn.forEach(f),zs=z(i),y(se.$$.fragment,i),y(ae.$$.fragment,i),Ts=z(i),Fe=P(i,"FIGURE",{class:!0});var pn=R(Fe);ft=P(pn,"IMG",{class:!0,src:!0,alt:!0,width:!0,height:!0}),y(ie.$$.fragment,pn),pn.forEach(f),xs=z(i),y(oe.$$.fragment,i),Ps=z(i),Ve=P(i,"FIGURE",{});var en=R(Ve);ct=P(en,"IMG",{class:!0,src:!0,alt:!0,width:!0,height:!0}),Ls=z(en),y(re.$$.fragment,en),en.forEach(f),Ss=z(i),y(le.$$.fragment,i),Rs=z(i),He=P(i,"FIGURE",{class:!0});var $n=R(He);ht=P($n,"IMG",{class:!0,src:!0,alt:!0,width:!0,height:!0}),y(fe.$$.fragment,$n),$n.forEach(f),Ns=z(i),y(ce.$$.fragment,i),Gs=z(i),y(he.$$.fragment,i),qs=z(i),y(ue.$$.fragment,i),Ds=z(i),y(Be.$$.fragment,i),Cs=z(i),y(me.$$.fragment,i),Fs=z(i),y(pe.$$.fragment,i),Vs=z(i),Oe=P(i,"FIGURE",{class:!0});var dn=R(Oe);ut=P(dn,"IMG",{class:!0,src:!0,alt:!0,width:!0,height:!0}),y($e.$$.fragment,dn),dn.forEach(f),Hs=z(i),y(de.$$.fragment,i),Bs=z(i),Ue=P(i,"FIGURE",{});var nn=R(Ue);mt=P(nn,"IMG",{class:!0,src:!0,alt:!0,width:!0,height:!0}),Os=z(nn),y(ge.$$.fragment,nn),nn.forEach(f),Us=z(i),y(we.$$.fragment,i),Ws=z(i),y(ve.$$.fragment,i),Ks=z(i),y(ye.$$.fragment,i),y(_e.$$.fragment,i),Js=z(i),We=P(i,"FIGURE",{class:!0});var gn=R(We);pt=P(gn,"IMG",{class:!0,src:!0,alt:!0,width:!0,height:!0}),y(be.$$.fragment,gn),gn.forEach(f),Ys=z(i),y(ke.$$.fragment,i),y(Ie.$$.fragment,i),Xs=z(i),Me=P(i,"FIGURE",{class:!0});var sn=R(Me);$t=P(sn,"IMG",{class:!0,src:!0,alt:!0,width:!0,height:!0}),Zs=z(sn),y(Ae.$$.fragment,sn),sn.forEach(f),Qs=z(i),y(Ee.$$.fragment,i),ta=z(i),y(je.$$.fragment,i),ea=z(i),y(ze.$$.fragment,i),na=z(i),y(Te.$$.fragment,i),y(xe.$$.fragment,i),sa=z(i),y(Pe.$$.fragment,i),aa=z(i),y(Ke.$$.fragment,i),i.forEach(f),this.h()},h(){k(e,"class","mt-8 text-2xl text-center"),k(n,"class","mt-2 text-sm text-center"),k(E,"class","my-4 mx-16 text-sm font-serif -indent-16"),k(yt,"class","m-auto"),k(yt,"width","128"),k(yt,"alt","A Trajectory Through Phase Space in a Lorenz Attractor"),et(yt.src,la="https://upload.wikimedia.org/wikipedia/commons/1/13/A_Trajectory_Through_Phase_Space_in_a_Lorenz_Attractor.gif")||k(yt,"src",la),k(Le,"title","Dan Quinn, CC BY-SA 3.0 <https://creativecommons.org/licenses/by-sa/3.0>, via Wikimedia Commons"),k(Le,"href","https://commons.wikimedia.org/wiki/File:A_Trajectory_Through_Phase_Space_in_a_Lorenz_Attractor.gif"),k(rn,"class","my-2 self-center"),k(ln,"class","self-center"),k(fn,"class","self-center"),k(cn,"class","self-center"),k(Re,"class","mt-2"),k(it,"class","m-auto"),et(it.src,ca=`${Z}/Model1ErrDist.png`)||k(it,"src",ca),k(it,"alt",""),k(it,"width","600"),k(it,"height","600"),k(Ft,"class","mb-6 self-center"),k(Ge,"class","flex flex-wrap justify-center"),k(Bt,"class","mt-6 mb-6 self-center"),k(ot,"class","m-auto"),et(ot.src,ua=`${Z}/model-1-pod.gif`)||k(ot,"src",ua),k(ot,"alt","prediction point of divergence"),k(ot,"width","450"),k(ot,"height","350"),k(Wt,"class","mt-6 mb-6 self-center"),k(rt,"class","m-auto"),et(rt.src,ma=`${Z}/trajectories.gif`)||k(rt,"src",ma),k(rt,"alt","trajectories approaching origin"),k(rt,"width","350"),k(rt,"height","300"),k(Xt,"class","mt-6 mb-6 self-center"),k(De,"class","mt-2"),k(lt,"class","m-auto"),et(lt.src,$a=`${Z}/Model2ErrDist.png`)||k(lt,"src",$a),k(lt,"alt",""),k(lt,"width","600"),k(lt,"height","600"),k(Ce,"class","mb-2 self-center"),k(ft,"class","m-auto"),et(ft.src,da=`${Z}/Model2Err3d.png`)||k(ft,"src",da),k(ft,"alt",""),k(ft,"width","600"),k(ft,"height","600"),k(Fe,"class","-mt-2 mb-2 self-center"),k(ct,"class","m-auto"),et(ct.src,ga=`${Z}/model-2-pod.gif`)||k(ct,"src",ga),k(ct,"alt","model 2 point of divergence"),k(ct,"width","400"),k(ct,"height","340"),k(ht,"class","m-auto"),et(ht.src,wa=`${Z}/Model2DFO.png`)||k(ht,"src",wa),k(ht,"alt","distance from origin vs. sMAPE"),k(ht,"width","800"),k(ht,"height","600"),k(He,"class","-mt-2 mb-2 self-center"),k(ut,"class","m-auto"),et(ut.src,va=`${Z}/Model2vModel3.png`)||k(ut,"src",va),k(ut,"alt",""),k(ut,"width","600"),k(ut,"height","600"),k(Oe,"class","mb-2 self-center"),k(mt,"class","m-auto"),et(mt.src,ya=`${Z}/model-3-low-dfo.gif`)||k(mt,"src",ya),k(mt,"alt","Model 3 trajectory example"),k(mt,"width","800"),k(mt,"height","800"),k(pt,"class","m-auto"),et(pt.src,_a=`${Z}/ref_v_ar.png`)||k(pt,"src",_a),k(pt,"alt","Reference vs. Autoregressive Trajectories"),k(pt,"width","450"),k(pt,"height","500"),k(We,"class","mb-2 self-center"),k($t,"class","m-auto"),et($t.src,ba=`${Z}/rk45_vs_nhits.png`)||k($t,"src",ba),k($t,"alt","Comparison of RK45 and predictions to Radua's solutions"),k($t,"width","600"),k($t,"height","600"),k(Me,"class","mb-2 self-center"),k(t,"class","flex flex-col mx-4 sm:mx-16 lg:mx-32 xl:mx-64")},m(c,i){m(c,t,i),d(t,e),d(t,a),d(t,n),d(t,l),d(t,p),d(t,$),_(I,t,null),_(A,t,null),d(t,M),d(t,E),d(E,S),d(E,q),d(E,N),d(N,C),_(B,N,null),d(E,W),d(E,U),d(U,tt),_(J,U,null),d(E,O),d(E,F),d(F,X),_(L,F,null),d(F,D),d(t,at),_(Y,t,null),d(t,on),_(vt,t,null),d(t,Dn),d(t,rn),d(rn,Le),d(Le,Se),d(Se,yt),d(Se,Cn),_(_t,Se,null),d(t,Fn),_(bt,t,null),d(t,Vn),d(t,ln),d(ln,wn),d(wn,Hn),d(t,Bn),_(kt,t,null),d(t,On),d(t,fn),d(fn,vn),d(vn,Un),d(t,Wn),_(It,t,null),d(t,Kn),_(Mt,t,null),d(t,Jn),_(At,t,null),d(t,Yn),_(Et,t,null),d(t,Xn),_(jt,t,null),d(t,Zn),_(zt,t,null),d(t,Qn),_(Tt,t,null),_(xt,t,null),_(Pt,t,null),_(Lt,t,null),d(t,ts),d(t,cn),d(cn,yn),d(yn,es),d(t,ns),_(St,t,null),_(Rt,t,null),d(t,ss),_(Nt,t,null),d(t,as),_(Gt,t,null),d(t,is),_(qt,t,null),d(t,os),d(t,Re),d(t,rs),_(Ne,t,null),d(t,ls),_(Dt,t,null),d(t,fs),d(t,_n),d(_n,cs),d(t,hs),_(Ct,t,null),d(t,us),d(t,Ft),d(Ft,it),d(Ft,ms),_(Vt,Ft,null),d(t,ps),_(Ht,t,null),d(t,$s),d(t,Bt),d(Bt,Ge),d(Bt,ds),_(Ot,Bt,null),d(t,gs),_(Ut,t,null),d(t,ws),d(t,Wt),d(Wt,ot),d(Wt,vs),_(Kt,Wt,null),d(t,ys),_(Jt,t,null),_(Yt,t,null),d(t,_s),d(t,Xt),d(Xt,rt),d(Xt,bs),_(Zt,Xt,null),d(t,ks),_(Qt,t,null),d(t,Is),_(te,t,null),d(t,Ms),_(ee,t,null),d(t,As),_(qe,t,null),d(t,Es),d(t,De),d(t,js),d(t,Ce),d(Ce,lt),_(ne,Ce,null),d(t,zs),_(se,t,null),_(ae,t,null),d(t,Ts),d(t,Fe),d(Fe,ft),_(ie,Fe,null),d(t,xs),_(oe,t,null),d(t,Ps),d(t,Ve),d(Ve,ct),d(Ve,Ls),_(re,Ve,null),d(t,Ss),_(le,t,null),d(t,Rs),d(t,He),d(He,ht),_(fe,He,null),d(t,Ns),_(ce,t,null),d(t,Gs),_(he,t,null),d(t,qs),_(ue,t,null),d(t,Ds),_(Be,t,null),d(t,Cs),_(me,t,null),d(t,Fs),_(pe,t,null),d(t,Vs),d(t,Oe),d(Oe,ut),_($e,Oe,null),d(t,Hs),_(de,t,null),d(t,Bs),d(t,Ue),d(Ue,mt),d(Ue,Os),_(ge,Ue,null),d(t,Us),_(we,t,null),d(t,Ws),_(ve,t,null),d(t,Ks),_(ye,t,null),_(_e,t,null),d(t,Js),d(t,We),d(We,pt),_(be,We,null),d(t,Ys),_(ke,t,null),_(Ie,t,null),d(t,Xs),d(t,Me),d(Me,$t),d(Me,Zs),_(Ae,Me,null),d(t,Qs),_(Ee,t,null),d(t,ta),_(je,t,null),d(t,ea),_(ze,t,null),d(t,na),_(Te,t,null),_(xe,t,null),d(t,sa),_(Pe,t,null),d(t,aa),_(Ke,t,null),Nn=!0},p(c,[i]){const nt={};i&128&&(nt.$$scope={dirty:i,ctx:c}),I.$set(nt);const hn={};i&128&&(hn.$$scope={dirty:i,ctx:c}),A.$set(hn);const un={};i&128&&(un.$$scope={dirty:i,ctx:c}),B.$set(un);const Je={};i&128&&(Je.$$scope={dirty:i,ctx:c}),J.$set(Je);const bn={};i&128&&(bn.$$scope={dirty:i,ctx:c}),L.$set(bn);const kn={};i&128&&(kn.$$scope={dirty:i,ctx:c}),Y.$set(kn);const Ye={};i&128&&(Ye.$$scope={dirty:i,ctx:c}),vt.$set(Ye);const In={};i&128&&(In.$$scope={dirty:i,ctx:c}),_t.$set(In);const Mn={};i&128&&(Mn.$$scope={dirty:i,ctx:c}),bt.$set(Mn);const An={};i&128&&(An.$$scope={dirty:i,ctx:c}),kt.$set(An);const En={};i&128&&(En.$$scope={dirty:i,ctx:c}),It.$set(En);const jn={};i&128&&(jn.$$scope={dirty:i,ctx:c}),Mt.$set(jn);const zn={};i&128&&(zn.$$scope={dirty:i,ctx:c}),At.$set(zn);const Tn={};i&128&&(Tn.$$scope={dirty:i,ctx:c}),Et.$set(Tn);const Xe={};i&128&&(Xe.$$scope={dirty:i,ctx:c}),jt.$set(Xe);const Ze={};i&128&&(Ze.$$scope={dirty:i,ctx:c}),zt.$set(Ze);const Qe={};i&128&&(Qe.$$scope={dirty:i,ctx:c}),Tt.$set(Qe);const tn={};i&128&&(tn.$$scope={dirty:i,ctx:c}),xt.$set(tn);const mn={};i&128&&(mn.$$scope={dirty:i,ctx:c}),Pt.$set(mn);const pn={};i&128&&(pn.$$scope={dirty:i,ctx:c}),Lt.$set(pn);const en={};i&128&&(en.$$scope={dirty:i,ctx:c}),St.$set(en);const $n={};i&128&&($n.$$scope={dirty:i,ctx:c}),Rt.$set($n);const dn={};i&128&&(dn.$$scope={dirty:i,ctx:c}),Nt.$set(dn);const nn={};i&128&&(nn.$$scope={dirty:i,ctx:c}),Gt.$set(nn);const gn={};i&128&&(gn.$$scope={dirty:i,ctx:c}),qt.$set(gn);const sn={};i&128&&(sn.$$scope={dirty:i,ctx:c}),Dt.$set(sn);const ka={};i&128&&(ka.$$scope={dirty:i,ctx:c}),Ct.$set(ka);const Ia={};i&128&&(Ia.$$scope={dirty:i,ctx:c}),Vt.$set(Ia);const Ma={};i&128&&(Ma.$$scope={dirty:i,ctx:c}),Ht.$set(Ma);const Aa={};i&128&&(Aa.$$scope={dirty:i,ctx:c}),Ot.$set(Aa);const Ea={};i&128&&(Ea.$$scope={dirty:i,ctx:c}),Ut.$set(Ea);const ja={};i&128&&(ja.$$scope={dirty:i,ctx:c}),Kt.$set(ja);const za={};i&128&&(za.$$scope={dirty:i,ctx:c}),Jt.$set(za);const Ta={};i&128&&(Ta.$$scope={dirty:i,ctx:c}),Yt.$set(Ta);const xa={};i&128&&(xa.$$scope={dirty:i,ctx:c}),Zt.$set(xa);const Pa={};i&128&&(Pa.$$scope={dirty:i,ctx:c}),Qt.$set(Pa);const La={};i&128&&(La.$$scope={dirty:i,ctx:c}),te.$set(La);const Sa={};i&128&&(Sa.$$scope={dirty:i,ctx:c}),ee.$set(Sa);const Ra={};i&128&&(Ra.$$scope={dirty:i,ctx:c}),ne.$set(Ra);const Na={};i&128&&(Na.$$scope={dirty:i,ctx:c}),se.$set(Na);const Ga={};i&128&&(Ga.$$scope={dirty:i,ctx:c}),ae.$set(Ga);const qa={};i&128&&(qa.$$scope={dirty:i,ctx:c}),ie.$set(qa);const Da={};i&128&&(Da.$$scope={dirty:i,ctx:c}),oe.$set(Da);const Ca={};i&128&&(Ca.$$scope={dirty:i,ctx:c}),re.$set(Ca);const Fa={};i&128&&(Fa.$$scope={dirty:i,ctx:c}),le.$set(Fa);const Va={};i&128&&(Va.$$scope={dirty:i,ctx:c}),fe.$set(Va);const Ha={};i&128&&(Ha.$$scope={dirty:i,ctx:c}),ce.$set(Ha);const Ba={};i&128&&(Ba.$$scope={dirty:i,ctx:c}),he.$set(Ba);const Oa={};i&128&&(Oa.$$scope={dirty:i,ctx:c}),ue.$set(Oa);const Ua={};i&128&&(Ua.$$scope={dirty:i,ctx:c}),me.$set(Ua);const Wa={};i&128&&(Wa.$$scope={dirty:i,ctx:c}),pe.$set(Wa);const Ka={};i&128&&(Ka.$$scope={dirty:i,ctx:c}),$e.$set(Ka);const Ja={};i&128&&(Ja.$$scope={dirty:i,ctx:c}),de.$set(Ja);const Ya={};i&128&&(Ya.$$scope={dirty:i,ctx:c}),ge.$set(Ya);const Xa={};i&128&&(Xa.$$scope={dirty:i,ctx:c}),we.$set(Xa);const Za={};i&128&&(Za.$$scope={dirty:i,ctx:c}),ve.$set(Za);const Qa={};i&128&&(Qa.$$scope={dirty:i,ctx:c}),ye.$set(Qa);const ti={};i&128&&(ti.$$scope={dirty:i,ctx:c}),_e.$set(ti);const ei={};i&128&&(ei.$$scope={dirty:i,ctx:c}),be.$set(ei);const ni={};i&128&&(ni.$$scope={dirty:i,ctx:c}),ke.$set(ni);const si={};i&128&&(si.$$scope={dirty:i,ctx:c}),Ie.$set(si);const ai={};i&128&&(ai.$$scope={dirty:i,ctx:c}),Ae.$set(ai);const ii={};i&128&&(ii.$$scope={dirty:i,ctx:c}),Ee.$set(ii);const oi={};i&128&&(oi.$$scope={dirty:i,ctx:c}),je.$set(oi);const ri={};i&128&&(ri.$$scope={dirty:i,ctx:c}),ze.$set(ri);const li={};i&128&&(li.$$scope={dirty:i,ctx:c}),Te.$set(li);const fi={};i&128&&(fi.$$scope={dirty:i,ctx:c}),xe.$set(fi);const ci={};i&128&&(ci.$$scope={dirty:i,ctx:c}),Pe.$set(ci)},i(c){Nn||(g(I.$$.fragment,c),g(A.$$.fragment,c),g(B.$$.fragment,c),g(J.$$.fragment,c),g(L.$$.fragment,c),g(Y.$$.fragment,c),g(vt.$$.fragment,c),g(_t.$$.fragment,c),g(bt.$$.fragment,c),g(kt.$$.fragment,c),g(It.$$.fragment,c),g(Mt.$$.fragment,c),g(At.$$.fragment,c),g(Et.$$.fragment,c),g(jt.$$.fragment,c),g(zt.$$.fragment,c),g(Tt.$$.fragment,c),g(xt.$$.fragment,c),g(Pt.$$.fragment,c),g(Lt.$$.fragment,c),g(St.$$.fragment,c),g(Rt.$$.fragment,c),g(Nt.$$.fragment,c),g(Gt.$$.fragment,c),g(qt.$$.fragment,c),g(Ne.$$.fragment,c),g(Dt.$$.fragment,c),g(Ct.$$.fragment,c),g(Vt.$$.fragment,c),g(Ht.$$.fragment,c),g(Ot.$$.fragment,c),g(Ut.$$.fragment,c),g(Kt.$$.fragment,c),g(Jt.$$.fragment,c),g(Yt.$$.fragment,c),g(Zt.$$.fragment,c),g(Qt.$$.fragment,c),g(te.$$.fragment,c),g(ee.$$.fragment,c),g(qe.$$.fragment,c),g(ne.$$.fragment,c),g(se.$$.fragment,c),g(ae.$$.fragment,c),g(ie.$$.fragment,c),g(oe.$$.fragment,c),g(re.$$.fragment,c),g(le.$$.fragment,c),g(fe.$$.fragment,c),g(ce.$$.fragment,c),g(he.$$.fragment,c),g(ue.$$.fragment,c),g(Be.$$.fragment,c),g(me.$$.fragment,c),g(pe.$$.fragment,c),g($e.$$.fragment,c),g(de.$$.fragment,c),g(ge.$$.fragment,c),g(we.$$.fragment,c),g(ve.$$.fragment,c),g(ye.$$.fragment,c),g(_e.$$.fragment,c),g(be.$$.fragment,c),g(ke.$$.fragment,c),g(Ie.$$.fragment,c),g(Ae.$$.fragment,c),g(Ee.$$.fragment,c),g(je.$$.fragment,c),g(ze.$$.fragment,c),g(Te.$$.fragment,c),g(xe.$$.fragment,c),g(Pe.$$.fragment,c),g(Ke.$$.fragment,c),Nn=!0)},o(c){w(I.$$.fragment,c),w(A.$$.fragment,c),w(B.$$.fragment,c),w(J.$$.fragment,c),w(L.$$.fragment,c),w(Y.$$.fragment,c),w(vt.$$.fragment,c),w(_t.$$.fragment,c),w(bt.$$.fragment,c),w(kt.$$.fragment,c),w(It.$$.fragment,c),w(Mt.$$.fragment,c),w(At.$$.fragment,c),w(Et.$$.fragment,c),w(jt.$$.fragment,c),w(zt.$$.fragment,c),w(Tt.$$.fragment,c),w(xt.$$.fragment,c),w(Pt.$$.fragment,c),w(Lt.$$.fragment,c),w(St.$$.fragment,c),w(Rt.$$.fragment,c),w(Nt.$$.fragment,c),w(Gt.$$.fragment,c),w(qt.$$.fragment,c),w(Ne.$$.fragment,c),w(Dt.$$.fragment,c),w(Ct.$$.fragment,c),w(Vt.$$.fragment,c),w(Ht.$$.fragment,c),w(Ot.$$.fragment,c),w(Ut.$$.fragment,c),w(Kt.$$.fragment,c),w(Jt.$$.fragment,c),w(Yt.$$.fragment,c),w(Zt.$$.fragment,c),w(Qt.$$.fragment,c),w(te.$$.fragment,c),w(ee.$$.fragment,c),w(qe.$$.fragment,c),w(ne.$$.fragment,c),w(se.$$.fragment,c),w(ae.$$.fragment,c),w(ie.$$.fragment,c),w(oe.$$.fragment,c),w(re.$$.fragment,c),w(le.$$.fragment,c),w(fe.$$.fragment,c),w(ce.$$.fragment,c),w(he.$$.fragment,c),w(ue.$$.fragment,c),w(Be.$$.fragment,c),w(me.$$.fragment,c),w(pe.$$.fragment,c),w($e.$$.fragment,c),w(de.$$.fragment,c),w(ge.$$.fragment,c),w(we.$$.fragment,c),w(ve.$$.fragment,c),w(ye.$$.fragment,c),w(_e.$$.fragment,c),w(be.$$.fragment,c),w(ke.$$.fragment,c),w(Ie.$$.fragment,c),w(Ae.$$.fragment,c),w(Ee.$$.fragment,c),w(je.$$.fragment,c),w(ze.$$.fragment,c),w(Te.$$.fragment,c),w(xe.$$.fragment,c),w(Pe.$$.fragment,c),w(Ke.$$.fragment,c),Nn=!1},d(c){c&&f(t),b(I),b(A),b(B),b(J),b(L),b(Y),b(vt),b(_t),b(bt),b(kt),b(It),b(Mt),b(At),b(Et),b(jt),b(zt),b(Tt),b(xt),b(Pt),b(Lt),b(St),b(Rt),b(Nt),b(Gt),b(qt),b(Ne),b(Dt),b(Ct),b(Vt),b(Ht),b(Ot),b(Ut),b(Kt),b(Jt),b(Yt),b(Zt),b(Qt),b(te),b(ee),b(qe),b(ne),b(se),b(ae),b(ie),b(oe),b(re),b(le),b(fe),b(ce),b(he),b(ue),b(Be),b(me),b(pe),b($e),b(de),b(ge),b(we),b(ve),b(ye),b(_e),b(be),b(ke),b(Ie),b(Ae),b(Ee),b(je),b(ze),b(Te),b(xe),b(Pe),b(Ke)}}}function $r(r){const t=`
\\begin{align}
\\dot{x} & = \\sigma(y-x) \\\\
\\dot{y} & = \\rho x - y - xz \\\\
\\dot{z} & = -\\beta z + xy
\\end{align}`,e=`
\\begin{align}
\\sigma & = 10 \\\\
\\beta & = \\frac{8}{3} \\\\
\\rho & = 28 \\\\
\\end{align}`,s=`
\\begin{align}
dt & \\approx 0.015 \\mathrm{s} \\\\
\\lambda_{max}^{-1} & \\approx 1.121 \\mathrm{s} \\\\
H = 100 \\text{ points} & \\approx 1.34\\lambda_{max}^{-1} \\\\
\\end{align}`,a=`
\\begin{align} 
\\operatorname{\\epsilon}(t) := \\frac{200}{t} \\sum_{t'=1}^t \\frac{|\\operatorname{\\boldsymbol{y}}(t')-\\operatorname{\\boldsymbol{\\hat{y}}}(t')|}{|\\operatorname{\\boldsymbol{y}}(t')| + |\\operatorname{\\boldsymbol{\\hat{y}}}(t')|} \\\\
\\end{align}`,n=[{desc:"horizon length",val:100},{desc:"lookback window length",val:500},{desc:"dt",val:.015008},{desc:"number of stacks",val:3},{desc:"blocks per stack",val:1},{desc:"mlp layers per block",val:4},{desc:"mlp layer size",val:1024},{desc:"activation function",val:"ReLU"},{desc:"max pooling factors",val:"2, 2, 2"},{desc:"frequency downsampling factors",val:"24, 12, 1"},{desc:"batch size",val:512},{desc:"# training steps",val:1e4},{desc:"learning rate",val:"1e-3"},{desc:"learning rate schedule",val:"halve every 3,333 steps"},{desc:"total trainable parameters",val:"~20 million"}],o=[{desc:"number of stacks",val:4},{desc:"blocks per stack",val:8},{desc:"mlp layer size",val:2048},{desc:"max pooling factors",val:"10, 4, 2, 1"},{desc:"frequency downsampling factors",val:"25, 12, 6, 1"},{desc:"batch size",val:512},{desc:"# training steps",val:15e4},{desc:"run validation every",val:"5000 steps"},{desc:"learning rate",val:"1e-4"},{desc:"learning rate schedule",val:"halve whenever validation loss does not decrease"},{desc:"all other hyperparameters",val:"same as Model 1"},{desc:"total trainable parameters",val:"~645 million"}],l=[{desc:"horizon",val:500},{desc:"lookback",val:2500},{desc:"dt",val:.0030016},{desc:"all other hyperparameters",val:"same as Model 2"}];return di(()=>{let p=document.createElement("script");p.src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js",p.async=!0,document.head.append(p),window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},processEscapes:!0}}),[t,e,s,a,n,o,l]}class vr extends gt{constructor(t){super(),wt(this,t,$r,pr,dt,{})}}export{vr as component};
