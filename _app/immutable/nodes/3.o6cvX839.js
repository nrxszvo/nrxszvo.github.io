import{s as wt,a as Cn,u as Vn,g as Fn,b as Un,n as O,x as nt,o as qa}from"../chunks/scheduler.BeaK0CkN.js";import{S as vt,i as yt,e as j,c as P,h as N,f as o,b as y,d as u,m as w,n as v,r as Ga,p as Ha,z as Na,t as f,v as _,j as c,w as b,k as d,x as k,y as I,l as Ai,s as x,a as z,g as Q,A as Ca}from"../chunks/index.BFxZNdMt.js";import{b as Z}from"../chunks/paths.B9s71t3G.js";const Va=typeof window<"u"?window:typeof globalThis<"u"?globalThis:global;function On(l){return(l==null?void 0:l.length)!==void 0?l:Array.from(l)}function Fa(l){let e,t,s;const i=l[4].default,n=Cn(i,l,l[3],null);return{c(){e=j("a"),n&&n.c(),this.h()},l(a){e=P(a,"A",{href:!0,class:!0,target:!0,rel:!0});var r=N(e);n&&n.l(r),r.forEach(o),this.h()},h(){y(e,"href",l[0]),y(e,"class",t="font-medium text-blue-600 dark:text-blue-500 hover:underline "+l[1]),y(e,"target",l[2]),y(e,"rel","noopener noreferrer")},m(a,r){u(a,e,r),n&&n.m(e,null),s=!0},p(a,[r]){n&&n.p&&(!s||r&8)&&Vn(n,i,a,a[3],s?Un(i,a[3],r,null):Fn(a[3]),null),(!s||r&1)&&y(e,"href",a[0]),(!s||r&2&&t!==(t="font-medium text-blue-600 dark:text-blue-500 hover:underline "+a[1]))&&y(e,"class",t),(!s||r&4)&&y(e,"target",a[2])},i(a){s||(w(n,a),s=!0)},o(a){v(n,a),s=!1},d(a){a&&o(e),n&&n.d(a)}}}function Ua(l,e,t){let{$$slots:s={},$$scope:i}=e,{href:n}=e,{styling:a=""}=e,{target:r="_blank"}=e;return l.$$set=m=>{"href"in m&&t(0,n=m.href),"styling"in m&&t(1,a=m.styling),"target"in m&&t(2,r=m.target),"$$scope"in m&&t(3,i=m.$$scope)},[n,a,r,i,s]}class B extends vt{constructor(e){super(),yt(this,e,Ua,Fa,wt,{href:0,styling:1,target:2})}}function La(l,e,t){const s=l.slice();return s[0]=e[t],s}function Ba(l){let e;return{c(){e=f("link")},l(t){e=c(t,"link")},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function Sa(l){let e,t,s=l[0].author+"",i,n,a,r=l[0].title+"",m,p,M,S=l[0].publisher+"",D,L,E,A=l[0].year+"",q,T,G,W,U,X,K,tt;return U=new B({props:{href:l[0].link,$$slots:{default:[Ba]},$$scope:{ctx:l}}}),{c(){e=j("li"),t=j("span"),i=f(s),n=f(", "),a=j("span"),m=f(r),p=f(`,
			`),M=j("span"),D=f(S),L=f(`,
			`),E=j("span"),q=f(A),T=f(", "),G=j("span"),W=f("["),_(U.$$.fragment),X=f("]"),K=f(`.
		`),this.h()},l(F){e=P(F,"LI",{});var V=N(e);t=P(V,"SPAN",{class:!0});var J=N(t);i=c(J,s),J.forEach(o),n=c(V,", "),a=P(V,"SPAN",{});var R=N(a);m=c(R,r),R.forEach(o),p=c(V,`,
			`),M=P(V,"SPAN",{});var C=N(M);D=c(C,S),C.forEach(o),L=c(V,`,
			`),E=P(V,"SPAN",{});var _t=N(E);q=c(_t,A),_t.forEach(o),T=c(V,", "),G=P(V,"SPAN",{});var st=N(G);W=c(st,"["),b(U.$$.fragment,st),X=c(st,"]"),st.forEach(o),K=c(V,`.
		`),V.forEach(o),this.h()},h(){y(t,"class","ms-4")},m(F,V){u(F,e,V),d(e,t),d(t,i),d(e,n),d(e,a),d(a,m),d(e,p),d(e,M),d(M,D),d(e,L),d(e,E),d(E,q),d(e,T),d(e,G),d(G,W),k(U,G,null),d(G,X),d(e,K),tt=!0},p(F,V){const J={};V&8&&(J.$$scope={dirty:V,ctx:F}),U.$set(J)},i(F){tt||(w(U.$$.fragment,F),tt=!0)},o(F){v(U.$$.fragment,F),tt=!1},d(F){F&&o(e),I(U)}}}function Oa(l){let e,t,s=On(Hn),i=[];for(let a=0;a<s.length;a+=1)i[a]=Sa(La(l,s,a));const n=a=>v(i[a],1,1,()=>{i[a]=null});return{c(){e=j("ol");for(let a=0;a<i.length;a+=1)i[a].c();this.h()},l(a){e=P(a,"OL",{class:!0});var r=N(e);for(let m=0;m<i.length;m+=1)i[m].l(r);r.forEach(o),this.h()},h(){y(e,"class","pl-5 my-2 text-xs list-decimal")},m(a,r){u(a,e,r);for(let m=0;m<i.length;m+=1)i[m]&&i[m].m(e,null);t=!0},p(a,[r]){if(r&0){s=On(Hn);let m;for(m=0;m<s.length;m+=1){const p=La(a,s,m);i[m]?(i[m].p(p,r),w(i[m],1)):(i[m]=Sa(p),i[m].c(),w(i[m],1),i[m].m(e,null))}for(Ga(),m=s.length;m<i.length;m+=1)n(m);Ha()}},i(a){if(!t){for(let r=0;r<s.length;r+=1)w(i[r]);t=!0}},o(a){i=i.filter(Boolean);for(let r=0;r<i.length;r+=1)v(i[r]);t=!1},d(a){a&&o(e),Na(i,a)}}}const Wn=l=>{for(let e=0;e<Hn.length;e++)if(Hn[e].id==l)return{index:e+1,link:Hn[e].link};throw new Error},Hn=[{id:"gilpin",author:"William Gilpin",title:"Model scale versus domain knoweldge in statistical forecasting of chaotic systems",publisher:"Phys. Rev. Res., vol. 5, pp. 043252, Dec",year:2023,link:"https://link.aps.org/doi/10.1103/PhysRevResearch.5.043252"},{id:"seo",author:"Seo, J., Kim, S., Jalalvand, A. et al.",title:"Avoiding fusion plasma tearing instability with deep reinforcement learning",publisher:"Nature",year:"2024",link:"https://doi.org/10.1038/s41586-024-07024-9"},{id:"degrave",author:"Jonas Degrave, Federico Felici, Jonas Buchli, Michael Neunert, Brendan Tracey, Francesco Carpanese, Timo Ewalds, Roland Hafner, et. al.",title:"Magnetic control of tokamak plasmas through deep reinforcement learning",publisher:"Nature",year:"2021",link:"https://doi.org/10.1038/s41586-021-04301-9"},{id:"challu",author:"Cristian Challu, Kin G. Olivares, Boris N. Oreshkin, Federico Garza, Max Mergenthaler-Canseco, Artur Dubrawski",title:"N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting",publisher:"arXiv:2201.12886",year:"2022",link:"https://arxiv.org/abs/2201.12886"},{id:"oreshkin",author:"Boris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, Yoshua Bengio",title:"N-BEATS: Neural Basis Expansion Analaysis for Interpretable Time Series Forecasting",publisher:"arXiv:1905.10437",year:"2019",link:"https://arxiv.org/abs/1905.10437"},{id:"osinga",author:"Hinke M. Osinga",title:"Understanding the geometry of dynamics: the stable manifold of the Lorenz system",publisher:"Journal of the Royal Society of New Zealand",year:"2018",link:"https://doi.org/10.1080/03036758.2018.1434802"}];class Wa extends vt{constructor(e){super(),yt(this,e,null,Oa,wt,{})}}function Ka(l){let e=Wn(l[0]).index+"",t;return{c(){t=f(e)},l(s){t=c(s,e)},m(s,i){u(s,t,i)},p(s,i){i&1&&e!==(e=Wn(s[0]).index+"")&&Ai(t,e)},d(s){s&&o(t)}}}function Ja(l){let e,t,s,i;return t=new B({props:{href:Wn(l[0]).link,$$slots:{default:[Ka]},$$scope:{ctx:l}}}),{c(){e=f("["),_(t.$$.fragment),s=f("]")},l(n){e=c(n,"["),b(t.$$.fragment,n),s=c(n,"]")},m(n,a){u(n,e,a),k(t,n,a),u(n,s,a),i=!0},p(n,[a]){const r={};a&1&&(r.href=Wn(n[0]).link),a&3&&(r.$$scope={dirty:a,ctx:n}),t.$set(r)},i(n){i||(w(t.$$.fragment,n),i=!0)},o(n){v(t.$$.fragment,n),i=!1},d(n){n&&(o(e),o(s)),I(t,n)}}}function Ya(l,e,t){let{id:s}=e;return l.$$set=i=>{"id"in i&&t(0,s=i.id)},[s]}class Y extends vt{constructor(e){super(),yt(this,e,Ya,Ja,wt,{id:0})}}function Xa(l){let e,t;const s=l[2].default,i=Cn(s,l,l[1],null);return{c(){e=j("div"),i&&i.c(),this.h()},l(n){e=P(n,"DIV",{id:!0,class:!0});var a=N(e);i&&i.l(a),a.forEach(o),this.h()},h(){y(e,"id",l[0]),y(e,"class","text-2xl font-bold text-center my-8")},m(n,a){u(n,e,a),i&&i.m(e,null),t=!0},p(n,[a]){i&&i.p&&(!t||a&2)&&Vn(i,s,n,n[1],t?Un(s,n[1],a,null):Fn(n[1]),null),(!t||a&1)&&y(e,"id",n[0])},i(n){t||(w(i,n),t=!0)},o(n){v(i,n),t=!1},d(n){n&&o(e),i&&i.d(n)}}}function Za(l,e,t){let{$$slots:s={},$$scope:i}=e,{id:n}=e;return l.$$set=a=>{"id"in a&&t(0,n=a.id),"$$scope"in a&&t(1,i=a.$$scope)},[n,i,s]}class qn extends vt{constructor(e){super(),yt(this,e,Za,Xa,wt,{id:0})}}function Qa(l){let e,t;const s=l[2].default,i=Cn(s,l,l[1],null);return{c(){e=j("div"),i&&i.c(),this.h()},l(n){e=P(n,"DIV",{id:!0,class:!0});var a=N(e);i&&i.l(a),a.forEach(o),this.h()},h(){y(e,"id",l[0]),y(e,"class","text-xl font-medium text-left mt-8 mb-4")},m(n,a){u(n,e,a),i&&i.m(e,null),t=!0},p(n,[a]){i&&i.p&&(!t||a&2)&&Vn(i,s,n,n[1],t?Un(s,n[1],a,null):Fn(n[1]),null),(!t||a&1)&&y(e,"id",n[0])},i(n){t||(w(i,n),t=!0)},o(n){v(i,n),t=!1},d(n){n&&o(e),i&&i.d(n)}}}function to(l,e,t){let{$$slots:s={},$$scope:i}=e,{id:n}=e;return l.$$set=a=>{"id"in a&&t(0,n=a.id),"$$scope"in a&&t(1,i=a.$$scope)},[n,i,s]}class Gn extends vt{constructor(e){super(),yt(this,e,to,Qa,wt,{id:0})}}function eo(l){let e,t,s;const i=l[3].default,n=Cn(i,l,l[2],null);return{c(){e=j("p"),n&&n.c(),this.h()},l(a){e=P(a,"P",{class:!0});var r=N(e);n&&n.l(r),r.forEach(o),this.h()},h(){y(e,"class",t="my-2 "+l[0]+" "+l[1])},m(a,r){u(a,e,r),n&&n.m(e,null),s=!0},p(a,[r]){n&&n.p&&(!s||r&4)&&Vn(n,i,a,a[2],s?Un(i,a[2],r,null):Fn(a[2]),null),(!s||r&3&&t!==(t="my-2 "+a[0]+" "+a[1]))&&y(e,"class",t)},i(a){s||(w(n,a),s=!0)},o(a){v(n,a),s=!1},d(a){a&&o(e),n&&n.d(a)}}}function no(l,e,t){let{$$slots:s={},$$scope:i}=e,{indent:n="indent-8"}=e,{style:a=""}=e;return l.$$set=r=>{"indent"in r&&t(0,n=r.indent),"style"in r&&t(1,a=r.style),"$$scope"in r&&t(2,i=r.$$scope)},[n,a,i,s]}class H extends vt{constructor(e){super(),yt(this,e,no,eo,wt,{indent:0,style:1})}}function Ra(l,e,t){const s=l.slice();return s[1]=e[t],s}function Da(l){let e,t,s=l[1].desc+"",i,n,a,r,m=l[1].val+"",p,M;return{c(){e=j("li"),t=j("div"),i=f(s),n=x(),a=j("div"),r=j("span"),p=f(m),M=x(),this.h()},l(S){e=P(S,"LI",{class:!0});var D=N(e);t=P(D,"DIV",{class:!0});var L=N(t);i=c(L,s),L.forEach(o),n=z(D),a=P(D,"DIV",{class:!0});var E=N(a);r=P(E,"SPAN",{class:!0});var A=N(r);p=c(A,m),A.forEach(o),E.forEach(o),M=z(D),D.forEach(o),this.h()},h(){y(t,"class","flex-none w-32 sm:w-64"),y(r,"class","rounded-0.5 p-1 font-mono"),y(a,"class","w-fit flex-wrap"),y(e,"class","flex items-center justify-left")},m(S,D){u(S,e,D),d(e,t),d(t,i),d(e,n),d(e,a),d(a,r),d(r,p),d(e,M)},p(S,D){D&1&&s!==(s=S[1].desc+"")&&Ai(i,s),D&1&&m!==(m=S[1].val+"")&&Ai(p,m)},d(S){S&&o(e)}}}function so(l){let e,t,s=On(l[0]),i=[];for(let n=0;n<s.length;n+=1)i[n]=Da(Ra(l,s,n));return{c(){e=j("section"),t=j("ul");for(let n=0;n<i.length;n+=1)i[n].c();this.h()},l(n){e=P(n,"SECTION",{class:!0});var a=N(e);t=P(a,"UL",{class:!0});var r=N(t);for(let m=0;m<i.length;m+=1)i[m].l(r);r.forEach(o),a.forEach(o),this.h()},h(){y(t,"class","flex flex-col m-auto ps-2 rounded gap-0.5 bg-gray-100 divide-y divide-gray-200 w-fit"),y(e,"class","relative block my-4")},m(n,a){u(n,e,a),d(e,t);for(let r=0;r<i.length;r+=1)i[r]&&i[r].m(t,null)},p(n,[a]){if(a&1){s=On(n[0]);let r;for(r=0;r<s.length;r+=1){const m=Ra(n,s,r);i[r]?i[r].p(m,a):(i[r]=Da(m),i[r].c(),i[r].m(t,null))}for(;r<i.length;r+=1)i[r].d(1);i.length=s.length}},i:O,o:O,d(n){n&&o(e),Na(i,n)}}}function io(l,e,t){let{hps:s=[]}=e;return l.$$set=i=>{"hps"in i&&t(0,s=i.hps)},[s]}class Mi extends vt{constructor(e){super(),yt(this,e,io,so,wt,{hps:0})}}function ao(l){let e,t;const s=l[1].default,i=Cn(s,l,l[0],null);return{c(){e=j("figcaption"),i&&i.c(),this.h()},l(n){e=P(n,"FIGCAPTION",{class:!0});var a=N(e);i&&i.l(a),a.forEach(o),this.h()},h(){y(e,"class","text-center text-xs mt-2 mx-0 sm:mx-36")},m(n,a){u(n,e,a),i&&i.m(e,null),t=!0},p(n,[a]){i&&i.p&&(!t||a&1)&&Vn(i,s,n,n[0],t?Un(s,n[0],a,null):Fn(n[0]),null)},i(n){t||(w(i,n),t=!0)},o(n){v(i,n),t=!1},d(n){n&&o(e),i&&i.d(n)}}}function oo(l,e,t){let{$$slots:s={},$$scope:i}=e;return l.$$set=n=>{"$$scope"in n&&t(0,i=n.$$scope)},[i,s]}class et extends vt{constructor(e){super(),yt(this,e,oo,ao,wt,{})}}const ro=`
\\begin{align}
\\dot{x} & = \\sigma(y-x) \\\\
\\dot{y} & = \\rho x - y - xz \\\\
\\dot{z} & = -\\beta z + xy
\\end{align}`,lo=`
\\begin{align}
\\sigma & = 10 \\\\
\\beta & = \\frac{8}{3} \\\\
\\rho & = 28 \\\\
\\end{align}`,fo=`
\\begin{align}
dt & \\approx 0.015 \\mathrm{s} \\\\
\\lambda_{max}^{-1} & \\approx 1.121 \\mathrm{s} \\\\
H = 100 \\text{ points} & \\approx 1.34\\lambda_{max}^{-1} \\\\
\\end{align}`,co=`
\\begin{align} 
\\operatorname{\\epsilon}(t) := \\frac{200}{t} \\sum_{t'=1}^t \\frac{|\\operatorname{\\boldsymbol{y}}(t')-\\operatorname{\\boldsymbol{\\hat{y}}}(t')|}{|\\operatorname{\\boldsymbol{y}}(t')| + |\\operatorname{\\boldsymbol{\\hat{y}}}(t')|} \\\\
\\end{align}`,ho=[{desc:"horizon length",val:100},{desc:"lookback window length",val:500},{desc:"dt",val:.015008},{desc:"number of stacks",val:3},{desc:"blocks per stack",val:1},{desc:"mlp layers per block",val:4},{desc:"mlp layer size",val:1024},{desc:"activation function",val:"ReLU"},{desc:"max pooling factors",val:"2, 2, 2"},{desc:"frequency downsampling factors",val:"24, 12, 1"},{desc:"batch size",val:512},{desc:"# training steps",val:1e4},{desc:"learning rate",val:"1e-3"},{desc:"learning rate schedule",val:"halve every 3,333 steps"},{desc:"total trainable parameters",val:"~20 million"}],uo=[{desc:"number of stacks",val:4},{desc:"blocks per stack",val:8},{desc:"mlp layer size",val:2048},{desc:"max pooling factors",val:"10, 4, 2, 1"},{desc:"frequency downsampling factors",val:"25, 12, 6, 1"},{desc:"batch size",val:512},{desc:"# training steps",val:15e4},{desc:"run validation every",val:"5000 steps"},{desc:"learning rate",val:"1e-4"},{desc:"learning rate schedule",val:"halve whenever validation loss does not decrease"},{desc:"all other hyperparameters",val:"same as Model 1"},{desc:"total trainable parameters",val:"~645 million"}],mo=[{desc:"horizon",val:500},{desc:"lookback",val:2500},{desc:"dt",val:.0030016},{desc:"all other hyperparameters",val:"same as Model 2"}],{window:po}=Va;function $o(l){let e;return{c(){e=f("mochaNN")},l(t){e=c(t,"mochaNN")},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function go(l){let e,t,s="Model scale versus domain knowledge in statistical forecasting of chaotic systems",i,n,a,r,m,p,M,S,D,L,E='<a href="#sabine">1</a>',A,q;return n=new Y({props:{id:"gilpin"}}),r=new Y({props:{id:"gilpin"}}),p=new Y({props:{id:"seo"}}),S=new Y({props:{id:"degrave"}}),{c(){e=f(`This project is inspired by several recent publications involving the use of deep
			learning to predict or control chaotic dynamical systems, in particular William Gilpin's
			paper, `),t=j("i"),t.textContent=s,i=x(),_(n.$$.fragment),a=f(`. Gilpin found that, given enough training data, generic neural
			architectures can match or exceed the performance of state-of-the-art domain-specific
			chaotic forecasting models such as reservoir computers and neural ODEs. I have recently
			become intrigued by the prospect of applying deep learning to prediction tasks involving
			chaotic systems, as I explore ways to contribute to the efforts to find technical
			solutions to climate change, and Gilpin's paper provided the impetus for me to begin
			this investigation. Along with `),_(r.$$.fragment),m=f(`, several other recent publications in
			related areas have been especially exciting to me, particularly the ones applying deep
			learning to tokamak control in nuclear fusion reactors (see e.g. `),_(p.$$.fragment),M=f(", "),_(S.$$.fragment),D=f(")"),L=j("sup"),L.innerHTML=E,A=f(`.
		`)},l(T){e=c(T,`This project is inspired by several recent publications involving the use of deep
			learning to predict or control chaotic dynamical systems, in particular William Gilpin's
			paper, `),t=P(T,"I",{"data-svelte-h":!0}),Q(t)!=="svelte-oq8z8z"&&(t.textContent=s),i=z(T),b(n.$$.fragment,T),a=c(T,`. Gilpin found that, given enough training data, generic neural
			architectures can match or exceed the performance of state-of-the-art domain-specific
			chaotic forecasting models such as reservoir computers and neural ODEs. I have recently
			become intrigued by the prospect of applying deep learning to prediction tasks involving
			chaotic systems, as I explore ways to contribute to the efforts to find technical
			solutions to climate change, and Gilpin's paper provided the impetus for me to begin
			this investigation. Along with `),b(r.$$.fragment,T),m=c(T,`, several other recent publications in
			related areas have been especially exciting to me, particularly the ones applying deep
			learning to tokamak control in nuclear fusion reactors (see e.g. `),b(p.$$.fragment,T),M=c(T,", "),b(S.$$.fragment,T),D=c(T,")"),L=P(T,"SUP",{"data-svelte-h":!0}),Q(L)!=="svelte-6x9dc9"&&(L.innerHTML=E),A=c(T,`.
		`)},m(T,G){u(T,e,G),u(T,t,G),u(T,i,G),k(n,T,G),u(T,a,G),k(r,T,G),u(T,m,G),k(p,T,G),u(T,M,G),k(S,T,G),u(T,D,G),u(T,L,G),u(T,A,G),q=!0},p:O,i(T){q||(w(n.$$.fragment,T),w(r.$$.fragment,T),w(p.$$.fragment,T),w(S.$$.fragment,T),q=!0)},o(T){v(n.$$.fragment,T),v(r.$$.fragment,T),v(p.$$.fragment,T),v(S.$$.fragment,T),q=!1},d(T){T&&(o(e),o(t),o(i),o(a),o(m),o(M),o(D),o(L),o(A)),I(n,T),I(r,T),I(p,T),I(S,T)}}}function wo(l){let e,t,s='<a href="#paperspace">2</a>',i,n,a='<a href="#brunton">3</a>',r;return{c(){e=f(`My goal with this project is to get some hands-on experience with a chaotic system and
			probe deeper into Gilpin's findings by testing the limits of a neural network's ability
			to model a single chaotic system (within the computational constraints imposed by my
			budget`),t=j("sup"),t.innerHTML=s,i=f(`). I'll start with what is probably the most
			well known chaotic system, the Lorenz Attractor. I will approach the problem more from
			the perspective of a generalist deep learning practioner than a dynamical systems
			expert, and so I will be (re)discovering many of the properties of the Lorenz System,
			and dynamical systems in general, as I go, often using the results of my experiments to
			guide my investigation`),n=j("sup"),n.innerHTML=a,r=f(`. What exactly makes the Lorenz
			Attractor chaotic? And what constraints will that impose on the ability of a deep neural
			network to model it? Let's find out!`)},l(m){e=c(m,`My goal with this project is to get some hands-on experience with a chaotic system and
			probe deeper into Gilpin's findings by testing the limits of a neural network's ability
			to model a single chaotic system (within the computational constraints imposed by my
			budget`),t=P(m,"SUP",{"data-svelte-h":!0}),Q(t)!=="svelte-yuo1gg"&&(t.innerHTML=s),i=c(m,`). I'll start with what is probably the most
			well known chaotic system, the Lorenz Attractor. I will approach the problem more from
			the perspective of a generalist deep learning practioner than a dynamical systems
			expert, and so I will be (re)discovering many of the properties of the Lorenz System,
			and dynamical systems in general, as I go, often using the results of my experiments to
			guide my investigation`),n=P(m,"SUP",{"data-svelte-h":!0}),Q(n)!=="svelte-1n6e9eh"&&(n.innerHTML=a),r=c(m,`. What exactly makes the Lorenz
			Attractor chaotic? And what constraints will that impose on the ability of a deep neural
			network to model it? Let's find out!`)},m(m,p){u(m,e,p),u(m,t,p),u(m,i,p),u(m,n,p),u(m,r,p)},p:O,d(m){m&&(o(e),o(t),o(i),o(n),o(r))}}}function vo(l){let e;return{c(){e=f("Youtube channel")},l(t){e=c(t,"Youtube channel")},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function yo(l){let e;return{c(){e=f("lecture series")},l(t){e=c(t,"lecture series")},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function _o(l){let e;return{c(){e=f("The Lorenz Attractor")},l(t){e=c(t,"The Lorenz Attractor")},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function bo(l){let e;return{c(){e=f("Lorenz Attractor")},l(t){e=c(t,"Lorenz Attractor")},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function ko(l){let e,t,s,i;return t=new B({props:{href:"https://en.wikipedia.org/wiki/Lorenz_system",$$slots:{default:[bo]},$$scope:{ctx:l}}}),{c(){e=f(`The
			`),_(t.$$.fragment),s=f(` was developed
			by Edward Lorenz et. al. in 1963 as a simplified model of atmospheric convection.`)},l(n){e=c(n,`The
			`),b(t.$$.fragment,n),s=c(n,` was developed
			by Edward Lorenz et. al. in 1963 as a simplified model of atmospheric convection.`)},m(n,a){u(n,e,a),k(t,n,a),u(n,s,a),i=!0},p(n,a){const r={};a&256&&(r.$$scope={dirty:a,ctx:n}),t.$set(r)},i(n){i||(w(t.$$.fragment,n),i=!0)},o(n){v(t.$$.fragment,n),i=!1},d(n){n&&(o(e),o(s)),I(t,n)}}}function Io(l){let e;return{c(){e=f("The Lorenz Attractor")},l(t){e=c(t,"The Lorenz Attractor")},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function Mo(l){let e;return{c(){e=f(`The Lorenz system is comprised of three ordinary differential equations representing the
			properties of convection and horizontal and vertical temperature in a two-dimensional
			fluid layer:`)},l(t){e=c(t,`The Lorenz system is comprised of three ordinary differential equations representing the
			properties of convection and horizontal and vertical temperature in a two-dimensional
			fluid layer:`)},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function Ao(l){let e,t,s="Attractor",i;return{c(){e=f("The Lorenz "),t=j("i"),t.textContent=s,i=f(" refers to a set of chaotic solutions to the system, most commonly:")},l(n){e=c(n,"The Lorenz "),t=P(n,"I",{"data-svelte-h":!0}),Q(t)!=="svelte-7jrnvq"&&(t.textContent=s),i=c(n," refers to a set of chaotic solutions to the system, most commonly:")},m(n,a){u(n,e,a),u(n,t,a),u(n,i,a)},p:O,d(n){n&&(o(e),o(t),o(i))}}}function Eo(l){let e;return{c(){e=f("dysts")},l(t){e=c(t,"dysts")},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function To(l){let e,t,s,i;return t=new B({props:{href:"https://github.com/williamgilpin/dysts",$$slots:{default:[Eo]},$$scope:{ctx:l}}}),{c(){e=f("I used Gilpin's "),_(t.$$.fragment),s=f(` python
			module to generate the training data for this solution.`)},l(n){e=c(n,"I used Gilpin's "),b(t.$$.fragment,n),s=c(n,` python
			module to generate the training data for this solution.`)},m(n,a){u(n,e,a),k(t,n,a),u(n,s,a),i=!0},p(n,a){const r={};a&256&&(r.$$scope={dirty:a,ctx:n}),t.$set(r)},i(n){i||(w(t.$$.fragment,n),i=!0)},o(n){v(t.$$.fragment,n),i=!1},d(n){n&&(o(e),o(s)),I(t,n)}}}function xo(l){let e;return{c(){e=f("Neural Architecture: N-HiTS")},l(t){e=c(t,"Neural Architecture: N-HiTS")},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function zo(l){let e,t,s,i,n,a;return t=new Y({props:{id:"challu"}}),i=new Y({props:{id:"gilpin"}}),{c(){e=f("The N-HiTS "),_(t.$$.fragment),s=f(` forecasting network is known to produce state-of-the-art results,
			at the time of writing, for univariate time series prediction, with up to an order of magnitude
			lower computational requirement than some competitors. Given my limited budget and its strong
			performance reported in `),_(i.$$.fragment),n=f(`, it seemed like the natural starting point
			for a network architecture.`)},l(r){e=c(r,"The N-HiTS "),b(t.$$.fragment,r),s=c(r,` forecasting network is known to produce state-of-the-art results,
			at the time of writing, for univariate time series prediction, with up to an order of magnitude
			lower computational requirement than some competitors. Given my limited budget and its strong
			performance reported in `),b(i.$$.fragment,r),n=c(r,`, it seemed like the natural starting point
			for a network architecture.`)},m(r,m){u(r,e,m),k(t,r,m),u(r,s,m),k(i,r,m),u(r,n,m),a=!0},p:O,i(r){a||(w(t.$$.fragment,r),w(i.$$.fragment,r),a=!0)},o(r){v(t.$$.fragment,r),v(i.$$.fragment,r),a=!1},d(r){r&&(o(e),o(s),o(n)),I(t,r),I(i,r)}}}function jo(l){let e,t,s,i,n,a;return t=new Y({props:{id:"oreshkin"}}),i=new Y({props:{id:"oreshkin"}}),{c(){e=f("The architectural ideas in N-HiTS build on those of its predecessor, N-BEATS "),_(t.$$.fragment),s=f(`, a neural basis expansion network for time series prediction. The key ideas inherited
			from N-BEATS include the organization of fully connected layers into blocks that output
			basis expansions (linear projections of the preceding fully connected layer's output)
			and the use of both forecast and backcast predictions from each block. The forecast
			predictions from all blocks are summed together to produce the final output of the
			network, while the backcasts are subtracted from the input of the corresponding block to
			produce a residual connection as the input to the next block. The goal of the backcasts
			is to help the downstream blocks by "removing components of their input that are not
			helpful for forecasting" `),_(i.$$.fragment),n=f(".")},l(r){e=c(r,"The architectural ideas in N-HiTS build on those of its predecessor, N-BEATS "),b(t.$$.fragment,r),s=c(r,`, a neural basis expansion network for time series prediction. The key ideas inherited
			from N-BEATS include the organization of fully connected layers into blocks that output
			basis expansions (linear projections of the preceding fully connected layer's output)
			and the use of both forecast and backcast predictions from each block. The forecast
			predictions from all blocks are summed together to produce the final output of the
			network, while the backcasts are subtracted from the input of the corresponding block to
			produce a residual connection as the input to the next block. The goal of the backcasts
			is to help the downstream blocks by "removing components of their input that are not
			helpful for forecasting" `),b(i.$$.fragment,r),n=c(r,".")},m(r,m){u(r,e,m),k(t,r,m),u(r,s,m),k(i,r,m),u(r,n,m),a=!0},p:O,i(r){a||(w(t.$$.fragment,r),w(i.$$.fragment,r),a=!0)},o(r){v(t.$$.fragment,r),v(i.$$.fragment,r),a=!1},d(r){r&&(o(e),o(s),o(n)),I(t,r),I(i,r)}}}function Po(l){let e,t,s,i;return t=new Y({props:{id:"challu"}}),{c(){e=f(`The novel ideas from N-HiTS enable the possiblity of modeling increasingly long time
			horizons while keeping computational complexity low. They include the use of pooling
			layers that downsample the inputs to each block and upsampling layers that map a
			compressed representation of the forecast to the output sample rate. In addition to the
			complexity savings, the compressed representations may induce a bias towards a temporal
			hierarchical modeling of the time series across the blocks that allows N-HiTS to exceed
			the performance of competing long-horizon forecasting models while requiring an order of
			magnitude lower computational complexity `),_(t.$$.fragment),s=f(".")},l(n){e=c(n,`The novel ideas from N-HiTS enable the possiblity of modeling increasingly long time
			horizons while keeping computational complexity low. They include the use of pooling
			layers that downsample the inputs to each block and upsampling layers that map a
			compressed representation of the forecast to the output sample rate. In addition to the
			complexity savings, the compressed representations may induce a bias towards a temporal
			hierarchical modeling of the time series across the blocks that allows N-HiTS to exceed
			the performance of competing long-horizon forecasting models while requiring an order of
			magnitude lower computational complexity `),b(t.$$.fragment,n),s=c(n,".")},m(n,a){u(n,e,a),k(t,n,a),u(n,s,a),i=!0},p:O,i(n){i||(w(t.$$.fragment,n),i=!0)},o(n){v(t.$$.fragment,n),i=!1},d(n){n&&(o(e),o(s)),I(t,n)}}}function Lo(l){let e;return{c(){e=f("Experiments")},l(t){e=c(t,"Experiments")},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function So(l){let e,t,s,i;return t=new Y({props:{id:"gilpin"}}),{c(){e=f(`While Gilpin's experiments focus on testing 24 different time-series prediction models
			on over 130 different chaotic systems using a relatively narrow range of hyper
			parameters for tuning `),_(t.$$.fragment),s=f(`, my experiments aim to tune a single model,
			N-HiTS, on a single system, the Lorenz Attractor, to maximize its accuracy for a given,
			relatively long, fixed horizon (aka prediction window length). And more specifically, I
			aim not only to achieve a low average error on the test set but also to limit the
			worst-case error as much as possible, which will likely mean achieving a degree of
			predictive power over the most chaotic regions of the system. Is this a completely naive
			aspiration given what is known about chaotic systems? Maybe, but I'm not really sure
			yet, and either way this should be a fun learning experience...
		`)},l(n){e=c(n,`While Gilpin's experiments focus on testing 24 different time-series prediction models
			on over 130 different chaotic systems using a relatively narrow range of hyper
			parameters for tuning `),b(t.$$.fragment,n),s=c(n,`, my experiments aim to tune a single model,
			N-HiTS, on a single system, the Lorenz Attractor, to maximize its accuracy for a given,
			relatively long, fixed horizon (aka prediction window length). And more specifically, I
			aim not only to achieve a low average error on the test set but also to limit the
			worst-case error as much as possible, which will likely mean achieving a degree of
			predictive power over the most chaotic regions of the system. Is this a completely naive
			aspiration given what is known about chaotic systems? Maybe, but I'm not really sure
			yet, and either way this should be a fun learning experience...
		`)},m(n,a){u(n,e,a),k(t,n,a),u(n,s,a),i=!0},p:O,i(n){i||(w(t.$$.fragment,n),i=!0)},o(n){v(t.$$.fragment,n),i=!1},d(n){n&&(o(e),o(s)),I(t,n)}}}function Ro(l){let e;return{c(){e=f("Data Generation")},l(t){e=c(t,"Data Generation")},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function Do(l){let e;return{c(){e=f("dysts")},l(t){e=c(t,"dysts")},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function No(l){let e;return{c(){e=f("IVP solver")},l(t){e=c(t,"IVP solver")},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function qo(l){let e;return{c(){e=f("dysts")},l(t){e=c(t,"dysts")},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function Go(l){let e,t="$dt$",s,i,n="$0.015$",a,r,m,p,M,S,D="$dt$",L,E,A,q="after",T,G="$dt$",W,U,X,K,tt="$\\text{first_step} = 0.0001801$",F,V,J;return m=new B({props:{href:"https://github.com/williamgilpin/dysts",$$slots:{default:[Do]},$$scope:{ctx:l}}}),M=new B({props:{href:"https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.solve_ivp.html",$$slots:{default:[No]},$$scope:{ctx:l}}}),X=new B({props:{href:"https://github.com/williamgilpin/dysts",$$slots:{default:[qo]},$$scope:{ctx:l}}}),{c(){e=f("I begin with a horizon (prediction window) of 100 points, using a "),s=f(t),i=f(` of approximately
			`),a=f(n),r=f(" seconds per point (the default from "),_(m.$$.fragment),p=f(") to sample the solution produced by the "),_(M.$$.fragment),S=f(". Importantly, note that this "),L=f(D),E=f(` is only the one used for sampling the solution
			`),A=j("i"),A.textContent=q,T=f(`
			it is generated by the IVP solver. The actual `),W=f(G),U=f(` used internally by the IVP solver
			can vary dynamically, but the initial target value used by `),_(X.$$.fragment),K=f(" is: "),F=f(tt),V=f(`.
		`)},l(R){e=c(R,"I begin with a horizon (prediction window) of 100 points, using a "),s=c(R,t),i=c(R,` of approximately
			`),a=c(R,n),r=c(R," seconds per point (the default from "),b(m.$$.fragment,R),p=c(R,") to sample the solution produced by the "),b(M.$$.fragment,R),S=c(R,". Importantly, note that this "),L=c(R,D),E=c(R,` is only the one used for sampling the solution
			`),A=P(R,"I",{"data-svelte-h":!0}),Q(A)!=="svelte-10nlrz4"&&(A.textContent=q),T=c(R,`
			it is generated by the IVP solver. The actual `),W=c(R,G),U=c(R,` used internally by the IVP solver
			can vary dynamically, but the initial target value used by `),b(X.$$.fragment,R),K=c(R," is: "),F=c(R,tt),V=c(R,`.
		`)},m(R,C){u(R,e,C),u(R,s,C),u(R,i,C),u(R,a,C),u(R,r,C),k(m,R,C),u(R,p,C),k(M,R,C),u(R,S,C),u(R,L,C),u(R,E,C),u(R,A,C),u(R,T,C),u(R,W,C),u(R,U,C),k(X,R,C),u(R,K,C),u(R,F,C),u(R,V,C),J=!0},p(R,C){const _t={};C&256&&(_t.$$scope={dirty:C,ctx:R}),m.$set(_t);const st={};C&256&&(st.$$scope={dirty:C,ctx:R}),M.$set(st);const bt={};C&256&&(bt.$$scope={dirty:C,ctx:R}),X.$set(bt)},i(R){J||(w(m.$$.fragment,R),w(M.$$.fragment,R),w(X.$$.fragment,R),J=!0)},o(R){v(m.$$.fragment,R),v(M.$$.fragment,R),v(X.$$.fragment,R),J=!1},d(R){R&&(o(e),o(s),o(i),o(a),o(r),o(p),o(S),o(L),o(E),o(A),o(T),o(W),o(U),o(K),o(F),o(V)),I(m,R),I(M,R),I(X,R)}}}function Ho(l){let e;return{c(){e=f("Lyapunov exponent")},l(t){e=c(t,"Lyapunov exponent")},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function Co(l){let e;return{c(){e=f("dysts")},l(t){e=c(t,"dysts")},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function Vo(l){let e;return{c(){e=f("Lyapunov time")},l(t){e=c(t,"Lyapunov time")},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function Fo(l){let e,t,s,i,n,a="$0.8917$",r,m,p,M,S="$1.121s$",D,L,E;return t=new B({props:{href:"https://en.wikipedia.org/wiki/Lyapunov_exponent",$$slots:{default:[Ho]},$$scope:{ctx:l}}}),i=new B({props:{href:"https://github.com/williamgilpin/dysts",$$slots:{default:[Co]},$$scope:{ctx:l}}}),p=new B({props:{href:"https://en.wikipedia.org/wiki/Lyapunov_time",$$slots:{default:[Vo]},$$scope:{ctx:l}}}),{c(){e=f(`At this stage, it may also be worth mentioning one of the common metrics for measuring
			the average chaoticity of a system, the maximum
			`),_(t.$$.fragment),s=f(`.
			As reported in
			`),_(i.$$.fragment),n=f(`, the Lyapunov exponent
			for the Lorenz Attractor is approx. `),r=f(a),m=f(", and so the "),_(p.$$.fragment),M=f(` is approx.
			`),D=f(S),L=f(".")},l(A){e=c(A,`At this stage, it may also be worth mentioning one of the common metrics for measuring
			the average chaoticity of a system, the maximum
			`),b(t.$$.fragment,A),s=c(A,`.
			As reported in
			`),b(i.$$.fragment,A),n=c(A,`, the Lyapunov exponent
			for the Lorenz Attractor is approx. `),r=c(A,a),m=c(A,", and so the "),b(p.$$.fragment,A),M=c(A,` is approx.
			`),D=c(A,S),L=c(A,".")},m(A,q){u(A,e,q),k(t,A,q),u(A,s,q),k(i,A,q),u(A,n,q),u(A,r,q),u(A,m,q),k(p,A,q),u(A,M,q),u(A,D,q),u(A,L,q),E=!0},p(A,q){const T={};q&256&&(T.$$scope={dirty:q,ctx:A}),t.$set(T);const G={};q&256&&(G.$$scope={dirty:q,ctx:A}),i.$set(G);const W={};q&256&&(W.$$scope={dirty:q,ctx:A}),p.$set(W)},i(A){E||(w(t.$$.fragment,A),w(i.$$.fragment,A),w(p.$$.fragment,A),E=!0)},o(A){v(t.$$.fragment,A),v(i.$$.fragment,A),v(p.$$.fragment,A),E=!1},d(A){A&&(o(e),o(s),o(n),o(r),o(m),o(M),o(D),o(L)),I(t,A),I(i,A),I(p,A)}}}function Uo(l){let e,t,s="on average",i,n="$e$",a,r,m="$1.121$",p,M,S="$\\frac{4}{3}$",D,L;return{c(){e=f("This tells us that, "),t=j("i"),t.textContent=s,i=f(`, the distance between any two trajectories from
			the Lorenz Attractor are expected to diverge by a factor of `),a=f(n),r=f(` after
			`),p=f(m),M=f(` seconds. Note that with these parameters, the horizon covers a time period of
			about
			`),D=f(S),L=f(`
			of the Lyapunov time.
		`)},l(E){e=c(E,"This tells us that, "),t=P(E,"I",{"data-svelte-h":!0}),Q(t)!=="svelte-ggx8py"&&(t.textContent=s),i=c(E,`, the distance between any two trajectories from
			the Lorenz Attractor are expected to diverge by a factor of `),a=c(E,n),r=c(E,` after
			`),p=c(E,m),M=c(E,` seconds. Note that with these parameters, the horizon covers a time period of
			about
			`),D=c(E,S),L=c(E,`
			of the Lyapunov time.
		`)},m(E,A){u(E,e,A),u(E,t,A),u(E,i,A),u(E,a,A),u(E,r,A),u(E,p,A),u(E,M,A),u(E,D,A),u(E,L,A)},p:O,d(E){E&&(o(e),o(t),o(i),o(a),o(r),o(p),o(M),o(D),o(L))}}}function Bo(l){let e,t="$[-9.79, -15.04, 20.53]$",s,i,n="$[0.99,1.01]$",a,r;return{c(){e=f(`The train and test sets are comprised of many trajectories with initial conditions all
			centered at approx. `),s=f(t),i=f(` and multiplied by a random perturbation
			uniformly sampled from the interval `),a=f(n),r=f(".")},l(m){e=c(m,`The train and test sets are comprised of many trajectories with initial conditions all
			centered at approx. `),s=c(m,t),i=c(m,` and multiplied by a random perturbation
			uniformly sampled from the interval `),a=c(m,n),r=c(m,".")},m(m,p){u(m,e,p),u(m,s,p),u(m,i,p),u(m,a,p),u(m,r,p)},p:O,d(m){m&&(o(e),o(s),o(i),o(a),o(r))}}}function Oo(l){let e,t="$3*100 = 300$",s,i,n="$3 * (500 + 100) = 1800$",a,r;return{c(){e=f(`The input to the N-HiTs model is a lookback window of the previous series values whose
			length is typically some multiple of the horizon window. I went with the default value
			from the N-HiTS paper of 5 times the horizon window length, or 500 points, making each
			training sample a total of 600 points. (Note that because N-HiTs is a univariate model,
			while the Lorenz System is three-dimensional, the data points must be flattened into one
			dimension. Therefore, the horizon window length is actually `),s=f(t),i=f(`, and each
			training sample's length is `),a=f(n),r=f(").")},l(m){e=c(m,`The input to the N-HiTs model is a lookback window of the previous series values whose
			length is typically some multiple of the horizon window. I went with the default value
			from the N-HiTS paper of 5 times the horizon window length, or 500 points, making each
			training sample a total of 600 points. (Note that because N-HiTs is a univariate model,
			while the Lorenz System is three-dimensional, the data points must be flattened into one
			dimension. Therefore, the horizon window length is actually `),s=c(m,t),i=c(m,`, and each
			training sample's length is `),a=c(m,n),r=c(m,").")},m(m,p){u(m,e,p),u(m,s,p),u(m,i,p),u(m,a,p),u(m,r,p)},p:O,d(m){m&&(o(e),o(s),o(i),o(a),o(r))}}}function Wo(l){let e,t=`$10,000 - 600 +
		1 = 9401$`,s,i;return{c(){e=f(`I choose, somewhat arbitrarily, to generate 10,000 points per series, and in order to
			increase data efficiency, I select each training sample by sliding the 600-point window
			along the series with a one-point stride. Each series, therefore, contributes `),s=f(t),i=f(` training samples. For the initial experiment, I generate 25 series with unique initial conditions,
			and train on 19 of them, and hold out 3 series for validation and 3 series for testing.`)},l(n){e=c(n,`I choose, somewhat arbitrarily, to generate 10,000 points per series, and in order to
			increase data efficiency, I select each training sample by sliding the 600-point window
			along the series with a one-point stride. Each series, therefore, contributes `),s=c(n,t),i=c(n,` training samples. For the initial experiment, I generate 25 series with unique initial conditions,
			and train on 19 of them, and hold out 3 series for validation and 3 series for testing.`)},m(n,a){u(n,e,a),u(n,s,a),u(n,i,a)},p:O,d(n){n&&(o(e),o(s),o(i))}}}function Ko(l){let e;return{c(){e=f("Model 1")},l(t){e=c(t,"Model 1")},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function Jo(l){let e;return{c(){e=f("The full set of N-HiTS hyperparameters for the first model configuration is:")},l(t){e=c(t,"The full set of N-HiTS hyperparameters for the first model configuration is:")},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function Yo(l){let e,t,s,i,n,a;return t=new Y({props:{id:"challu"}}),i=new Y({props:{id:"gilpin"}}),{c(){e=f("The model is optimized with MAE loss, consistent with the default loss from "),_(t.$$.fragment),s=f(`. For evaluation, I use the symmetric mean absolute percentage error (sMAPE) as
			defined in
			`),_(i.$$.fragment),n=f(":")},l(r){e=c(r,"The model is optimized with MAE loss, consistent with the default loss from "),b(t.$$.fragment,r),s=c(r,`. For evaluation, I use the symmetric mean absolute percentage error (sMAPE) as
			defined in
			`),b(i.$$.fragment,r),n=c(r,":")},m(r,m){u(r,e,m),k(t,r,m),u(r,s,m),k(i,r,m),u(r,n,m),a=!0},p:O,i(r){a||(w(t.$$.fragment,r),w(i.$$.fragment,r),a=!0)},o(r){v(t.$$.fragment,r),v(i.$$.fragment,r),a=!1},d(r){r&&(o(e),o(s),o(n)),I(t,r),I(i,r)}}}function Xo(l){let e;return{c(){e=f(`In this formulation, sMAPE is bound to the interval [0, 200]. The distribution of
			average window errors and its CDF on the test set are shown below. Note that the left y
			axis is log-scaled.`)},l(t){e=c(t,`In this formulation, sMAPE is bound to the interval [0, 200]. The distribution of
			average window errors and its CDF on the test set are shown below. Note that the left y
			axis is log-scaled.`)},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function Zo(l){let e;return{c(){e=f("Model 1 - sMAPE error distribution on the test set")},l(t){e=c(t,"Model 1 - sMAPE error distribution on the test set")},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function Qo(l){let e;return{c(){e=f(`To gain a more intuitive understanding of the magnitude of these errors, we can plot
			individual window predictions against the references:`)},l(t){e=c(t,`To gain a more intuitive understanding of the magnitude of these errors, we can plot
			individual window predictions against the references:`)},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function tr(l){let e;return{c(){e=f(`Samples of three different 100-point predictions from Model 1 with small, medium,
				and large sMAPE errors`)},l(t){e=c(t,`Samples of three different 100-point predictions from Model 1 with small, medium,
				and large sMAPE errors`)},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function er(l){let e;return{c(){e=f(`One interesting observation in all three graphs is that there appears to be a kind of
			"point of divergence" on the prediction before which the average error is very low and
			after which the error grows quickly. In the first graph, this point is about in the
			middle of the prediction, in the second it is maybe one third of the way into the
			prediction, and in the third it is near the beginning. If we look at the predictions of
			adjacent windows, we see that the behavior at this point is consisent across the
			windows, indicating that there is something about the system's behavior in this region
			that is very difficult for this model to fit, regardless of its alignment within the
			prediction window.`)},l(t){e=c(t,`One interesting observation in all three graphs is that there appears to be a kind of
			"point of divergence" on the prediction before which the average error is very low and
			after which the error grows quickly. In the first graph, this point is about in the
			middle of the prediction, in the second it is maybe one third of the way into the
			prediction, and in the third it is near the beginning. If we look at the predictions of
			adjacent windows, we see that the behavior at this point is consisent across the
			windows, indicating that there is something about the system's behavior in this region
			that is very difficult for this model to fit, regardless of its alignment within the
			prediction window.`)},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function nr(l){let e;return{c(){e=f(`The behavior of the model near the origin, which is a critical point of the system,
				for an especially challenging case. In contrast to all other regions of this
				trajectory, the model seems highly uncertain of how the trajectory will evolve
				shortly after passing near the origin.`)},l(t){e=c(t,`The behavior of the model near the origin, which is a critical point of the system,
				for an especially challenging case. In contrast to all other regions of this
				trajectory, the model seems highly uncertain of how the trajectory will evolve
				shortly after passing near the origin.`)},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function sr(l){let e;return{c(){e=f("critical points")},l(t){e=c(t,"critical points")},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function ir(l){let e,t,s,i,n,a,r,m;return t=new B({props:{href:"https://en.wikipedia.org/wiki/Critical_point_(mathematics)",$$slots:{default:[sr]},$$scope:{ctx:l}}}),i=new Y({props:{id:"osinga"}}),a=new Y({props:{id:"osinga"}}),{c(){e=f(`For anyone familiar with dynamical systems theory, it won't be a surprise that this
			point coincides with one of the three `),_(t.$$.fragment),s=f(` of the Lorenz system--in this case, the origin. And in this parameterization, the origin
			is known to be a saddle point comprised of the intersection of a stable 2D manifold and an
			unstable 1D manifold. "Stable" here means that trajectories near the critical point tend
			to move towards it even if they are perturbed slightly away from it by other forces, while
			"unstable" implies the opposite. (See `),_(i.$$.fragment),n=f(` for some excellent visualizations
			of these manifolds.) Near the origin, the unstable manifold is a line that is approximately
			perpendicular to the Z axis and parallel to the lengthwise orientation of the Attractor,
			which is why the trajectories always diverge at the near 90-degree angles that we see in
			the animations as they approach the origin. And the (incredibly complex) topography of the
			stable 2D manifold determines towards which of the other two critical points a trajectory
			will be deflected `),_(a.$$.fragment),r=f(`. In fact, I would tenatively conclude that the
			primary goal of the neural network is to learn the topography of the origin's stable 2D
			manifold. Since this manifold defines a boundary across which trajectories can never
			pass, we can confine the past and future path of any trajectory based on the boundaries
			of this manifold.
		`)},l(p){e=c(p,`For anyone familiar with dynamical systems theory, it won't be a surprise that this
			point coincides with one of the three `),b(t.$$.fragment,p),s=c(p,` of the Lorenz system--in this case, the origin. And in this parameterization, the origin
			is known to be a saddle point comprised of the intersection of a stable 2D manifold and an
			unstable 1D manifold. "Stable" here means that trajectories near the critical point tend
			to move towards it even if they are perturbed slightly away from it by other forces, while
			"unstable" implies the opposite. (See `),b(i.$$.fragment,p),n=c(p,` for some excellent visualizations
			of these manifolds.) Near the origin, the unstable manifold is a line that is approximately
			perpendicular to the Z axis and parallel to the lengthwise orientation of the Attractor,
			which is why the trajectories always diverge at the near 90-degree angles that we see in
			the animations as they approach the origin. And the (incredibly complex) topography of the
			stable 2D manifold determines towards which of the other two critical points a trajectory
			will be deflected `),b(a.$$.fragment,p),r=c(p,`. In fact, I would tenatively conclude that the
			primary goal of the neural network is to learn the topography of the origin's stable 2D
			manifold. Since this manifold defines a boundary across which trajectories can never
			pass, we can confine the past and future path of any trajectory based on the boundaries
			of this manifold.
		`)},m(p,M){u(p,e,M),k(t,p,M),u(p,s,M),k(i,p,M),u(p,n,M),k(a,p,M),u(p,r,M),m=!0},p(p,M){const S={};M&256&&(S.$$scope={dirty:M,ctx:p}),t.$set(S)},i(p){m||(w(t.$$.fragment,p),w(i.$$.fragment,p),w(a.$$.fragment,p),m=!0)},o(p){v(t.$$.fragment,p),v(i.$$.fragment,p),v(a.$$.fragment,p),m=!1},d(p){p&&(o(e),o(s),o(n),o(r)),I(t,p),I(i,p),I(a,p)}}}function ar(l){let e,t="$f(t) = \\exp(\\lambda t)x_0$",s,i,n="$\\lambda$",a,r,m="$x_0$",p,M,S="$11.8$",D,L;return{c(){e=f(`We can estimate how unstable the 1D manifold is by calculating the eigenvalues of the
			Jacobian matrix of the system at the origin and assuming the dynamics are approximately
			linear in this region. When we do this, we get three eigenvalues, two of which have
			negative real components and are associated with the stable 2D manifold, and the third
			which has positive real component and is associated with the unstable 1D manifold. The
			dynamics along the manifolds near the origin can be approximated by the expression `),s=f(t),i=f(`,
			where `),a=f(n),r=f(" equals the eigenvalue and "),p=f(m),M=f(` is the starting point. For the Lorenz
			Attractor, the eigenvalue associated with the unstable manifold is `),D=f(S),L=f(`, so
			trajectories will be rapidly deflected away from the origin along the unstable manifold,
			as we see in the below animation:`)},l(E){e=c(E,`We can estimate how unstable the 1D manifold is by calculating the eigenvalues of the
			Jacobian matrix of the system at the origin and assuming the dynamics are approximately
			linear in this region. When we do this, we get three eigenvalues, two of which have
			negative real components and are associated with the stable 2D manifold, and the third
			which has positive real component and is associated with the unstable 1D manifold. The
			dynamics along the manifolds near the origin can be approximated by the expression `),s=c(E,t),i=c(E,`,
			where `),a=c(E,n),r=c(E," equals the eigenvalue and "),p=c(E,m),M=c(E,` is the starting point. For the Lorenz
			Attractor, the eigenvalue associated with the unstable manifold is `),D=c(E,S),L=c(E,`, so
			trajectories will be rapidly deflected away from the origin along the unstable manifold,
			as we see in the below animation:`)},m(E,A){u(E,e,A),u(E,s,A),u(E,i,A),u(E,a,A),u(E,r,A),u(E,p,A),u(E,M,A),u(E,D,A),u(E,L,A)},p:O,d(E){E&&(o(e),o(s),o(i),o(a),o(r),o(p),o(M),o(D),o(L))}}}function or(l){let e;return{c(){e=f(`The trajectories from the training set all begin at nearly the same point but
				quickly diverge as they approach the critical point at the origin.`)},l(t){e=c(t,`The trajectories from the training set all begin at nearly the same point but
				quickly diverge as they approach the critical point at the origin.`)},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function rr(l){let e;return{c(){e=f(`Given all of this background, it is now unsurprising that the model is struggling to
			predict the behavior of the system near the origin. But we should also note that the
			model does quite well at predicting basically every other region of the system. We just
			have to figure out a way to improve the predictions near the origin, and then we should
			have a model with an overall very robust representation of the Lorenz Attractor. As this
			model and its training set are relatively modest in size, the next most obvious step to
			try is to signifcantly increase both the amount of training data and the model's
			capacity, and see if those changes alone are enough to resolve the weaknesses of Model
			1.`)},l(t){e=c(t,`Given all of this background, it is now unsurprising that the model is struggling to
			predict the behavior of the system near the origin. But we should also note that the
			model does quite well at predicting basically every other region of the system. We just
			have to figure out a way to improve the predictions near the origin, and then we should
			have a model with an overall very robust representation of the Lorenz Attractor. As this
			model and its training set are relatively modest in size, the next most obvious step to
			try is to signifcantly increase both the amount of training data and the model's
			capacity, and see if those changes alone are enough to resolve the weaknesses of Model
			1.`)},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function lr(l){let e;return{c(){e=f("Model 2")},l(t){e=c(t,"Model 2")},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function fr(l){let e;return{c(){e=f(`For the next model, I increased the number of unique initial conditions from 25 to
			10000, and held out 100 for validation and 200 for testing, leaving 9700 unique initial
			conditions, each of length 10,000 points, or about 150 seconds, in the training set. I
			also expanded the range of hyperparameters for tuning to include significantly larger
			models, both in depth and width. After tuning, I arrived at the following settings:`)},l(t){e=c(t,`For the next model, I increased the number of unique initial conditions from 25 to
			10000, and held out 100 for validation and 200 for testing, leaving 9700 unique initial
			conditions, each of length 10,000 points, or about 150 seconds, in the training set. I
			also expanded the range of hyperparameters for tuning to include significantly larger
			models, both in depth and width. After tuning, I arrived at the following settings:`)},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function cr(l){let e;return{c(){e=f("Model 2 vs Model 1 - sMAPE error distribution.")},l(t){e=c(t,"Model 2 vs Model 1 - sMAPE error distribution.")},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function hr(l){let e;return{c(){e=f(`From the plot, we see a significant increase in the first bin and a reduction in every
			subsequent bin of the per-window error histogram relative to Model 1, so the larger
			dataset and new hyperparameter tunings have a definite and significant positive impact.
			99% of windows from Model 2 have a sMAPE less than 6, compared to only 74% for Model 1,
			and 99.9% have a sMAPE less than 40, compared to 98% for Model 1.`)},l(t){e=c(t,`From the plot, we see a significant increase in the first bin and a reduction in every
			subsequent bin of the per-window error histogram relative to Model 1, so the larger
			dataset and new hyperparameter tunings have a definite and significant positive impact.
			99% of windows from Model 2 have a sMAPE less than 6, compared to only 74% for Model 1,
			and 99.9% have a sMAPE less than 40, compared to 98% for Model 1.`)},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function ur(l){let e;return{c(){e=f(`There are, however, still a handful of windows with very large sMAPE errors. We can
			visualize these errors slightly differently to get a better sense of how they are
			distributed within and across the test series:`)},l(t){e=c(t,`There are, however, still a handful of windows with very large sMAPE errors. We can
			visualize these errors slightly differently to get a better sense of how they are
			distributed within and across the test series:`)},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function mr(l){let e;return{c(){e=f("Model 2 - sMAPE errors per series per window in the test set.")},l(t){e=c(t,"Model 2 - sMAPE errors per series per window in the test set.")},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function pr(l){let e;return{c(){e=f(`We see that very large errors occur quite rarely and briefly, with the predictions
			spending most of the time near the ground truth. Let's check the animation for one of
			the large spikes with a sMAPE greater than 100:`)},l(t){e=c(t,`We see that very large errors occur quite rarely and briefly, with the predictions
			spending most of the time near the ground truth. Let's check the animation for one of
			the large spikes with a sMAPE greater than 100:`)},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function dr(l){let e;return{c(){e=f(`Model 2 - a trajectory with one of the largest sMAPE errors from the test set. DFO =
				'distance from origin'`)},l(t){e=c(t,`Model 2 - a trajectory with one of the largest sMAPE errors from the test set. DFO =
				'distance from origin'`)},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function $r(l){let e;return{c(){e=f(`Not surprisingly, this trajectory passes very close to the origin, and we immediately
			see how similar this failure case is to the one from Model 1. Despite the average
			improvement across all error magnitudes, has the model's ability to predict the behavior
			near the unstable origin actually improved significantly relative to Model 1? Let's
			check:`)},l(t){e=c(t,`Not surprisingly, this trajectory passes very close to the origin, and we immediately
			see how similar this failure case is to the one from Model 1. Despite the average
			improvement across all error magnitudes, has the model's ability to predict the behavior
			near the unstable origin actually improved significantly relative to Model 1? Let's
			check:`)},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function gr(l){let e,t="$n$",s,i,n="$n-1$",a,r,m="$n+1$",p,M;return{c(){e=f(`Each local minimum distance from the origin on the ground truth trajectories is
				calculated, and the corresponding maximum sMAPE error among the windows that
				included the minimum in its target is shown. A local minimum is defined as a point
				at time `),s=f(t),i=f(`
				that is closer to the origin than the points at `),a=f(n),r=f(" and "),p=f(m),M=f(".")},l(S){e=c(S,`Each local minimum distance from the origin on the ground truth trajectories is
				calculated, and the corresponding maximum sMAPE error among the windows that
				included the minimum in its target is shown. A local minimum is defined as a point
				at time `),s=c(S,t),i=c(S,`
				that is closer to the origin than the points at `),a=c(S,n),r=c(S," and "),p=c(S,m),M=c(S,".")},m(S,D){u(S,e,D),u(S,s,D),u(S,i,D),u(S,a,D),u(S,r,D),u(S,p,D),u(S,M,D)},p:O,d(S){S&&(o(e),o(s),o(i),o(a),o(r),o(p),o(M))}}}function wr(l){let e;return{c(){e=f(`As we can clearly see from the plot, Model 2 is able to predict points that are closer
			to the origin significantly more accurately than Model 1. So although Model 2 is not
			able to avoid catastraphic failure for all points, it has indeed reduced the number of
			points for which these failures occur.`)},l(t){e=c(t,`As we can clearly see from the plot, Model 2 is able to predict points that are closer
			to the origin significantly more accurately than Model 1. So although Model 2 is not
			able to avoid catastraphic failure for all points, it has indeed reduced the number of
			points for which these failures occur.`)},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function vr(l){let e,t="$dt$",s,i,n="$dt$",a,r,m="$\\approx0.015$",p,M,S="$\\approx0.003$",D,L,E="$\\approx1.5$",A,q;return{c(){e=f(`So we've drastically increased both model capacity and dataset size, and we have only
			achieved marginal improvement on the most chaotic trajectories. To continue to make
			progress, we probably need to try a different approach. One idea is to increase the
			temporal resolution of the model by using a smaller `),s=f(t),i=f(` to generate our data points.
			So far we've used a `),a=f(n),r=f(" of "),p=f(m),M=f(`. Let's try reducing that by a factor
			of 5 to `),D=f(S),L=f(`, and in order to keep the prediction task equally difficult,
			we'll also increase the horizon window, and lookback window, by a factor of 5 to 500 and
			2500 respectively, so that the total amount of time being predicted is still `),A=f(E),q=f(`
			seconds. We'll call this Model 3.`)},l(T){e=c(T,`So we've drastically increased both model capacity and dataset size, and we have only
			achieved marginal improvement on the most chaotic trajectories. To continue to make
			progress, we probably need to try a different approach. One idea is to increase the
			temporal resolution of the model by using a smaller `),s=c(T,t),i=c(T,` to generate our data points.
			So far we've used a `),a=c(T,n),r=c(T," of "),p=c(T,m),M=c(T,`. Let's try reducing that by a factor
			of 5 to `),D=c(T,S),L=c(T,`, and in order to keep the prediction task equally difficult,
			we'll also increase the horizon window, and lookback window, by a factor of 5 to 500 and
			2500 respectively, so that the total amount of time being predicted is still `),A=c(T,E),q=c(T,`
			seconds. We'll call this Model 3.`)},m(T,G){u(T,e,G),u(T,s,G),u(T,i,G),u(T,a,G),u(T,r,G),u(T,p,G),u(T,M,G),u(T,D,G),u(T,L,G),u(T,A,G),u(T,q,G)},p:O,d(T){T&&(o(e),o(s),o(i),o(a),o(r),o(p),o(M),o(D),o(L),o(A),o(q))}}}function yr(l){let e;return{c(){e=f("Model 3")},l(t){e=c(t,"Model 3")},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function _r(l){let e;return{c(){e=f("The new hyperparmeters for Model 3 are:")},l(t){e=c(t,"The new hyperparmeters for Model 3 are:")},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function br(l){let e;return{c(){e=f("FSDP Strategy")},l(t){e=c(t,"FSDP Strategy")},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function kr(l){let e,t,s,i,n,a;return i=new B({props:{href:"https://lightning.ai/docs/pytorch/stable/advanced/model_parallel/fsdp.html",$$slots:{default:[br]},$$scope:{ctx:l}}}),{c(){e=f("A sidenote on the practicality of training this model: "),t=j("p"),s=f(`Although we have not increased the number of parameters relative to Model 2, by
				increasing the input size and horizon length by a factor of 5, we have significantly
				increased the memory requirement for training this model. Now in order to fit the
				model on two GPUs with 16 GB of RAM each, I have to use Lightning's `),_(i.$$.fragment),n=f(` to distribute the model across both GPUs in order to get the per-GPU memory requirement
				to be just a hair under 16 GB. This also means that the model trains significantly more
				slowly, taking about 40 hours to converge, compared to about 16 hours for Model 2.`),this.h()},l(r){e=c(r,"A sidenote on the practicality of training this model: "),t=P(r,"P",{class:!0});var m=N(t);s=c(m,`Although we have not increased the number of parameters relative to Model 2, by
				increasing the input size and horizon length by a factor of 5, we have significantly
				increased the memory requirement for training this model. Now in order to fit the
				model on two GPUs with 16 GB of RAM each, I have to use Lightning's `),b(i.$$.fragment,m),n=c(m,` to distribute the model across both GPUs in order to get the per-GPU memory requirement
				to be just a hair under 16 GB. This also means that the model trains significantly more
				slowly, taking about 40 hours to converge, compared to about 16 hours for Model 2.`),m.forEach(o),this.h()},h(){y(t,"class","ms-8")},m(r,m){u(r,e,m),u(r,t,m),d(t,s),k(i,t,null),d(t,n),a=!0},p(r,m){const p={};m&256&&(p.$$scope={dirty:m,ctx:r}),i.$set(p)},i(r){a||(w(i.$$.fragment,r),a=!0)},o(r){v(i.$$.fragment,r),a=!1},d(r){r&&(o(e),o(t)),I(i)}}}function Ir(l){let e;return{c(){e=f("Model 3 vs. Model 2 - sMAPE error distribution.")},l(t){e=c(t,"Model 3 vs. Model 2 - sMAPE error distribution.")},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function Mr(l){let e;return{c(){e=f(`The maximum-error trajectory from the test set for Model 3. Although there is still
				lots of room for improvement, the predictions now at least roughly track the general
				contour of the ground truth.`)},l(t){e=c(t,`The maximum-error trajectory from the test set for Model 3. Although there is still
				lots of room for improvement, the predictions now at least roughly track the general
				contour of the ground truth.`)},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function Ar(l){let e,t="$dt \\approx 0.015$",s,i,n="$dt \\approx 0.003$",a,r;return{c(){e=f(`Based on the results of Model 3, we can conclude that a primary limiting factor with
			previous models was the temporal resolution of the trajectory's history; the information
			required to make an accurate prediction for the most challenging trajectories is
			apparently not contained in trajectories with a sample period of `),s=f(t),i=f(`,
			but much more of it is contained in trajectories with a sample period of `),a=f(n),r=f(".")},l(m){e=c(m,`Based on the results of Model 3, we can conclude that a primary limiting factor with
			previous models was the temporal resolution of the trajectory's history; the information
			required to make an accurate prediction for the most challenging trajectories is
			apparently not contained in trajectories with a sample period of `),s=c(m,t),i=c(m,`,
			but much more of it is contained in trajectories with a sample period of `),a=c(m,n),r=c(m,".")},m(m,p){u(m,e,p),u(m,s,p),u(m,i,p),u(m,a,p),u(m,r,p)},p:O,d(m){m&&(o(e),o(s),o(i),o(a),o(r))}}}function Er(l){let e;return{c(){e=f("Autoregressive Prediction")},l(t){e=c(t,"Autoregressive Prediction")},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function Tr(l){let e,t="$\\approx 7.5$",s,i;return{c(){e=f("Now that we have a model that adequately approximates the ODE given the last "),s=f(t),i=f(`
			seconds of the IVP solver's output, the next test is to measure how well the model continues
			to predict the trajectory given its own past predictions. To do this, for each trajectory
			in the test set, we will begin by using the first 2500 points to produce the model's prediction
			for points 2501-3000. Then we'll feed those 500 points back into the input to predict points
			3001-3500, and continue in this way for all 10,000 points in each trajectory. Then we can
			compare how closely the predicted trajectories match the ones produced by the IVP solver.
		`)},l(n){e=c(n,"Now that we have a model that adequately approximates the ODE given the last "),s=c(n,t),i=c(n,`
			seconds of the IVP solver's output, the next test is to measure how well the model continues
			to predict the trajectory given its own past predictions. To do this, for each trajectory
			in the test set, we will begin by using the first 2500 points to produce the model's prediction
			for points 2501-3000. Then we'll feed those 500 points back into the input to predict points
			3001-3500, and continue in this way for all 10,000 points in each trajectory. Then we can
			compare how closely the predicted trajectories match the ones produced by the IVP solver.
		`)},m(n,a){u(n,e,a),u(n,s,a),u(n,i,a)},p:O,d(n){n&&(o(e),o(s),o(i))}}}function xr(l){let e,t="$\\approx7.2$",s,i;return{c(){e=f("When we do this, we find that Model 3 is, on average, able to predict the first "),s=f(t),i=f(`
			seconds of the trajectory before it begins to diverge significantly from the reference (I
			arrived at this by calculating the mean time at which the maximum L2 distance between corresponding
			points on the trajectories exceeds 3). But we also note that, although there are clearly
			visible differences between the reference and the prediction, the full 10,000-point trajectories
			that Model 3 predicts are, to the naked eye at least, more or less indistinguishable from
			the typical trajectories of the Lorenz Attractor. In other words, they look like entirely
			plausible trajectories even if they eventually diverge significantly from the ones produced
			by the IVP solver for the same initial conditions.`)},l(n){e=c(n,"When we do this, we find that Model 3 is, on average, able to predict the first "),s=c(n,t),i=c(n,`
			seconds of the trajectory before it begins to diverge significantly from the reference (I
			arrived at this by calculating the mean time at which the maximum L2 distance between corresponding
			points on the trajectories exceeds 3). But we also note that, although there are clearly
			visible differences between the reference and the prediction, the full 10,000-point trajectories
			that Model 3 predicts are, to the naked eye at least, more or less indistinguishable from
			the typical trajectories of the Lorenz Attractor. In other words, they look like entirely
			plausible trajectories even if they eventually diverge significantly from the ones produced
			by the IVP solver for the same initial conditions.`)},m(n,a){u(n,e,a),u(n,s,a),u(n,i,a)},p:O,d(n){n&&(o(e),o(s),o(i))}}}function zr(l){let e;return{c(){e=f(`Comparison of trajectories generated by the IVP solver (left) and auto-regressively
				generated by Model 3 (right). Each row uses the same initial conditions.`)},l(t){e=c(t,`Comparison of trajectories generated by the IVP solver (left) and auto-regressively
				generated by Model 3 (right). Each row uses the same initial conditions.`)},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function jr(l){let e;return{c(){e=f("shadowing lemma")},l(t){e=c(t,"shadowing lemma")},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function Pr(l){let e,t,s,i;return t=new B({props:{href:"https://en.wikipedia.org/wiki/Shadowing_lemma",$$slots:{default:[jr]},$$scope:{ctx:l}}}),{c(){e=f(`Is there a way to confirm this observation more rigorously than with the eye test
			alone? I'm not sure, and I'll have to leave that question for future work. But it's also
			crucial to note that different IVP solvers also produce diverging trajectories in much
			the same manner as this. In fact, all numerical solutions to chaotic equations are known
			to diverge from the true solution due to the rounding error introduced by finite
			precision. The `),_(t.$$.fragment),s=f(` tells us that, in spite of this, the trajectories produced by IVP solvers still remain
			arbitrarily close to real trajectories from the ODE even if they do not exactly represent
			the ones that would be produced by the given initial conditions.
		`)},l(n){e=c(n,`Is there a way to confirm this observation more rigorously than with the eye test
			alone? I'm not sure, and I'll have to leave that question for future work. But it's also
			crucial to note that different IVP solvers also produce diverging trajectories in much
			the same manner as this. In fact, all numerical solutions to chaotic equations are known
			to diverge from the true solution due to the rounding error introduced by finite
			precision. The `),b(t.$$.fragment,n),s=c(n,` tells us that, in spite of this, the trajectories produced by IVP solvers still remain
			arbitrarily close to real trajectories from the ODE even if they do not exactly represent
			the ones that would be produced by the given initial conditions.
		`)},m(n,a){u(n,e,a),k(t,n,a),u(n,s,a),i=!0},p(n,a){const r={};a&256&&(r.$$scope={dirty:a,ctx:n}),t.$set(r)},i(n){i||(w(t.$$.fragment,n),i=!0)},o(n){v(t.$$.fragment,n),i=!1},d(n){n&&(o(e),o(s)),I(t,n)}}}function Lr(l){let e;return{c(){e=f("dysts")},l(t){e=c(t,"dysts")},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function Sr(l){let e;return{c(){e=f("Radau")},l(t){e=c(t,"Radau")},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function Rr(l){let e;return{c(){e=f("RK45")},l(t){e=c(t,"RK45")},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function Dr(l){let e,t,s,i,n,a,r,m;return t=new B({props:{href:"https://github.com/williamgilpin/dysts",$$slots:{default:[Lr]},$$scope:{ctx:l}}}),i=new B({props:{href:"https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.Radau.html",$$slots:{default:[Sr]},$$scope:{ctx:l}}}),a=new B({props:{href:"https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.RK45.html#scipy.integrate.RK45",$$slots:{default:[Rr]},$$scope:{ctx:l}}}),{c(){e=f(`In light of this, another way to evaluate the autoregressive output of the model is to
			compare it with the output from a different IVP solver with similar error constraints. `),_(t.$$.fragment),s=f(" uses the "),_(i.$$.fragment),n=f(" solver by default, and this is what I used to generate the dataset. "),_(a.$$.fragment),r=f(` has similar error constraints to Radau, so let's compare the autoregressive output against
			Radau relative to RK45's output against Radau:`)},l(p){e=c(p,`In light of this, another way to evaluate the autoregressive output of the model is to
			compare it with the output from a different IVP solver with similar error constraints. `),b(t.$$.fragment,p),s=c(p," uses the "),b(i.$$.fragment,p),n=c(p," solver by default, and this is what I used to generate the dataset. "),b(a.$$.fragment,p),r=c(p,` has similar error constraints to Radau, so let's compare the autoregressive output against
			Radau relative to RK45's output against Radau:`)},m(p,M){u(p,e,M),k(t,p,M),u(p,s,M),k(i,p,M),u(p,n,M),k(a,p,M),u(p,r,M),m=!0},p(p,M){const S={};M&256&&(S.$$scope={dirty:M,ctx:p}),t.$set(S);const D={};M&256&&(D.$$scope={dirty:M,ctx:p}),i.$set(D);const L={};M&256&&(L.$$scope={dirty:M,ctx:p}),a.$set(L)},i(p){m||(w(t.$$.fragment,p),w(i.$$.fragment,p),w(a.$$.fragment,p),m=!0)},o(p){v(t.$$.fragment,p),v(i.$$.fragment,p),v(a.$$.fragment,p),m=!1},d(p){p&&(o(e),o(s),o(n),o(r)),I(t,p),I(i,p),I(a,p)}}}function Nr(l){let e;return{c(){e=f("solve_ivp")},l(t){e=c(t,"solve_ivp")},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function qr(l){let e,t,s,i;return t=new B({props:{href:"https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.solve_ivp.html",$$slots:{default:[Nr]},$$scope:{ctx:l}}}),{c(){e=f(`Comparing the mean absolute error per timestep between Radau and Model 3 (blue) and
				Radua and RK45 (orange). The error is averaged across 200 different trajectories.
				Scipy's `),_(t.$$.fragment),s=f(" is used to produce the IVP solver outputs.")},l(n){e=c(n,`Comparing the mean absolute error per timestep between Radau and Model 3 (blue) and
				Radua and RK45 (orange). The error is averaged across 200 different trajectories.
				Scipy's `),b(t.$$.fragment,n),s=c(n," is used to produce the IVP solver outputs.")},m(n,a){u(n,e,a),k(t,n,a),u(n,s,a),i=!0},p(n,a){const r={};a&256&&(r.$$scope={dirty:a,ctx:n}),t.$set(r)},i(n){i||(w(t.$$.fragment,n),i=!0)},o(n){v(t.$$.fragment,n),i=!1},d(n){n&&(o(e),o(s)),I(t,n)}}}function Gr(l){let e;return{c(){e=f(`So we can say that the model is approximating the output of Radau more closely than
			another high-quality IVP solver. Ultimately, all three solvers diverge chaotically from
			each other, but in the short term, Model 3 remains closer to Radau than RK45. From this
			I tenatively conclude that the model is an effective IVP solver of the Lorenz Attractor.`)},l(t){e=c(t,`So we can say that the model is approximating the output of Radau more closely than
			another high-quality IVP solver. Ultimately, all three solvers diverge chaotically from
			each other, but in the short term, Model 3 remains closer to Radau than RK45. From this
			I tenatively conclude that the model is an effective IVP solver of the Lorenz Attractor.`)},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function Hr(l){let e;return{c(){e=f("Discussion")},l(t){e=c(t,"Discussion")},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function Cr(l){let e,t,s,i,n,a="$5H$",r,m,p="$H$",M,S,D;return t=new Y({props:{id:"gilpin"}}),i=new Y({props:{id:"challu"}}),{c(){e=f("Inspired by recent research ("),_(t.$$.fragment),s=f(`) that supports the potential for
			generic neural architectures to match or exceed the performance of domain-specific
			models at the task of predicting chaotic systems, this project demonstrated the strong
			potential of at least one generic neural architecture (`),_(i.$$.fragment),n=f(`) to
			qualitatively match the performance of state-of-the-art IVP solvers, such as Radau, at
			integrating the ODE for at least one specific system--Lorenz--using only examples of
			solutions, with no explicit representation of the underlying ODE, to build up a model of
			the entire dynamics of the system. Given `),r=f(a),m=f(` points of an initial trajectory and at
			a high enough temporal resolution, the neural model demonstrated the ability to predict the
			subsequent `),M=f(p),S=f(` outputs of the Radau solver with, in most cases, high accuracy, and in
			the worst case, marginal accuracy, for all trajectories in a test set that uniformly sampled
			the phase space of the system. When used autoregressively, the model demonstrated the potential
			to generate arbitrarily long trajectories that are visually indistinguishable from typical
			trajectories of the system and that match the output of the Radau solver at least as well
			as other state-of-the-art IVP solvers such as RK45.`)},l(L){e=c(L,"Inspired by recent research ("),b(t.$$.fragment,L),s=c(L,`) that supports the potential for
			generic neural architectures to match or exceed the performance of domain-specific
			models at the task of predicting chaotic systems, this project demonstrated the strong
			potential of at least one generic neural architecture (`),b(i.$$.fragment,L),n=c(L,`) to
			qualitatively match the performance of state-of-the-art IVP solvers, such as Radau, at
			integrating the ODE for at least one specific system--Lorenz--using only examples of
			solutions, with no explicit representation of the underlying ODE, to build up a model of
			the entire dynamics of the system. Given `),r=c(L,a),m=c(L,` points of an initial trajectory and at
			a high enough temporal resolution, the neural model demonstrated the ability to predict the
			subsequent `),M=c(L,p),S=c(L,` outputs of the Radau solver with, in most cases, high accuracy, and in
			the worst case, marginal accuracy, for all trajectories in a test set that uniformly sampled
			the phase space of the system. When used autoregressively, the model demonstrated the potential
			to generate arbitrarily long trajectories that are visually indistinguishable from typical
			trajectories of the system and that match the output of the Radau solver at least as well
			as other state-of-the-art IVP solvers such as RK45.`)},m(L,E){u(L,e,E),k(t,L,E),u(L,s,E),k(i,L,E),u(L,n,E),u(L,r,E),u(L,m,E),u(L,M,E),u(L,S,E),D=!0},p:O,i(L){D||(w(t.$$.fragment,L),w(i.$$.fragment,L),D=!0)},o(L){v(t.$$.fragment,L),v(i.$$.fragment,L),D=!1},d(L){L&&(o(e),o(s),o(n),o(r),o(m),o(M),o(S)),I(t,L),I(i,L)}}}function Vr(l){let e;return{c(){e=f(`It must be noted, however, that the amount of data and model capacity used to achieve
			these results was substantial. Roughly 100 million data points from the Lorenz Attractor
			were used to train a model with over half a billion parameters for 40 hours using two
			GPUs. Although these numbers are modest compared to many of the most successful deep
			learning applications today, they are likely still far from trivial, in my opinion. For
			how many real-world chaotic systems with no known ODE representation is it feasible to
			gather 100 million data points? And could such a large model be optimized to run
			predictions in real-time for systems that require it to? I certainly do not know, but it
			seems plausible that such requirements could pose a significant barrier in many
			real-world cases. Having said all of that, it must also be noted that maximizing data
			and model efficiency was not a focus of this project, and so the potential for
			optimization is an open question.`)},l(t){e=c(t,`It must be noted, however, that the amount of data and model capacity used to achieve
			these results was substantial. Roughly 100 million data points from the Lorenz Attractor
			were used to train a model with over half a billion parameters for 40 hours using two
			GPUs. Although these numbers are modest compared to many of the most successful deep
			learning applications today, they are likely still far from trivial, in my opinion. For
			how many real-world chaotic systems with no known ODE representation is it feasible to
			gather 100 million data points? And could such a large model be optimized to run
			predictions in real-time for systems that require it to? I certainly do not know, but it
			seems plausible that such requirements could pose a significant barrier in many
			real-world cases. Having said all of that, it must also be noted that maximizing data
			and model efficiency was not a focus of this project, and so the potential for
			optimization is an open question.`)},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function Fr(l){let e;return{c(){e=f("shadowing lemma")},l(t){e=c(t,"shadowing lemma")},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function Ur(l){let e,t,s,i,n,a,r,m;return t=new B({props:{href:"https://en.wikipedia.org/wiki/Shadowing_lemma",$$slots:{default:[Fr]},$$scope:{ctx:l}}}),i=new Y({props:{id:"seo"}}),a=new Y({props:{id:"degrave"}}),{c(){e=f(`Another, and possibly more critical, open question I have is, given that this model
			(and all IVP solvers) cannot actually predict the true solutions but instead can only
			predict 'shadows' of true solutions (see `),_(t.$$.fragment),s=f(`), how useful can these predictions actually be in real-world applications? Is there
			any practical use for such a system, or are projects like this merely academic
			exercises? Is the true potential of deep neural networks as applied to chaotic systems
			more in their ability to prevent systems from entering chaotic regimes, as is explored
			in `),_(i.$$.fragment),n=f(`
			and `),_(a.$$.fragment),r=f(`, rather than to actually predict how chaotic dynamics will
			unfold? I suspect the answer is 'yes', although I am again far from certain. In either
			case, these results show a definite ability of generic deep neural networks to mimic the
			dynamics of a chaotic system, which may not amount to predicting future states but may
			still be enough to enable control systems to effectively manage those future states.`)},l(p){e=c(p,`Another, and possibly more critical, open question I have is, given that this model
			(and all IVP solvers) cannot actually predict the true solutions but instead can only
			predict 'shadows' of true solutions (see `),b(t.$$.fragment,p),s=c(p,`), how useful can these predictions actually be in real-world applications? Is there
			any practical use for such a system, or are projects like this merely academic
			exercises? Is the true potential of deep neural networks as applied to chaotic systems
			more in their ability to prevent systems from entering chaotic regimes, as is explored
			in `),b(i.$$.fragment,p),n=c(p,`
			and `),b(a.$$.fragment,p),r=c(p,`, rather than to actually predict how chaotic dynamics will
			unfold? I suspect the answer is 'yes', although I am again far from certain. In either
			case, these results show a definite ability of generic deep neural networks to mimic the
			dynamics of a chaotic system, which may not amount to predicting future states but may
			still be enough to enable control systems to effectively manage those future states.`)},m(p,M){u(p,e,M),k(t,p,M),u(p,s,M),k(i,p,M),u(p,n,M),k(a,p,M),u(p,r,M),m=!0},p(p,M){const S={};M&256&&(S.$$scope={dirty:M,ctx:p}),t.$set(S)},i(p){m||(w(t.$$.fragment,p),w(i.$$.fragment,p),w(a.$$.fragment,p),m=!0)},o(p){v(t.$$.fragment,p),v(i.$$.fragment,p),v(a.$$.fragment,p),m=!1},d(p){p&&(o(e),o(s),o(n),o(r)),I(t,p),I(i,p),I(a,p)}}}function Br(l){let e;return{c(){e=f("References")},l(t){e=c(t,"References")},m(t,s){u(t,e,s)},d(t){t&&o(e)}}}function Or(l){let e,t,s='<nav class="border-r-2 sticky top-12"><div><section><ul class="menu"><li><a href="#intro">Intro</a></li> <li><a href="#lorenz">Lorenz Attractor</a></li> <li><a href="#nhits">Neural Architecture</a></li> <li><a href="#experiments">Experiments</a></li> <ul class="text-sm submenu"><li><a href="#datagen">Data Generation</a></li> <li><a href="#model-1">Model 1</a></li> <li><a href="#model-2">Model 2</a></li> <li><a href="#model-3">Model 3</a></li> <li><a href="#arpred">Autoregressive Prediction</a></li></ul> <li><a href="#discussion">Discussion</a></li> <li><a href="#references">References</a></li></ul></section></div></nav>',i,n,a,r="Modeling Chaotic Dynamics with Deep Learning: A Case Study on the Lorenz Attractor",m,p,M="Michael Horgan",S,D,L,E,A,q,T,G,W,U,X,K,tt,F,V,J,R,C,_t=`<sup id="paperspace">2. All of my experiments were run on a Paperspace VM using two RTX 5000s, each
					with 16 GB of RAM.</sup>`,st,bt,kt,Kn,It,Jn,Yn,Mt,Xn,At,Zn,un,Ce,Ve,Et,Ei,Qn,Tt,ts,xt,es,mn,kn,Ti=ro+"",ns,ss,zt,is,pn,In,xi=lo+"",as,os,jt,rs,Pt,ls,Lt,fs,St,cs,Rt,hs,Dt,us,Nt,qt,Gt,Ht,ms,dn,Mn,zi=fo+"",ps,ds,Ct,Vt,$s,Ft,gs,Ut,ws,Bt,vs,Ot,ys,Fe,_s,Wt,bs,An,ji=co+"",ks,Is,Kt,Ms,Jt,at,Pi,As,Yt,Es,Xt,Ts,Zt,Ue,Li=`<img class="" src="${`${Z}/Model1SmallErr.png`}" alt="" width="300" height="300"/> <img class="" src="${`${Z}/Model1MediumErr.png`}" alt="" width="300" height="300"/> <img class="" src="${`${Z}/Model1LargeErr.png`}" alt="" width="300" height="300"/>`,xs,Qt,zs,te,js,ee,ot,Si,Ps,ne,Ls,se,ie,Ss,ae,rt,Ri,Rs,oe,Ds,re,Ns,le,qs,fe,Gs,Be,Hs,Oe,Di=`Note that Model 2 has roughly 32x the number of trainable parameters as Model 1. I've
			increased the depth (number of stacks, blocks per stack) and width (mlp layer size) of
			the network, and I've also significantly increased the amount of compression in the
			initial stacks. Because the network is much deeper, I also added layer normalization
			after each block to try to help reduce convergence time. Lastly, I increased the number
			of training steps and reduced the initial learning rate by an order of magnitude, and I
			modified the learning rate schedule to reduce by half whenever the validation loss does
			not decrease from the previous validation step.`,Cs,We,lt,Ni,ce,Vs,he,ue,Fs,Ke,ft,qi,me,Us,pe,Bs,Je,ct,Gi,Os,de,Ws,$e,Ks,Ye,ht,Hi,ge,Js,we,Ys,ve,Xs,ye,Zs,_e,Qs,Xe,ti,be,ei,it,ni,Ci="$dt \\approx 0.003$",si,ii,Vi="$\\lt80$",ai,oi,ri,Ze,ut,Fi,ke,li,$n,Ui=`The most challenging trajectories from the test set are significantly improved, although
			still far from perfect:`,fi,Ie,mt,Bi,ci,Me,hi,Ae,ui,Ee,mi,Te,xe,pi,Qe,pt,Oi,ze,di,je,Pe,$i,Le,dt,Wi,gi,Se,wi,Re,vi,De,yi,Ne,_i,qe,Ge,bi,He,ki,tn,Bn,Ii,Ki;return q=new B({props:{href:"https://github.com/nrxszvo/mochaNN",$$slots:{default:[$o]},$$scope:{ctx:l}}}),W=new H({props:{$$slots:{default:[go]},$$scope:{ctx:l}}}),U=new H({props:{$$slots:{default:[wo]},$$scope:{ctx:l}}}),J=new B({props:{href:"https://www.youtube.com/@SabineHossenfelder",$$slots:{default:[vo]},$$scope:{ctx:l}}}),It=new B({props:{href:"https://www.youtube.com/playlist?list=PLMrJAkhIeNNTYaOnVI3QpH7jgULnAmvPA",$$slots:{default:[yo]},$$scope:{ctx:l}}}),Mt=new qn({props:{id:"lorenz",$$slots:{default:[_o]},$$scope:{ctx:l}}}),At=new H({props:{$$slots:{default:[ko]},$$scope:{ctx:l}}}),Tt=new et({props:{$$slots:{default:[Io]},$$scope:{ctx:l}}}),xt=new H({props:{$$slots:{default:[Mo]},$$scope:{ctx:l}}}),zt=new H({props:{$$slots:{default:[Ao]},$$scope:{ctx:l}}}),jt=new H({props:{$$slots:{default:[To]},$$scope:{ctx:l}}}),Pt=new qn({props:{id:"nhits",$$slots:{default:[xo]},$$scope:{ctx:l}}}),Lt=new H({props:{$$slots:{default:[zo]},$$scope:{ctx:l}}}),St=new H({props:{$$slots:{default:[jo]},$$scope:{ctx:l}}}),Rt=new H({props:{$$slots:{default:[Po]},$$scope:{ctx:l}}}),Dt=new qn({props:{id:"experiments",$$slots:{default:[Lo]},$$scope:{ctx:l}}}),Nt=new H({props:{$$slots:{default:[So]},$$scope:{ctx:l}}}),qt=new Gn({props:{id:"datagen",$$slots:{default:[Ro]},$$scope:{ctx:l}}}),Gt=new H({props:{$$slots:{default:[Go]},$$scope:{ctx:l}}}),Ht=new H({props:{$$slots:{default:[Fo]},$$scope:{ctx:l}}}),Ct=new H({props:{indent:"indent-0",$$slots:{default:[Uo]},$$scope:{ctx:l}}}),Vt=new H({props:{$$slots:{default:[Bo]},$$scope:{ctx:l}}}),Ft=new H({props:{indent:"indent-0",$$slots:{default:[Oo]},$$scope:{ctx:l}}}),Ut=new H({props:{$$slots:{default:[Wo]},$$scope:{ctx:l}}}),Bt=new Gn({props:{id:"model-1",$$slots:{default:[Ko]},$$scope:{ctx:l}}}),Ot=new H({props:{$$slots:{default:[Jo]},$$scope:{ctx:l}}}),Fe=new Mi({props:{hps:ho}}),Wt=new H({props:{indent:"indent-0",$$slots:{default:[Yo]},$$scope:{ctx:l}}}),Kt=new H({props:{$$slots:{default:[Xo]},$$scope:{ctx:l}}}),Yt=new et({props:{$$slots:{default:[Zo]},$$scope:{ctx:l}}}),Xt=new H({props:{$$slots:{default:[Qo]},$$scope:{ctx:l}}}),Qt=new et({props:{$$slots:{default:[tr]},$$scope:{ctx:l}}}),te=new H({props:{$$slots:{default:[er]},$$scope:{ctx:l}}}),ne=new et({props:{$$slots:{default:[nr]},$$scope:{ctx:l}}}),se=new H({props:{$$slots:{default:[ir]},$$scope:{ctx:l}}}),ie=new H({props:{$$slots:{default:[ar]},$$scope:{ctx:l}}}),oe=new et({props:{$$slots:{default:[or]},$$scope:{ctx:l}}}),re=new H({props:{$$slots:{default:[rr]},$$scope:{ctx:l}}}),le=new Gn({props:{id:"model-2",$$slots:{default:[lr]},$$scope:{ctx:l}}}),fe=new H({props:{$$slots:{default:[fr]},$$scope:{ctx:l}}}),Be=new Mi({props:{hps:uo}}),ce=new et({props:{$$slots:{default:[cr]},$$scope:{ctx:l}}}),he=new H({props:{$$slots:{default:[hr]},$$scope:{ctx:l}}}),ue=new H({props:{$$slots:{default:[ur]},$$scope:{ctx:l}}}),me=new et({props:{$$slots:{default:[mr]},$$scope:{ctx:l}}}),pe=new H({props:{$$slots:{default:[pr]},$$scope:{ctx:l}}}),de=new et({props:{$$slots:{default:[dr]},$$scope:{ctx:l}}}),$e=new H({props:{$$slots:{default:[$r]},$$scope:{ctx:l}}}),ge=new et({props:{$$slots:{default:[gr]},$$scope:{ctx:l}}}),we=new H({props:{$$slots:{default:[wr]},$$scope:{ctx:l}}}),ve=new H({props:{$$slots:{default:[vr]},$$scope:{ctx:l}}}),ye=new Gn({props:{id:"model-3",$$slots:{default:[yr]},$$scope:{ctx:l}}}),_e=new H({props:{$$slots:{default:[_r]},$$scope:{ctx:l}}}),Xe=new Mi({props:{hps:mo}}),be=new H({props:{style:"my-4 text-xs font-serif",$$slots:{default:[kr]},$$scope:{ctx:l}}}),ke=new et({props:{$$slots:{default:[Ir]},$$scope:{ctx:l}}}),Me=new et({props:{$$slots:{default:[Mr]},$$scope:{ctx:l}}}),Ae=new H({props:{$$slots:{default:[Ar]},$$scope:{ctx:l}}}),Ee=new Gn({props:{id:"arpred",$$slots:{default:[Er]},$$scope:{ctx:l}}}),Te=new H({props:{$$slots:{default:[Tr]},$$scope:{ctx:l}}}),xe=new H({props:{$$slots:{default:[xr]},$$scope:{ctx:l}}}),ze=new et({props:{$$slots:{default:[zr]},$$scope:{ctx:l}}}),je=new H({props:{$$slots:{default:[Pr]},$$scope:{ctx:l}}}),Pe=new H({props:{$$slots:{default:[Dr]},$$scope:{ctx:l}}}),Se=new et({props:{$$slots:{default:[qr]},$$scope:{ctx:l}}}),Re=new H({props:{$$slots:{default:[Gr]},$$scope:{ctx:l}}}),De=new qn({props:{id:"discussion",$$slots:{default:[Hr]},$$scope:{ctx:l}}}),Ne=new H({props:{$$slots:{default:[Cr]},$$scope:{ctx:l}}}),qe=new H({props:{$$slots:{default:[Vr]},$$scope:{ctx:l}}}),Ge=new H({props:{$$slots:{default:[Ur]},$$scope:{ctx:l}}}),He=new qn({props:{id:"references",$$slots:{default:[Br]},$$scope:{ctx:l}}}),tn=new Wa({}),{c(){e=j("div"),t=j("aside"),t.innerHTML=s,i=x(),n=j("div"),a=j("div"),a.textContent=r,m=x(),p=j("div"),p.textContent=M,S=x(),D=j("br"),L=x(),E=j("div"),A=f("(All code used in this project is available in the github repo: "),_(q.$$.fragment),T=f(")"),G=x(),_(W.$$.fragment),_(U.$$.fragment),X=x(),K=j("div"),tt=j("p"),F=j("sup"),V=f(`1. For a quick and entertaining way to stay informed of new developments in the
					world of DL for dynamical systems modeling, I highly recommend Sabine
					Hossenfelder's `),_(J.$$.fragment),R=x(),C=j("p"),C.innerHTML=_t,st=x(),bt=j("p"),kt=j("sup"),Kn=f(`3. If you would like to refresh your background on dynamical systems theory, I
					highly recommend Steve Brunton's free `),_(It.$$.fragment),Jn=f(" on the subject"),Yn=x(),_(Mt.$$.fragment),Xn=x(),_(At.$$.fragment),Zn=x(),un=j("div"),Ce=j("a"),Ve=j("figure"),Et=j("img"),Qn=x(),_(Tt.$$.fragment),ts=x(),_(xt.$$.fragment),es=x(),mn=j("div"),kn=j("p"),ns=f(Ti),ss=x(),_(zt.$$.fragment),is=x(),pn=j("div"),In=j("p"),as=f(xi),os=x(),_(jt.$$.fragment),rs=x(),_(Pt.$$.fragment),ls=x(),_(Lt.$$.fragment),fs=x(),_(St.$$.fragment),cs=x(),_(Rt.$$.fragment),hs=x(),_(Dt.$$.fragment),us=x(),_(Nt.$$.fragment),_(qt.$$.fragment),_(Gt.$$.fragment),_(Ht.$$.fragment),ms=x(),dn=j("div"),Mn=j("p"),ps=f(zi),ds=x(),_(Ct.$$.fragment),_(Vt.$$.fragment),$s=x(),_(Ft.$$.fragment),gs=x(),_(Ut.$$.fragment),ws=x(),_(Bt.$$.fragment),vs=x(),_(Ot.$$.fragment),ys=x(),_(Fe.$$.fragment),_s=x(),_(Wt.$$.fragment),bs=x(),An=j("div"),ks=f(ji),Is=x(),_(Kt.$$.fragment),Ms=x(),Jt=j("figure"),at=j("img"),As=x(),_(Yt.$$.fragment),Es=x(),_(Xt.$$.fragment),Ts=x(),Zt=j("figure"),Ue=j("div"),Ue.innerHTML=Li,xs=x(),_(Qt.$$.fragment),zs=x(),_(te.$$.fragment),js=x(),ee=j("figure"),ot=j("img"),Ps=x(),_(ne.$$.fragment),Ls=x(),_(se.$$.fragment),_(ie.$$.fragment),Ss=x(),ae=j("figure"),rt=j("img"),Rs=x(),_(oe.$$.fragment),Ds=x(),_(re.$$.fragment),Ns=x(),_(le.$$.fragment),qs=x(),_(fe.$$.fragment),Gs=x(),_(Be.$$.fragment),Hs=x(),Oe=j("p"),Oe.textContent=Di,Cs=x(),We=j("figure"),lt=j("img"),_(ce.$$.fragment),Vs=x(),_(he.$$.fragment),_(ue.$$.fragment),Fs=x(),Ke=j("figure"),ft=j("img"),_(me.$$.fragment),Us=x(),_(pe.$$.fragment),Bs=x(),Je=j("figure"),ct=j("img"),Os=x(),_(de.$$.fragment),Ws=x(),_($e.$$.fragment),Ks=x(),Ye=j("figure"),ht=j("img"),_(ge.$$.fragment),Js=x(),_(we.$$.fragment),Ys=x(),_(ve.$$.fragment),Xs=x(),_(ye.$$.fragment),Zs=x(),_(_e.$$.fragment),Qs=x(),_(Xe.$$.fragment),ti=x(),_(be.$$.fragment),ei=x(),it=j("p"),ni=f(`After retraining Model 2 with a new dataset that samples the Lorenz Attractor
			trajectories with `),si=f(Ci),ii=f(`, we see that we are now able to predict all
			regions of the test set with sMAPE error `),ai=f(Vi),oi=f(":"),ri=x(),Ze=j("figure"),ut=j("img"),_(ke.$$.fragment),li=x(),$n=j("p"),$n.textContent=Ui,fi=x(),Ie=j("figure"),mt=j("img"),ci=x(),_(Me.$$.fragment),hi=x(),_(Ae.$$.fragment),ui=x(),_(Ee.$$.fragment),mi=x(),_(Te.$$.fragment),_(xe.$$.fragment),pi=x(),Qe=j("figure"),pt=j("img"),_(ze.$$.fragment),di=x(),_(je.$$.fragment),_(Pe.$$.fragment),$i=x(),Le=j("figure"),dt=j("img"),gi=x(),_(Se.$$.fragment),wi=x(),_(Re.$$.fragment),vi=x(),_(De.$$.fragment),yi=x(),_(Ne.$$.fragment),_i=x(),_(qe.$$.fragment),_(Ge.$$.fragment),bi=x(),_(He.$$.fragment),ki=x(),_(tn.$$.fragment),this.h()},l(h){e=P(h,"DIV",{class:!0});var g=N(e);t=P(g,"ASIDE",{"data-svelte-h":!0}),Q(t)!=="svelte-1jc65ah"&&(t.innerHTML=s),i=z(g),n=P(g,"DIV",{class:!0});var $=N(n);a=P($,"DIV",{id:!0,class:!0,"data-svelte-h":!0}),Q(a)!=="svelte-g7eo4d"&&(a.textContent=r),m=z($),p=P($,"DIV",{class:!0,"data-svelte-h":!0}),Q(p)!=="svelte-hve5fy"&&(p.textContent=M),S=z($),D=P($,"BR",{}),L=z($),E=P($,"DIV",{class:!0});var en=N(E);A=c(en,"(All code used in this project is available in the github repo: "),b(q.$$.fragment,en),T=c(en,")"),en.forEach(o),G=z($),b(W.$$.fragment,$),b(U.$$.fragment,$),X=z($),K=P($,"DIV",{class:!0});var $t=N(K);tt=P($t,"P",{});var En=N(tt);F=P(En,"SUP",{id:!0});var gn=N(F);V=c(gn,`1. For a quick and entertaining way to stay informed of new developments in the
					world of DL for dynamical systems modeling, I highly recommend Sabine
					Hossenfelder's `),b(J.$$.fragment,gn),gn.forEach(o),En.forEach(o),R=z($t),C=P($t,"P",{"data-svelte-h":!0}),Q(C)!=="svelte-7qdhvn"&&(C.innerHTML=_t),st=z($t),bt=P($t,"P",{});var Tn=N(bt);kt=P(Tn,"SUP",{id:!0});var nn=N(kt);Kn=c(nn,`3. If you would like to refresh your background on dynamical systems theory, I
					highly recommend Steve Brunton's free `),b(It.$$.fragment,nn),Jn=c(nn," on the subject"),nn.forEach(o),Tn.forEach(o),$t.forEach(o),Yn=z($),b(Mt.$$.fragment,$),Xn=z($),b(At.$$.fragment,$),Zn=z($),un=P($,"DIV",{class:!0});var xn=N(un);Ce=P(xn,"A",{title:!0,href:!0});var zn=N(Ce);Ve=P(zn,"FIGURE",{});var sn=N(Ve);Et=P(sn,"IMG",{class:!0,width:!0,alt:!0,src:!0}),Qn=z(sn),b(Tt.$$.fragment,sn),sn.forEach(o),zn.forEach(o),xn.forEach(o),ts=z($),b(xt.$$.fragment,$),es=z($),mn=P($,"DIV",{class:!0});var jn=N(mn);kn=P(jn,"P",{});var Pn=N(kn);ns=c(Pn,Ti),Pn.forEach(o),jn.forEach(o),ss=z($),b(zt.$$.fragment,$),is=z($),pn=P($,"DIV",{class:!0});var Ln=N(pn);In=P(Ln,"P",{});var Sn=N(In);as=c(Sn,xi),Sn.forEach(o),Ln.forEach(o),os=z($),b(jt.$$.fragment,$),rs=z($),b(Pt.$$.fragment,$),ls=z($),b(Lt.$$.fragment,$),fs=z($),b(St.$$.fragment,$),cs=z($),b(Rt.$$.fragment,$),hs=z($),b(Dt.$$.fragment,$),us=z($),b(Nt.$$.fragment,$),b(qt.$$.fragment,$),b(Gt.$$.fragment,$),b(Ht.$$.fragment,$),ms=z($),dn=P($,"DIV",{class:!0});var Rn=N(dn);Mn=P(Rn,"P",{});var Dn=N(Mn);ps=c(Dn,zi),Dn.forEach(o),Rn.forEach(o),ds=z($),b(Ct.$$.fragment,$),b(Vt.$$.fragment,$),$s=z($),b(Ft.$$.fragment,$),gs=z($),b(Ut.$$.fragment,$),ws=z($),b(Bt.$$.fragment,$),vs=z($),b(Ot.$$.fragment,$),ys=z($),b(Fe.$$.fragment,$),_s=z($),b(Wt.$$.fragment,$),bs=z($),An=P($,"DIV",{});var Nn=N(An);ks=c(Nn,ji),Nn.forEach(o),Is=z($),b(Kt.$$.fragment,$),Ms=z($),Jt=P($,"FIGURE",{class:!0});var an=N(Jt);at=P(an,"IMG",{class:!0,src:!0,alt:!0,width:!0,height:!0}),As=z(an),b(Yt.$$.fragment,an),an.forEach(o),Es=z($),b(Xt.$$.fragment,$),Ts=z($),Zt=P($,"FIGURE",{class:!0});var on=N(Zt);Ue=P(on,"DIV",{class:!0,"data-svelte-h":!0}),Q(Ue)!=="svelte-1hknb0z"&&(Ue.innerHTML=Li),xs=z(on),b(Qt.$$.fragment,on),on.forEach(o),zs=z($),b(te.$$.fragment,$),js=z($),ee=P($,"FIGURE",{class:!0});var rn=N(ee);ot=P(rn,"IMG",{class:!0,src:!0,alt:!0,width:!0,height:!0}),Ps=z(rn),b(ne.$$.fragment,rn),rn.forEach(o),Ls=z($),b(se.$$.fragment,$),b(ie.$$.fragment,$),Ss=z($),ae=P($,"FIGURE",{class:!0});var ln=N(ae);rt=P(ln,"IMG",{class:!0,src:!0,alt:!0,width:!0,height:!0}),Rs=z(ln),b(oe.$$.fragment,ln),ln.forEach(o),Ds=z($),b(re.$$.fragment,$),Ns=z($),b(le.$$.fragment,$),qs=z($),b(fe.$$.fragment,$),Gs=z($),b(Be.$$.fragment,$),Hs=z($),Oe=P($,"P",{class:!0,"data-svelte-h":!0}),Q(Oe)!=="svelte-8lzr6y"&&(Oe.textContent=Di),Cs=z($),We=P($,"FIGURE",{class:!0});var wn=N(We);lt=P(wn,"IMG",{class:!0,src:!0,alt:!0,width:!0,height:!0}),b(ce.$$.fragment,wn),wn.forEach(o),Vs=z($),b(he.$$.fragment,$),b(ue.$$.fragment,$),Fs=z($),Ke=P($,"FIGURE",{class:!0});var vn=N(Ke);ft=P(vn,"IMG",{class:!0,src:!0,alt:!0,width:!0,height:!0}),b(me.$$.fragment,vn),vn.forEach(o),Us=z($),b(pe.$$.fragment,$),Bs=z($),Je=P($,"FIGURE",{});var fn=N(Je);ct=P(fn,"IMG",{class:!0,src:!0,alt:!0,width:!0,height:!0}),Os=z(fn),b(de.$$.fragment,fn),fn.forEach(o),Ws=z($),b($e.$$.fragment,$),Ks=z($),Ye=P($,"FIGURE",{class:!0});var yn=N(Ye);ht=P(yn,"IMG",{class:!0,src:!0,alt:!0,width:!0,height:!0}),b(ge.$$.fragment,yn),yn.forEach(o),Js=z($),b(we.$$.fragment,$),Ys=z($),b(ve.$$.fragment,$),Xs=z($),b(ye.$$.fragment,$),Zs=z($),b(_e.$$.fragment,$),Qs=z($),b(Xe.$$.fragment,$),ti=z($),b(be.$$.fragment,$),ei=z($),it=P($,"P",{class:!0});var gt=N(it);ni=c(gt,`After retraining Model 2 with a new dataset that samples the Lorenz Attractor
			trajectories with `),si=c(gt,Ci),ii=c(gt,`, we see that we are now able to predict all
			regions of the test set with sMAPE error `),ai=c(gt,Vi),oi=c(gt,":"),gt.forEach(o),ri=z($),Ze=P($,"FIGURE",{class:!0});var _n=N(Ze);ut=P(_n,"IMG",{class:!0,src:!0,alt:!0,width:!0,height:!0}),b(ke.$$.fragment,_n),_n.forEach(o),li=z($),$n=P($,"P",{"data-svelte-h":!0}),Q($n)!=="svelte-8g9ay2"&&($n.textContent=Ui),fi=z($),Ie=P($,"FIGURE",{class:!0});var cn=N(Ie);mt=P(cn,"IMG",{class:!0,src:!0,alt:!0,width:!0,height:!0}),ci=z(cn),b(Me.$$.fragment,cn),cn.forEach(o),hi=z($),b(Ae.$$.fragment,$),ui=z($),b(Ee.$$.fragment,$),mi=z($),b(Te.$$.fragment,$),b(xe.$$.fragment,$),pi=z($),Qe=P($,"FIGURE",{class:!0});var bn=N(Qe);pt=P(bn,"IMG",{class:!0,src:!0,alt:!0,width:!0,height:!0}),b(ze.$$.fragment,bn),bn.forEach(o),di=z($),b(je.$$.fragment,$),b(Pe.$$.fragment,$),$i=z($),Le=P($,"FIGURE",{class:!0});var hn=N(Le);dt=P(hn,"IMG",{class:!0,src:!0,alt:!0,width:!0,height:!0}),gi=z(hn),b(Se.$$.fragment,hn),hn.forEach(o),wi=z($),b(Re.$$.fragment,$),vi=z($),b(De.$$.fragment,$),yi=z($),b(Ne.$$.fragment,$),_i=z($),b(qe.$$.fragment,$),b(Ge.$$.fragment,$),bi=z($),b(He.$$.fragment,$),ki=z($),b(tn.$$.fragment,$),$.forEach(o),g.forEach(o),this.h()},h(){y(a,"id","intro"),y(a,"class","mt-8 text-2xl text-center"),y(p,"class","mt-2 text-sm text-center"),y(E,"class","text-sm text-center font-serif mb-4"),y(F,"id","sabine"),y(kt,"id","brunton"),y(K,"class","my-4 ms-4 -indent-4 font-serif leading-4"),y(Et,"class","m-auto"),y(Et,"width","128"),y(Et,"alt","A Trajectory Through Phase Space in a Lorenz Attractor"),nt(Et.src,Ei="https://upload.wikimedia.org/wikipedia/commons/1/13/A_Trajectory_Through_Phase_Space_in_a_Lorenz_Attractor.gif")||y(Et,"src",Ei),y(Ce,"title","Dan Quinn, CC BY-SA 3.0 <https://creativecommons.org/licenses/by-sa/3.0>, via Wikimedia Commons"),y(Ce,"href","https://commons.wikimedia.org/wiki/File:A_Trajectory_Through_Phase_Space_in_a_Lorenz_Attractor.gif"),y(un,"class","my-2 self-center"),y(mn,"class","self-center"),y(pn,"class","self-center"),y(dn,"class","self-center"),y(at,"class","m-auto"),nt(at.src,Pi=`${Z}/Model1ErrDist.png`)||y(at,"src",Pi),y(at,"alt",""),y(at,"width","600"),y(at,"height","600"),y(Jt,"class","mb-6 self-center"),y(Ue,"class","flex flex-wrap justify-center"),y(Zt,"class","mt-6 mb-6 self-center"),y(ot,"class","m-auto"),nt(ot.src,Si=`${Z}/model-1-pod.gif`)||y(ot,"src",Si),y(ot,"alt","prediction point of divergence"),y(ot,"width","450"),y(ot,"height","350"),y(ee,"class","mt-6 mb-6 self-center"),y(rt,"class","m-auto"),nt(rt.src,Ri=`${Z}/trajectories.gif`)||y(rt,"src",Ri),y(rt,"alt","trajectories approaching origin"),y(rt,"width","350"),y(rt,"height","300"),y(ae,"class","mt-6 mb-6 self-center"),y(Oe,"class","mt-2"),y(lt,"class","m-auto"),nt(lt.src,Ni=`${Z}/Model2ErrDist.png`)||y(lt,"src",Ni),y(lt,"alt",""),y(lt,"width","600"),y(lt,"height","600"),y(We,"class","mb-2 self-center"),y(ft,"class","m-auto"),nt(ft.src,qi=`${Z}/Model2Err3d.png`)||y(ft,"src",qi),y(ft,"alt",""),y(ft,"width","600"),y(ft,"height","600"),y(Ke,"class","-mt-2 mb-2 self-center"),y(ct,"class","m-auto"),nt(ct.src,Gi=`${Z}/model-2-pod.gif`)||y(ct,"src",Gi),y(ct,"alt","model 2 point of divergence"),y(ct,"width","400"),y(ct,"height","340"),y(ht,"class","m-auto"),nt(ht.src,Hi=`${Z}/Model2DFO.png`)||y(ht,"src",Hi),y(ht,"alt","distance from origin vs. sMAPE"),y(ht,"width","800"),y(ht,"height","600"),y(Ye,"class","-mt-2 mb-2 self-center"),y(it,"class","mt-4"),y(ut,"class","m-auto"),nt(ut.src,Fi=`${Z}/Model2vModel3.png`)||y(ut,"src",Fi),y(ut,"alt",""),y(ut,"width","600"),y(ut,"height","600"),y(Ze,"class","mb-8 self-center"),y(mt,"class","m-auto"),nt(mt.src,Bi=`${Z}/model-3-low-dfo.gif`)||y(mt,"src",Bi),y(mt,"alt","Model 3 trajectory example"),y(mt,"width","800"),y(mt,"height","800"),y(Ie,"class","my-8 self-center"),y(pt,"class","m-auto"),nt(pt.src,Oi=`${Z}/ref_v_ar.png`)||y(pt,"src",Oi),y(pt,"alt","Reference vs. Autoregressive Trajectories"),y(pt,"width","450"),y(pt,"height","500"),y(Qe,"class","mb-2 self-center"),y(dt,"class","m-auto"),nt(dt.src,Wi=`${Z}/rk45_vs_nhits.png`)||y(dt,"src",Wi),y(dt,"alt","Comparison of RK45 and predictions to Radua's solutions"),y(dt,"width","600"),y(dt,"height","600"),y(Le,"class","mb-2 self-center"),y(n,"class","flex flex-col mx-4 sm:mx-16"),y(e,"class","grid grid-flow-col auto-cols-auto")},m(h,g){u(h,e,g),d(e,t),d(e,i),d(e,n),d(n,a),d(n,m),d(n,p),d(n,S),d(n,D),d(n,L),d(n,E),d(E,A),k(q,E,null),d(E,T),d(n,G),k(W,n,null),k(U,n,null),d(n,X),d(n,K),d(K,tt),d(tt,F),d(F,V),k(J,F,null),d(K,R),d(K,C),d(K,st),d(K,bt),d(bt,kt),d(kt,Kn),k(It,kt,null),d(kt,Jn),d(n,Yn),k(Mt,n,null),d(n,Xn),k(At,n,null),d(n,Zn),d(n,un),d(un,Ce),d(Ce,Ve),d(Ve,Et),d(Ve,Qn),k(Tt,Ve,null),d(n,ts),k(xt,n,null),d(n,es),d(n,mn),d(mn,kn),d(kn,ns),d(n,ss),k(zt,n,null),d(n,is),d(n,pn),d(pn,In),d(In,as),d(n,os),k(jt,n,null),d(n,rs),k(Pt,n,null),d(n,ls),k(Lt,n,null),d(n,fs),k(St,n,null),d(n,cs),k(Rt,n,null),d(n,hs),k(Dt,n,null),d(n,us),k(Nt,n,null),k(qt,n,null),k(Gt,n,null),k(Ht,n,null),d(n,ms),d(n,dn),d(dn,Mn),d(Mn,ps),d(n,ds),k(Ct,n,null),k(Vt,n,null),d(n,$s),k(Ft,n,null),d(n,gs),k(Ut,n,null),d(n,ws),k(Bt,n,null),d(n,vs),k(Ot,n,null),d(n,ys),k(Fe,n,null),d(n,_s),k(Wt,n,null),d(n,bs),d(n,An),d(An,ks),d(n,Is),k(Kt,n,null),d(n,Ms),d(n,Jt),d(Jt,at),d(Jt,As),k(Yt,Jt,null),d(n,Es),k(Xt,n,null),d(n,Ts),d(n,Zt),d(Zt,Ue),d(Zt,xs),k(Qt,Zt,null),d(n,zs),k(te,n,null),d(n,js),d(n,ee),d(ee,ot),d(ee,Ps),k(ne,ee,null),d(n,Ls),k(se,n,null),k(ie,n,null),d(n,Ss),d(n,ae),d(ae,rt),d(ae,Rs),k(oe,ae,null),d(n,Ds),k(re,n,null),d(n,Ns),k(le,n,null),d(n,qs),k(fe,n,null),d(n,Gs),k(Be,n,null),d(n,Hs),d(n,Oe),d(n,Cs),d(n,We),d(We,lt),k(ce,We,null),d(n,Vs),k(he,n,null),k(ue,n,null),d(n,Fs),d(n,Ke),d(Ke,ft),k(me,Ke,null),d(n,Us),k(pe,n,null),d(n,Bs),d(n,Je),d(Je,ct),d(Je,Os),k(de,Je,null),d(n,Ws),k($e,n,null),d(n,Ks),d(n,Ye),d(Ye,ht),k(ge,Ye,null),d(n,Js),k(we,n,null),d(n,Ys),k(ve,n,null),d(n,Xs),k(ye,n,null),d(n,Zs),k(_e,n,null),d(n,Qs),k(Xe,n,null),d(n,ti),k(be,n,null),d(n,ei),d(n,it),d(it,ni),d(it,si),d(it,ii),d(it,ai),d(it,oi),d(n,ri),d(n,Ze),d(Ze,ut),k(ke,Ze,null),d(n,li),d(n,$n),d(n,fi),d(n,Ie),d(Ie,mt),d(Ie,ci),k(Me,Ie,null),d(n,hi),k(Ae,n,null),d(n,ui),k(Ee,n,null),d(n,mi),k(Te,n,null),k(xe,n,null),d(n,pi),d(n,Qe),d(Qe,pt),k(ze,Qe,null),d(n,di),k(je,n,null),k(Pe,n,null),d(n,$i),d(n,Le),d(Le,dt),d(Le,gi),k(Se,Le,null),d(n,wi),k(Re,n,null),d(n,vi),k(De,n,null),d(n,yi),k(Ne,n,null),d(n,_i),k(qe,n,null),k(Ge,n,null),d(n,bi),k(He,n,null),d(n,ki),k(tn,n,null),Bn=!0,Ii||(Ki=Ca(po,"scroll",l[0]),Ii=!0)},p(h,[g]){const $={};g&256&&($.$$scope={dirty:g,ctx:h}),q.$set($);const en={};g&256&&(en.$$scope={dirty:g,ctx:h}),W.$set(en);const $t={};g&256&&($t.$$scope={dirty:g,ctx:h}),U.$set($t);const En={};g&256&&(En.$$scope={dirty:g,ctx:h}),J.$set(En);const gn={};g&256&&(gn.$$scope={dirty:g,ctx:h}),It.$set(gn);const Tn={};g&256&&(Tn.$$scope={dirty:g,ctx:h}),Mt.$set(Tn);const nn={};g&256&&(nn.$$scope={dirty:g,ctx:h}),At.$set(nn);const xn={};g&256&&(xn.$$scope={dirty:g,ctx:h}),Tt.$set(xn);const zn={};g&256&&(zn.$$scope={dirty:g,ctx:h}),xt.$set(zn);const sn={};g&256&&(sn.$$scope={dirty:g,ctx:h}),zt.$set(sn);const jn={};g&256&&(jn.$$scope={dirty:g,ctx:h}),jt.$set(jn);const Pn={};g&256&&(Pn.$$scope={dirty:g,ctx:h}),Pt.$set(Pn);const Ln={};g&256&&(Ln.$$scope={dirty:g,ctx:h}),Lt.$set(Ln);const Sn={};g&256&&(Sn.$$scope={dirty:g,ctx:h}),St.$set(Sn);const Rn={};g&256&&(Rn.$$scope={dirty:g,ctx:h}),Rt.$set(Rn);const Dn={};g&256&&(Dn.$$scope={dirty:g,ctx:h}),Dt.$set(Dn);const Nn={};g&256&&(Nn.$$scope={dirty:g,ctx:h}),Nt.$set(Nn);const an={};g&256&&(an.$$scope={dirty:g,ctx:h}),qt.$set(an);const on={};g&256&&(on.$$scope={dirty:g,ctx:h}),Gt.$set(on);const rn={};g&256&&(rn.$$scope={dirty:g,ctx:h}),Ht.$set(rn);const ln={};g&256&&(ln.$$scope={dirty:g,ctx:h}),Ct.$set(ln);const wn={};g&256&&(wn.$$scope={dirty:g,ctx:h}),Vt.$set(wn);const vn={};g&256&&(vn.$$scope={dirty:g,ctx:h}),Ft.$set(vn);const fn={};g&256&&(fn.$$scope={dirty:g,ctx:h}),Ut.$set(fn);const yn={};g&256&&(yn.$$scope={dirty:g,ctx:h}),Bt.$set(yn);const gt={};g&256&&(gt.$$scope={dirty:g,ctx:h}),Ot.$set(gt);const _n={};g&256&&(_n.$$scope={dirty:g,ctx:h}),Wt.$set(_n);const cn={};g&256&&(cn.$$scope={dirty:g,ctx:h}),Kt.$set(cn);const bn={};g&256&&(bn.$$scope={dirty:g,ctx:h}),Yt.$set(bn);const hn={};g&256&&(hn.$$scope={dirty:g,ctx:h}),Xt.$set(hn);const Ji={};g&256&&(Ji.$$scope={dirty:g,ctx:h}),Qt.$set(Ji);const Yi={};g&256&&(Yi.$$scope={dirty:g,ctx:h}),te.$set(Yi);const Xi={};g&256&&(Xi.$$scope={dirty:g,ctx:h}),ne.$set(Xi);const Zi={};g&256&&(Zi.$$scope={dirty:g,ctx:h}),se.$set(Zi);const Qi={};g&256&&(Qi.$$scope={dirty:g,ctx:h}),ie.$set(Qi);const ta={};g&256&&(ta.$$scope={dirty:g,ctx:h}),oe.$set(ta);const ea={};g&256&&(ea.$$scope={dirty:g,ctx:h}),re.$set(ea);const na={};g&256&&(na.$$scope={dirty:g,ctx:h}),le.$set(na);const sa={};g&256&&(sa.$$scope={dirty:g,ctx:h}),fe.$set(sa);const ia={};g&256&&(ia.$$scope={dirty:g,ctx:h}),ce.$set(ia);const aa={};g&256&&(aa.$$scope={dirty:g,ctx:h}),he.$set(aa);const oa={};g&256&&(oa.$$scope={dirty:g,ctx:h}),ue.$set(oa);const ra={};g&256&&(ra.$$scope={dirty:g,ctx:h}),me.$set(ra);const la={};g&256&&(la.$$scope={dirty:g,ctx:h}),pe.$set(la);const fa={};g&256&&(fa.$$scope={dirty:g,ctx:h}),de.$set(fa);const ca={};g&256&&(ca.$$scope={dirty:g,ctx:h}),$e.$set(ca);const ha={};g&256&&(ha.$$scope={dirty:g,ctx:h}),ge.$set(ha);const ua={};g&256&&(ua.$$scope={dirty:g,ctx:h}),we.$set(ua);const ma={};g&256&&(ma.$$scope={dirty:g,ctx:h}),ve.$set(ma);const pa={};g&256&&(pa.$$scope={dirty:g,ctx:h}),ye.$set(pa);const da={};g&256&&(da.$$scope={dirty:g,ctx:h}),_e.$set(da);const $a={};g&256&&($a.$$scope={dirty:g,ctx:h}),be.$set($a);const ga={};g&256&&(ga.$$scope={dirty:g,ctx:h}),ke.$set(ga);const wa={};g&256&&(wa.$$scope={dirty:g,ctx:h}),Me.$set(wa);const va={};g&256&&(va.$$scope={dirty:g,ctx:h}),Ae.$set(va);const ya={};g&256&&(ya.$$scope={dirty:g,ctx:h}),Ee.$set(ya);const _a={};g&256&&(_a.$$scope={dirty:g,ctx:h}),Te.$set(_a);const ba={};g&256&&(ba.$$scope={dirty:g,ctx:h}),xe.$set(ba);const ka={};g&256&&(ka.$$scope={dirty:g,ctx:h}),ze.$set(ka);const Ia={};g&256&&(Ia.$$scope={dirty:g,ctx:h}),je.$set(Ia);const Ma={};g&256&&(Ma.$$scope={dirty:g,ctx:h}),Pe.$set(Ma);const Aa={};g&256&&(Aa.$$scope={dirty:g,ctx:h}),Se.$set(Aa);const Ea={};g&256&&(Ea.$$scope={dirty:g,ctx:h}),Re.$set(Ea);const Ta={};g&256&&(Ta.$$scope={dirty:g,ctx:h}),De.$set(Ta);const xa={};g&256&&(xa.$$scope={dirty:g,ctx:h}),Ne.$set(xa);const za={};g&256&&(za.$$scope={dirty:g,ctx:h}),qe.$set(za);const ja={};g&256&&(ja.$$scope={dirty:g,ctx:h}),Ge.$set(ja);const Pa={};g&256&&(Pa.$$scope={dirty:g,ctx:h}),He.$set(Pa)},i(h){Bn||(w(q.$$.fragment,h),w(W.$$.fragment,h),w(U.$$.fragment,h),w(J.$$.fragment,h),w(It.$$.fragment,h),w(Mt.$$.fragment,h),w(At.$$.fragment,h),w(Tt.$$.fragment,h),w(xt.$$.fragment,h),w(zt.$$.fragment,h),w(jt.$$.fragment,h),w(Pt.$$.fragment,h),w(Lt.$$.fragment,h),w(St.$$.fragment,h),w(Rt.$$.fragment,h),w(Dt.$$.fragment,h),w(Nt.$$.fragment,h),w(qt.$$.fragment,h),w(Gt.$$.fragment,h),w(Ht.$$.fragment,h),w(Ct.$$.fragment,h),w(Vt.$$.fragment,h),w(Ft.$$.fragment,h),w(Ut.$$.fragment,h),w(Bt.$$.fragment,h),w(Ot.$$.fragment,h),w(Fe.$$.fragment,h),w(Wt.$$.fragment,h),w(Kt.$$.fragment,h),w(Yt.$$.fragment,h),w(Xt.$$.fragment,h),w(Qt.$$.fragment,h),w(te.$$.fragment,h),w(ne.$$.fragment,h),w(se.$$.fragment,h),w(ie.$$.fragment,h),w(oe.$$.fragment,h),w(re.$$.fragment,h),w(le.$$.fragment,h),w(fe.$$.fragment,h),w(Be.$$.fragment,h),w(ce.$$.fragment,h),w(he.$$.fragment,h),w(ue.$$.fragment,h),w(me.$$.fragment,h),w(pe.$$.fragment,h),w(de.$$.fragment,h),w($e.$$.fragment,h),w(ge.$$.fragment,h),w(we.$$.fragment,h),w(ve.$$.fragment,h),w(ye.$$.fragment,h),w(_e.$$.fragment,h),w(Xe.$$.fragment,h),w(be.$$.fragment,h),w(ke.$$.fragment,h),w(Me.$$.fragment,h),w(Ae.$$.fragment,h),w(Ee.$$.fragment,h),w(Te.$$.fragment,h),w(xe.$$.fragment,h),w(ze.$$.fragment,h),w(je.$$.fragment,h),w(Pe.$$.fragment,h),w(Se.$$.fragment,h),w(Re.$$.fragment,h),w(De.$$.fragment,h),w(Ne.$$.fragment,h),w(qe.$$.fragment,h),w(Ge.$$.fragment,h),w(He.$$.fragment,h),w(tn.$$.fragment,h),Bn=!0)},o(h){v(q.$$.fragment,h),v(W.$$.fragment,h),v(U.$$.fragment,h),v(J.$$.fragment,h),v(It.$$.fragment,h),v(Mt.$$.fragment,h),v(At.$$.fragment,h),v(Tt.$$.fragment,h),v(xt.$$.fragment,h),v(zt.$$.fragment,h),v(jt.$$.fragment,h),v(Pt.$$.fragment,h),v(Lt.$$.fragment,h),v(St.$$.fragment,h),v(Rt.$$.fragment,h),v(Dt.$$.fragment,h),v(Nt.$$.fragment,h),v(qt.$$.fragment,h),v(Gt.$$.fragment,h),v(Ht.$$.fragment,h),v(Ct.$$.fragment,h),v(Vt.$$.fragment,h),v(Ft.$$.fragment,h),v(Ut.$$.fragment,h),v(Bt.$$.fragment,h),v(Ot.$$.fragment,h),v(Fe.$$.fragment,h),v(Wt.$$.fragment,h),v(Kt.$$.fragment,h),v(Yt.$$.fragment,h),v(Xt.$$.fragment,h),v(Qt.$$.fragment,h),v(te.$$.fragment,h),v(ne.$$.fragment,h),v(se.$$.fragment,h),v(ie.$$.fragment,h),v(oe.$$.fragment,h),v(re.$$.fragment,h),v(le.$$.fragment,h),v(fe.$$.fragment,h),v(Be.$$.fragment,h),v(ce.$$.fragment,h),v(he.$$.fragment,h),v(ue.$$.fragment,h),v(me.$$.fragment,h),v(pe.$$.fragment,h),v(de.$$.fragment,h),v($e.$$.fragment,h),v(ge.$$.fragment,h),v(we.$$.fragment,h),v(ve.$$.fragment,h),v(ye.$$.fragment,h),v(_e.$$.fragment,h),v(Xe.$$.fragment,h),v(be.$$.fragment,h),v(ke.$$.fragment,h),v(Me.$$.fragment,h),v(Ae.$$.fragment,h),v(Ee.$$.fragment,h),v(Te.$$.fragment,h),v(xe.$$.fragment,h),v(ze.$$.fragment,h),v(je.$$.fragment,h),v(Pe.$$.fragment,h),v(Se.$$.fragment,h),v(Re.$$.fragment,h),v(De.$$.fragment,h),v(Ne.$$.fragment,h),v(qe.$$.fragment,h),v(Ge.$$.fragment,h),v(He.$$.fragment,h),v(tn.$$.fragment,h),Bn=!1},d(h){h&&o(e),I(q),I(W),I(U),I(J),I(It),I(Mt),I(At),I(Tt),I(xt),I(zt),I(jt),I(Pt),I(Lt),I(St),I(Rt),I(Dt),I(Nt),I(qt),I(Gt),I(Ht),I(Ct),I(Vt),I(Ft),I(Ut),I(Bt),I(Ot),I(Fe),I(Wt),I(Kt),I(Yt),I(Xt),I(Qt),I(te),I(ne),I(se),I(ie),I(oe),I(re),I(le),I(fe),I(Be),I(ce),I(he),I(ue),I(me),I(pe),I(de),I($e),I(ge),I(we),I(ve),I(ye),I(_e),I(Xe),I(be),I(ke),I(Me),I(Ae),I(Ee),I(Te),I(xe),I(ze),I(je),I(Pe),I(Se),I(Re),I(De),I(Ne),I(qe),I(Ge),I(He),I(tn),Ii=!1,Ki()}}}function Wr(l){const e=["intro","lorenz","nhits","experiments","datagen","model-1","model-2","model-3","arpred","discussion","references"],t=["bg-rose-100","rounded"];let s=e[0];const i=m=>{const p="a[href='#"+m+"']";return document.querySelector(p)},n=m=>{const p=i(m).parentElement;t.forEach(M=>p.classList.add(M))},a=m=>{const p=i(m).parentElement;t.forEach(M=>p.classList.remove(M))},r=()=>{let m;const p=window.outerHeight;console.log("wHeight",p,"scrollY",window.scrollY),e.forEach(M=>{const S=document.getElementById(M);console.log(M,S.offsetTop),window.scrollY>=S.offsetTop-1&&(m=M)}),m&&m!=s&&(a(s),s=m,n(s))};return qa(()=>{n(s),r();let m=document.createElement("script");m.src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js",m.async=!0,document.head.append(m),window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},processEscapes:!0}}),[r]}class Xr extends vt{constructor(e){super(),yt(this,e,Wr,Or,wt,{})}}export{Xr as component};
